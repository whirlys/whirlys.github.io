<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小旋锋</title>
  
  <subtitle>更努力，只为了我们想要的明天</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://laijianfeng.org/"/>
  <updated>2018-09-09T09:24:06.085Z</updated>
  <id>http://laijianfeng.org/</id>
  
  <author>
    <name>小旋锋</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>设计模式 | 工厂方法模式及典型应用</title>
    <link href="http://laijianfeng.org/2018/09/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%E5%8F%8A%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8/"/>
    <id>http://laijianfeng.org/2018/09/设计模式之工厂方法模式及典型应用/</id>
    <published>2018-09-09T09:20:14.000Z</published>
    <updated>2018-09-09T09:24:06.085Z</updated>
    
    <content type="html"><![CDATA[<h3 id="工厂方法模式"><a href="#工厂方法模式" class="headerlink" title="工厂方法模式"></a>工厂方法模式</h3><p>工厂方法模式(Factory Method Pattern)：定义一个用于创建对象的接口，让子类决定将哪一个类实例化。工厂方法模式让一个类的实例化延迟到其子类。</p><p>工厂方法模式又简称为工厂模式(Factory Pattern)，又可称作虚拟构造器模式(Virtual Constructor Pattern)或多态工厂模式(Polymorphic Factory Pattern)。</p><p>工厂方法模式是一种类创建型模式。</p><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><p>在工厂方法模式结构图中包含如下几个角色：</p><p><strong>Product（抽象产品）</strong>：它是定义产品的接口，是工厂方法模式所创建对象的超类型，也就是产品对象的公共父类</p><p><strong>ConcreteProduct（具体产品）</strong>：它实现了抽象产品接口，某种类型的具体产品由专门的具体工厂创建，具体工厂和具体产品之间一一对应。</p><p><strong>Factory（抽象工厂）</strong>：在抽象工厂类中，声明了工厂方法(Factory Method)，用于返回一个产品。抽象工厂是工厂方法模式的核心，所有创建对象的工厂类都必须实现该接口。</p><p><strong>ConcreteFactory（具体工厂）</strong>：它是抽象工厂类的子类，实现了抽象工厂中定义的工厂方法，并可由客户端调用，返回一个具体产品类的实例。</p><p>与简单工厂模式相比，工厂方法模式最重要的区别是引入了抽象工厂角色，抽象工厂可以是接口，也可以是抽象类或者具体类</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>抽象产品类 Video</p><pre><code>public abstract class Video {    public abstract void produce();}</code></pre><p>具体产品类 JavaVideo 和 PythonVideo，需要继承抽象产品类 Video</p><pre><code>public class JavaVideo extends Video {    @Override    public void produce() {        System.out.println(&quot;录制Java课程视频&quot;);    }}public class PythonVideo extends Video {    @Override    public void produce() {        System.out.println(&quot;录制Python课程视频&quot;);    }}</code></pre><p>抽象工厂类 VideoFactory</p><pre><code>public abstract class VideoFactory {    public abstract Video getVideo();}</code></pre><p>具体工厂类 JavaVideoFactory 和 PythonVideoFactory，需要继承抽象工厂类 VideoFactory</p><pre><code>public class JavaVideoFactory extends VideoFactory {    @Override    public Video getVideo() {        return new JavaVideo();    }}public class PythonVideoFactory extends VideoFactory {    @Override    public Video getVideo() {        return new PythonVideo();    }}</code></pre><p>客户端类，需要什么产品则通过该产品对应的工厂类来获取，不需要知道具体的创建过程</p><pre><code>public class Test {    public static void main(String[] args) {        VideoFactory pythonVideoFactory = new PythonVideoFactory();        VideoFactory javaVideoFactory = new JavaVideoFactory();        Video pythonVideo = pythonVideoFactory.getVideo();        pythonVideo.produce();        Video javaVideo = javaVideoFactory.getVideo();        javaVideo.produce();    }}</code></pre><p>输出</p><pre><code>录制Python课程视频录制Java课程视频</code></pre><p>当需要增加一个产品 FEVideo 时，只需要增加 FEVideo 具体产品类和 FEVideoFactory 具体工厂类即可，不需要修改原有的产品类和工厂类</p><pre><code>public class FEVideo extends Video{    @Override    public void produce() {        System.out.println(&quot;录制FE课程视频&quot;);    }}public class FEVideoFactory extends VideoFactory{    @Override    public Video getVideo() {        return new FEVideo();    }}</code></pre><p>修改客户端代码</p><pre><code>public class Test {    public static void main(String[] args) {        VideoFactory feVideoFactory = new FEVideoFactory();        Video feVideo = feVideoFactory.getVideo();        feVideo.produce();    }}</code></pre><p>还可以通过反射机制和配置文件配合，连客户端代码都不需要修改</p><pre><code>public class Test {    public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException {        // 从文件或数据库等外部渠道获取 工厂类名        String factoryName = &quot;com.designpattern.factorymethod.JavaVideoFactory&quot;;        // 通过反射机制获取工厂类        Class c = Class.forName(factoryName);        VideoFactory factory = (VideoFactory)c.newInstance();        // 生产产品        Video video = factory.getVideo();        video.produce();    }}</code></pre><p>最终的类图如下所示</p><p><img src="http://image.laijianfeng.org/20180908_223324.png" alt="示例.工厂方法结构图"></p><h3 id="工厂方法模式总结"><a href="#工厂方法模式总结" class="headerlink" title="工厂方法模式总结"></a>工厂方法模式总结</h3><p>工厂方法模式是简单工厂模式的延伸，它继承了简单工厂模式的优点，同时还弥补了简单工厂模式的不足。工厂方法模式是使用频率最高的设计模式之一，是很多开源框架和API类库的核心模式。</p><h4 id="工厂方法模式的主要优点"><a href="#工厂方法模式的主要优点" class="headerlink" title="工厂方法模式的主要优点"></a>工厂方法模式的主要优点</h4><ul><li>在工厂方法模式中，工厂方法用来创建客户所需要的产品，同时还向客户隐藏了哪种具体产品类将被实例化这一细节，用户只需要关心所需产品对应的工厂，无须关心创建细节，甚至无须知道具体产品类的类名。</li><li>基于工厂角色和产品角色的多态性设计是工厂方法模式的关键。它能够让工厂可以自主确定创建何种产品对象，而如何创建这个对象的细节则完全封装在具体工厂内部。工厂方法模式之所以又被称为多态工厂模式，就正是因为所有的具体工厂类都具有同一抽象父类。</li><li>使用工厂方法模式的另一个优点是在系统中加入新产品时，无须修改抽象工厂和抽象产品提供的接口，无须修改客户端，也无须修改其他的具体工厂和具体产品，而只要添加一个具体工厂和具体产品就可以了，这样，系统的可扩展性也就变得非常好，完全符合”开闭原则”。</li></ul><h4 id="工厂方法模式的主要缺点"><a href="#工厂方法模式的主要缺点" class="headerlink" title="工厂方法模式的主要缺点"></a>工厂方法模式的主要缺点</h4><ul><li>在添加新产品时，需要编写新的具体产品类，而且还要提供与之对应的具体工厂类，系统中类的个数将成对增加，在一定程度上增加了系统的复杂度，有更多的类需要编译和运行，会给系统带来一些额外的开销。</li><li>由于考虑到系统的可扩展性，需要引入抽象层，在客户端代码中均使用抽象层进行定义，增加了系统的抽象性和理解难度，且在实现时可能需要用到DOM、反射等技术，增加了系统的实现难度。</li></ul><h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><ul><li>客户端不知道它所需要的对象的类。在工厂方法模式中，客户端不需要知道具体产品类的类名，只需要知道所对应的工厂即可，具体的产品对象由具体工厂类创建，可将具体工厂类的类名存储在配置文件或数据库中。</li><li>抽象工厂类通过其子类来指定创建哪个对象。在工厂方法模式中，对于抽象工厂类只需要提供一个创建产品的接口，而由其子类来确定具体要创建的对象，利用面向对象的多态性和里氏代换原则，在程序运行时，子类对象将覆盖父类对象，从而使得系统更容易扩展。</li></ul><h3 id="工厂方法模式的典型应用及源码分析"><a href="#工厂方法模式的典型应用及源码分析" class="headerlink" title="工厂方法模式的典型应用及源码分析"></a>工厂方法模式的典型应用及源码分析</h3><h4 id="Java集合接口-Collection-中的工厂方法模式"><a href="#Java集合接口-Collection-中的工厂方法模式" class="headerlink" title="Java集合接口 Collection 中的工厂方法模式"></a>Java集合接口 Collection 中的工厂方法模式</h4><p>Collection 中的 iterator 方法如下：</p><pre><code>public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; {    Iterator&lt;E&gt; iterator();    // ...省略}</code></pre><blockquote><p>关于 iterator 方法的介绍：<br>Java的迭代器只在Collection中有，而Map没有迭代器，它有不同的迭代方法；<br> <strong>迭代器的终极目标</strong>：就是用统一的方法来迭代不同类型的集合！可能由于不同集合的内部数据结构不尽相同，如果要自己纯手工迭代的话相互之间会有很大的差别，而迭代器的作用就是统一的方法对不同的集合进行迭代，而在迭代器底层隐藏不同集合之间的差异，从而为迭代提供最大的方便<br><strong>使用用迭代器迭代的步骤</strong>： i. 第一步肯定是先获取集合的迭代器：调用集合的iterator方法就能获得，Iterator<e> Collection.iterator(); ii. 使用迭代器的hasNext、next往下迭代<br><strong>Iterator的常用方法</strong>：boolean hasNext()：是否还有下一个元素； Object next()：取出下一个元素并返回； void remove(); ：从容器中删除当前元素，直接会改变容器中的数据</e></p></blockquote><p>查看该接口的实现类，可以看到是非常的多</p><p><img src="http://image.laijianfeng.org/20180908_230822.png" alt="Collection接口的实现类(部分)"></p><p>我们仅看其中一个实现类 <code>java.util.ArrayList</code>，看其对 <code>iterator</code> 方法的实现</p><pre><code>public Iterator&lt;E&gt; iterator() {    return new Itr();}/** * An optimized version of AbstractList.Itr */private class Itr implements Iterator&lt;E&gt; {    int cursor;       // index of next element to return    int lastRet = -1; // index of last element returned; -1 if no such    int expectedModCount = modCount;    Itr() {}    public boolean hasNext() {        return cursor != size;    }    @SuppressWarnings(&quot;unchecked&quot;)    public E next() {        // ...省略...    }    public void remove() {        // ...省略...    }    @Override    @SuppressWarnings(&quot;unchecked&quot;)    public void forEachRemaining(Consumer&lt;? super E&gt; consumer) {        // ...省略...    }    final void checkForComodification() {        // ...省略...    }}</code></pre><p><code>Itr</code> 类实现了 <code>iterator</code> 接口，<code>iterator</code> 接口正是 <code>Collection</code> 接口中 <code>iterator</code> 方法的返回类型，其代码如下：</p><pre><code>public interface Iterator&lt;E&gt; {    boolean hasNext();    E next();    default void remove() {        throw new UnsupportedOperationException(&quot;remove&quot;);    }    default void forEachRemaining(Consumer&lt;? super E&gt; action) {        Objects.requireNonNull(action);        while (hasNext())            action.accept(next());    }}</code></pre><p><strong>由此可见</strong>，<code>Collection</code> 接口扮演了抽象工厂角色，工厂方法为 <code>iterator()</code>，<code>Collection</code> 的实现类譬如 <code>ArrayList</code> 扮演了具体工厂角色，而抽象产品为 <code>Iterator</code> 接口，具体产品为 <code>Itr</code> 类</p><h4 id="java-net-网络包中的工厂方法模式"><a href="#java-net-网络包中的工厂方法模式" class="headerlink" title="java.net 网络包中的工厂方法模式"></a>java.net 网络包中的工厂方法模式</h4><p>URLStreamHandlerFactory 接口为 URL 流协议处理程序定义一个工厂。URL 类使用它可为特定的协议创建 URLStreamHandler</p><pre><code>public interface URLStreamHandlerFactory {    /**     * Creates a new {@code URLStreamHandler} instance with the specified protocol.     *     * @param   protocol   the protocol (&quot;{@code ftp}&quot;, &quot;{@code http}&quot;, &quot;{@code nntp}&quot;, etc.).     * @return  a {@code URLStreamHandler} for the specific protocol.     * @see     java.net.URLStreamHandler     */    URLStreamHandler createURLStreamHandler(String protocol);}</code></pre><p>该接口的实现类为 <code>sun.misc.Launcher</code> 中的内部类 <code>Factory</code></p><pre><code>private static class Factory implements URLStreamHandlerFactory {    private static String PREFIX = &quot;sun.net.www.protocol&quot;;    private Factory() {    }    public URLStreamHandler createURLStreamHandler(String var1) {        String var2 = PREFIX + &quot;.&quot; + var1 + &quot;.Handler&quot;;        try {            Class var3 = Class.forName(var2);            return (URLStreamHandler)var3.newInstance();        } catch (ReflectiveOperationException var4) {            throw new InternalError(&quot;could not load &quot; + var1 + &quot;system protocol handler&quot;, var4);        }    }}</code></pre><p>可以看到 <code>createURLStreamHandler</code> 方法的实现为：传入参数，拼接前缀和后缀，之后通过反射机制获取创建一个 <code>URLStreamHandler</code> 对象</p><p><code>URLStreamHandler</code> 是一个抽象类，其中的方法如下图，只有 <code>openConnection</code> 为抽象方法，其他方法均有具体实现</p><p><img src="http://image.laijianfeng.org/20180908_235350.png" alt="URLStreamHandler抽象类中的方法"></p><blockquote><p>关于URLStreamHandler:<br>抽象类URLStreamHandler是所有流协议处理程序的通用超类。 流协议处理程序知道如何为特定协议类型建立连接，例如http或https</p></blockquote><p>其子类有如下(19个)：</p><p><img src="http://image.laijianfeng.org/20180908_234100.png" alt="URLStreamHandler的子类"></p><p>查看其中一个子类譬如 <code>sun.net.www.protocol.http.Handler</code></p><pre><code>public class Handler extends URLStreamHandler {    protected String proxy;    protected int proxyPort;    protected int getDefaultPort() {        return 80;    }    public Handler() {        this.proxy = null;        this.proxyPort = -1;    }    public Handler(String var1, int var2) {        this.proxy = var1;        this.proxyPort = var2;    }    protected URLConnection openConnection(URL var1) throws IOException {        return this.openConnection(var1, (Proxy)null);    }    protected URLConnection openConnection(URL var1, Proxy var2) throws IOException {        return new HttpURLConnection(var1, var2, this);    }}</code></pre><p>该类实现的 <code>openConnection</code> 方法的返回值类型为 <code>URLConnection</code>，最终返回了一个 <code>HttpURLConnection</code> 对象</p><p>我们又继续看 <code>java.net.URLConnection</code>，这也是一个抽象类</p><p><img src="http://image.laijianfeng.org/20180909_001513.png" alt="image"></p><blockquote><p><strong>URLConnection介绍</strong>：   </p><ul><li>URLConnection是一个功能强大的抽象类，它表示指向URL指定资源的活动连接。<br>与URL类相比，它与服务器的交互提供了更多的控制机制。尤其服务器是HTTP服务器，可以使用URLConnection对HTTP首部的访问，可以配置发送给服务器的请求参数。当然也可以通过它读取服务器的数据以及向服务器写入数据.   </li><li>URLConnection是Java的协议处理器机制的一部分。协议处理器机制是将处理协议的细节与特定数据类型分开。如果要实现一个特定的协议，则实现URLConnection的子类即可。程序运行时可以将该子类作为一个具体的协议处理器来使用。   </li><li><strong>使用URLConnection类的步骤</strong>：1.  构造一个URL对象；2. 调用该URL的openConnection()获取一个URLConnection；3. 配置这个URLConnection；4. 读取首部字段；5. 获得输入流并读取数据；6. 获得输出流并写入数据；7. 关闭连接</li></ul></blockquote><p>其子类有23个</p><p><img src="http://image.laijianfeng.org/20180909_001308.png" alt="image"></p><p>我们可以画出他们的关系图如下所示</p><p><img src="http://image.laijianfeng.org/20180909_164345.png" alt="URLConnection关系图"></p><p><strong>由此可知</strong>：抽象工厂角色为 <code>URLStreamHandlerFactory</code>，工厂方法为 <code>createURLStreamHandler</code>，抽象产品角色为 <code>URLStreamHandler</code>，具体产品角色为 <code>URLStreamHandler</code> 的子类譬如 <code>sun.net.www.protocol.http.Handler</code>、<code>sun.net.www.protocol.ftp.Handler</code> 等</p><p><strong>同时</strong>，<code>URLStreamHandler</code> 也扮演了抽象工厂角色，工厂方法为 <code>openConnection</code>，<code>URLStreamHandler</code> 的子类譬如 <code>sun.net.www.protocol.http.Handler</code> 也扮演了具体工厂角色，抽象产品为 <code>URLConnection</code>，具体产品角色为  <code>URLConnection</code> 的子类如 <code>sun.net.www.protocol.http.HttpURLConnection</code> 等</p><h4 id="Logback-中的工厂方法模式"><a href="#Logback-中的工厂方法模式" class="headerlink" title="Logback 中的工厂方法模式"></a>Logback 中的工厂方法模式</h4><p>在上一篇文章《<a href="https://mp.weixin.qq.com/s?__biz=MzI1NDU0MTE1NA==&amp;mid=2247483700&amp;idx=1&amp;sn=dd4d23f9400c8be248f5d125465ba941&amp;chksm=e9c2ed39deb5642fb809eca1351f00995f06c9a4875f09986cc5005059eff74d6c20fc1118a3&amp;scene=0#rd" target="_blank" rel="noopener">设计模式 | 简单工厂模式及典型应用</a>》 介绍的 Logback 里有简单工厂模式，其实也有工厂方法模式，画图如下</p><p><img src="http://image.laijianfeng.org/20180909_170301.png" alt="iLoggerFactory类关系"></p><p><strong>可以看出</strong>，抽象工厂角色为 <code>ILoggerFactory</code> 接口，工厂方法为 <code>getLogger</code>，具体工厂角色为 <code>LoggerContext</code>、<code>NOPLoggerFactory</code>、<code>SubstituteLoggerFactory</code> 等，抽象产品角色为 <code>Logger</code>，具体产品角色为 <code>Logger</code> 的实现类如下</p><p><img src="http://image.laijianfeng.org/20180909_171112.png" alt="Logger 的实现类"></p><p>而简单工厂模式应用在 <code>LoggerContext</code> 的  <code>getLogger</code> 方法中，根据参数返回相应的 <code>Logger</code> 对象</p><blockquote><p>参考：<br>刘伟：设计模式Java版<br>慕课网java设计模式精讲 Debug 方式+内存分析</p></blockquote><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;工厂方法模式&quot;&gt;&lt;a href=&quot;#工厂方法模式&quot; class=&quot;headerlink&quot; title=&quot;工厂方法模式&quot;&gt;&lt;/a&gt;工厂方法模式&lt;/h3&gt;&lt;p&gt;工厂方法模式(Factory Method Pattern)：定义一个用于创建对象的接口，让子类决定将哪一个
      
    
    </summary>
    
      <category term="后端" scheme="http://laijianfeng.org/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="设计模式" scheme="http://laijianfeng.org/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>设计模式 | 简单工厂模式及典型应用</title>
    <link href="http://laijianfeng.org/2018/09/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%E5%8F%8A%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8/"/>
    <id>http://laijianfeng.org/2018/09/设计模式-简单工厂模式及典型应用/</id>
    <published>2018-09-07T15:13:45.000Z</published>
    <updated>2018-09-07T15:14:50.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>设计模式(Design Pattern)是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结，使用设计模式是为了可重用代码、让代码更容易被他人<br>理解并且保证代码可靠性。</p><p>本文主要介绍简单工厂模式及典型应用，内容如下：</p><ul><li>简单工厂模式的介绍</li><li>简单工厂模式的典型应用及源码分析<ul><li>Calendar 类获取日历类对象</li><li>JDBC 获取数据库连接</li><li>LoggerFactory 获取 Logger 对象</li></ul></li></ul><h3 id="简单工厂模式"><a href="#简单工厂模式" class="headerlink" title="简单工厂模式"></a>简单工厂模式</h3><p>工厂模式是最常用的一类创建型设计模式，包括 抽象工厂模式，工厂方法模式和简单工厂模式 这三种，简单工厂模式是其中最简单的一种</p><p>简单工厂模式(Simple Factory Pattern)：定义一个工厂类，它可以<strong>根据参数的不同</strong>返回不同类的实例，被创建的实例通常都具有共同的父类。</p><p>因为在简单工厂模式中用于创建实例的方法是静态(static)方法，因此简单工厂模式又被称为静态工厂方法(Static Factory Method)模式，它属于类创建型模式，但不属于GOF23种设计模式</p><h4 id="角色"><a href="#角色" class="headerlink" title="角色"></a>角色</h4><p><strong>Factory（工厂角色）</strong>：工厂角色即工厂类，它是<strong>简单工厂模式的核心</strong>，负责实现创建所有产品实例的内部逻辑；工厂类可以被外界直接调用，创建所需的产品对象；在工厂类中提供了静态的工厂方法factoryMethod()，它的返回类型为抽象产品类型Product</p><p><strong>Product（抽象产品角色）</strong>：它是工厂类所创建的所有对象的父类，封装了各种产品对象的公有方法，它的引入将提高系统的灵活性，使得在工厂类中只需定义一个通用的工厂方法，因为所有创建的具体产品对象都是其子类对象。</p><p><strong>ConcreteProduct（具体产品角色）</strong>：它是简单工厂模式的创建目标，所有被创建的对象都充当这个角色的某个具体类的实例。每一个具体产品角色都继承了抽象产品角色，需要实现在抽象产品中声明的抽象方法</p><p>在简单工厂模式中，客户端通过工厂类来创建一个产品类的实例，而无须直接使用new关键字来创建对象，它是工厂模式家族中最简单的一员</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>抽象产品类 Video，定义了抽象方法 produce()</p><pre><code>public abstract class Video {    public abstract void produce();}</code></pre><p>具体产品类 JavaVideo 和 PythonVideo，都继承了抽象产品类 Video</p><pre><code>public class JavaVideo extends Video {    @Override    public void produce() {        System.out.println(&quot;录制Java课程视频&quot;);    }}public class PythonVideo extends Video {    @Override    public void produce() {        System.out.println(&quot;录制Python课程视频&quot;);    }}</code></pre><p>工厂类实现的两种方法：使用<code>if-else</code>判断和使用反射来创建对象</p><pre><code>public class VideoFactory {    /**     * 使用if else 判断类型，type 为 Java 则返回 JavaVideo， type为Python则返回 PythonVideo     */    public Video getVideo(String type) {        if (&quot;java&quot;.equalsIgnoreCase(type)) {            return new JavaVideo();        } else if (&quot;python&quot;.equalsIgnoreCase(type)) {            return new PythonVideo();        }        return null;    }    /**     * 使用反射来创建对象     */    public Video getVideo(Class c) {        Video video = null;        try {            video = (Video) Class.forName(c.getName()).newInstance();        } catch (InstantiationException e) {            e.printStackTrace();        } catch (IllegalAccessException e) {            e.printStackTrace();        } catch (ClassNotFoundException e) {            e.printStackTrace();        }        return video;    }}</code></pre><p>使用一个客户端来调用工厂类</p><pre><code>public class Test {    public static void main(String[] args) {        VideoFactory videoFactory = new VideoFactory();        Video video1 = videoFactory.getVideo(&quot;python&quot;);        if (video1 == null) {            return;        }        video1.produce();        Video video2 = videoFactory.getVideo(JavaVideo.class);        if (video2 == null) {            return;        }        video2.produce();    }}</code></pre><p>输出</p><pre><code>录制Python课程视频录制Java课程视频</code></pre><p><img src="http://image.laijianfeng.org/20180907_211234.png" alt="示例.简单工厂模式类图"></p><p>Test 类通过传递参数给 <code>VideoFactory.getVideo()</code> 来获取对象，创建对象的逻辑交给了工厂类 <code>VideoFactory</code> 来完成</p><h4 id="简单工厂模式总结"><a href="#简单工厂模式总结" class="headerlink" title="简单工厂模式总结"></a>简单工厂模式总结</h4><p>简单工厂模式的<strong>主要优点</strong>如下：</p><ul><li>工厂类包含必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例，客户端可以免除直接创建产品对象的职责，而仅仅“消费”产品，简单工厂模式实现了对象创建和使用的分离。</li><li>客户端无须知道所创建的具体产品类的类名，只需要知道具体产品类所对应的参数即可，对于一些复杂的类名，通过简单工厂模式可以在一定程度减少使用者的记忆量。</li><li>通过引入配置文件，可以在不修改任何客户端代码的情况下更换和增加新的具体产品类，在一定程度上提高了系统的灵活性。</li></ul><p>简单工厂模式的<strong>主要缺点</strong>如下：</p><ul><li>由于工厂类集中了所有产品的创建逻辑，职责过重，一旦不能正常工作，整个系统都要受到影响。</li><li>使用简单工厂模式势必会增加系统中类的个数（引入了新的工厂类），增加了系统的复杂度和理解难度。</li><li>系统扩展困难，一旦添加新产品就不得不修改工厂逻辑，在产品类型较多时，有可能造成工厂逻辑过于复杂，不利于系统的扩展和维护，且违背开闭原则。</li><li>简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。</li></ul><p><strong>适用场景</strong>：</p><ul><li>工厂类负责创建的对象比较少，由于创建的对象较少，不会造成工厂方法中的业务逻辑太过复杂。</li><li>客户端只知道传入工厂类的参数，对于如何创建对象并不关心。</li></ul><h3 id="简单工厂模式的典型应用及源码分析"><a href="#简单工厂模式的典型应用及源码分析" class="headerlink" title="简单工厂模式的典型应用及源码分析"></a>简单工厂模式的典型应用及源码分析</h3><h4 id="Calendar-类获取日历类对象"><a href="#Calendar-类获取日历类对象" class="headerlink" title="Calendar 类获取日历类对象"></a>Calendar 类获取日历类对象</h4><p><code>Calendar</code> 抽象类，该类的子类有 <code>BuddhistCalendar</code>、<code>JapaneseImperialCalendar</code>、<code>GregorianCalendar</code>、<code>RollingCalendar</code>等</p><p><code>getInstance</code>方法，根据参数获取一个<code>Calendar</code>子类对象，该方法实际将参数传给 <code>createCalendar</code> 方法，<code>createCalendar</code> 在根据参数通过 <code>provider</code> 或 <code>switch</code> 或者 <code>if-else</code> 创建相应的子类对象</p><p>以下为 Java8 中的 <code>Calendar</code> 类代码，Java7 中的实现为 <code>if-else</code> 方式</p><pre><code>public static Calendar getInstance(TimeZone zone, Locale aLocale) {    return createCalendar(zone, aLocale);}private static Calendar createCalendar(TimeZone zone, Locale aLocale) {    CalendarProvider provider = LocaleProviderAdapter.getAdapter(CalendarProvider.class, aLocale).getCalendarProvider();    if (provider != null) {        try {            return provider.getInstance(zone, aLocale);        } catch (IllegalArgumentException iae) {        }    }    Calendar cal = null;    if (aLocale.hasExtensions()) {        String caltype = aLocale.getUnicodeLocaleType(&quot;ca&quot;);        if (caltype != null) {            switch (caltype) {                case &quot;buddhist&quot;:                    cal = new BuddhistCalendar(zone, aLocale); break;                case &quot;japanese&quot;:                    cal = new JapaneseImperialCalendar(zone, aLocale); break;                case &quot;gregory&quot;:                    cal = new GregorianCalendar(zone, aLocale); break;            }        }    }    if (cal == null) {        if (aLocale.getLanguage() == &quot;th&quot; &amp;&amp; aLocale.getCountry() == &quot;TH&quot;) {            cal = new BuddhistCalendar(zone, aLocale);        } else if (aLocale.getVariant() == &quot;JP&quot; &amp;&amp; aLocale.getLanguage() == &quot;ja&quot; &amp;&amp; aLocale.getCountry() == &quot;JP&quot;) {            cal = new JapaneseImperialCalendar(zone, aLocale);        } else {            cal = new GregorianCalendar(zone, aLocale);        }    }    return cal;}</code></pre><p><img src="http://image.laijianfeng.org/20180907_213426.png" alt="Calendar的继承关系"></p><p>可以看到<code>抽象产品角色</code>和<code>工厂角色</code>都由  <code>Calendar</code> 担任，<code>具体产品角色</code>由 <code>Calendar</code> 的子类担任</p><h4 id="JDBC-获取数据库连接"><a href="#JDBC-获取数据库连接" class="headerlink" title="JDBC 获取数据库连接"></a>JDBC 获取数据库连接</h4><p>一般JDBC获取MySQL连接的写法如下：</p><pre><code>//加载MySql驱动Class.forName(&quot;com.mysql.jdbc.Driver&quot;);DriverManager.getConnection(&quot;jdbc:mysql://127.0.0.1:3306/test&quot;, &quot;root&quot;, &quot;123456&quot;);</code></pre><p>首先通过反射加载驱动类 <code>com.mysql.jdbc.Driver</code> 类，然后再通过 <code>DriverManager</code> 获取连接</p><p>看看 <code>com.mysql.jdbc.Driver</code> 的代码，该类主要的内容是静态代码块，其会随着类的加载一块执行</p><pre><code>public class Driver extends NonRegisteringDriver implements java.sql.Driver {    public Driver() throws SQLException {    }    static {        try {            DriverManager.registerDriver(new Driver());        } catch (SQLException var1) {            throw new RuntimeException(&quot;Can&#39;t register driver!&quot;);        }    }}</code></pre><p>静态代码块：new 一个 <code>Driver</code> 类并注册到 <code>DriverManager</code> 驱动管理类中</p><pre><code>public static synchronized void registerDriver(java.sql.Driver driver, DriverAction da) throws SQLException {    /* Register the driver if it has not already been added to our list */    if(driver != null) {        registeredDrivers.addIfAbsent(new DriverInfo(driver, da));    } else {        throw new NullPointerException();    }    println(&quot;registerDriver: &quot; + driver);}</code></pre><p>其中的 <code>registeredDrivers</code> 是一个 <code>CopyOnWriteArrayList</code> 对象</p><pre><code>private final static CopyOnWriteArrayList&lt;DriverInfo&gt; registeredDrivers = new CopyOnWriteArrayList&lt;&gt;();</code></pre><blockquote><p>CopyOnWriteArrayList是Java并发包中提供的一个并发容器，它是个线程安全且读操作无锁的ArrayList，写操作则通过创建底层数组的新副本来实现，是一种读写分离的并发策略，我们也可以称这种容器为”写时复制器”，Java并发包中类似的容器还有CopyOnWriteSet<br>一篇CopyOnWriteArrayList的文章：<a href="https://www.cnblogs.com/chengxiao/p/6881974.html" target="_blank" rel="noopener">https://www.cnblogs.com/chengxiao/p/6881974.html</a></p></blockquote><p>再通过 <code>DriverManager.getConnection</code> 获取连接对象的主要代码如下：通过for循环从已注册的驱动中(registeredDrivers)获取驱动，尝试连接，成功则返回连接</p><pre><code>private static Connection getConnection(String url, java.util.Properties info, Class&lt;?&gt; caller) throws SQLException {    // ...省略...    println(&quot;DriverManager.getConnection(\&quot;&quot; + url + &quot;\&quot;)&quot;);    for(DriverInfo aDriver : registeredDrivers) {        // If the caller does not have permission to load the driver then skip it.        if(isDriverAllowed(aDriver.driver, callerCL)) {            try {                println(&quot;    trying &quot; + aDriver.driver.getClass().getName());                Connection con = aDriver.driver.connect(url, info);                if (con != null) {                    // Success!                    println(&quot;getConnection returning &quot; + aDriver.driver.getClass().getName());                    return (con);                }            } catch (SQLException ex) {                if (reason == null) {                    reason = ex;                }            }        } else {            println(&quot;    skipping: &quot; + aDriver.getClass().getName());        }    }    // ...省略...}</code></pre><p><img src="http://image.laijianfeng.org/20180907_225826.png" alt="Connection 接口及子类实现关系"></p><p>工厂角色为 <code>DriverManager</code> 类，抽象产品角色为 <code>Connection</code>，具体产品角色则很多</p><h4 id="Logback-中的-LoggerFactory-获取-Logger-对象"><a href="#Logback-中的-LoggerFactory-获取-Logger-对象" class="headerlink" title="Logback 中的 LoggerFactory 获取 Logger 对象"></a>Logback 中的 LoggerFactory 获取 Logger 对象</h4><p>查看 <code>LoggerFactory</code> 类的 <code>getLogger</code> 方法，可看到调用了 <code>iLoggerFactory.getLogger()</code>，其中 <code>iLoggerFactory</code> 是一个接口</p><pre><code>public static Logger getLogger(String name) {    ILoggerFactory iLoggerFactory = getILoggerFactory();    return iLoggerFactory.getLogger(name);}public static Logger getLogger(Class clazz) {    return getLogger(clazz.getName());}</code></pre><p><code>iLoggerFactory</code> 接口只有一个 <code>getLogger</code> 方法</p><pre><code>public interface ILoggerFactory {    Logger getLogger(String var1);}</code></pre><p>查看其子类依赖关系</p><p><img src="http://image.laijianfeng.org/20180907_223038.png" alt="iLoggerFactory接口子类的依赖关系"></p><p>再看一个子类 <code>LoggerContext</code> 对 ILoggerFactory 的实现</p><p><img src="http://image.laijianfeng.org/20180907_222610.png" alt="image"></p><p>可看到这是通过 <code>if-else</code> 方式的简单工厂模式</p><p><img src="http://image.laijianfeng.org/20180907_230131.png" alt="Logger 接口及子类实现关系"></p><p>工厂角色为 <code>iLoggerFactory</code> 接口的子类如 <code>LoggerContext</code>，抽象产品角色为 <code>Logger</code>，具体产品角色为 <code>Logger</code> 的子类，主要是 <code>NOPLogger</code> 和 <code>Logger</code> 类</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>下一篇介绍工厂方法及典型应用</p><blockquote><p>参考：<br>刘伟：设计模式Java版<br>慕课网java设计模式精讲 Debug 方式+内存分析</p></blockquote><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;设计模式(Design Pattern)是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结，使用设计模式是为了可重用代码、让
      
    
    </summary>
    
      <category term="后端" scheme="http://laijianfeng.org/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="设计模式" scheme="http://laijianfeng.org/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 6.3.2 启动过程</title>
    <link href="http://laijianfeng.org/2018/09/Elasticsearch-6-3-2-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B/"/>
    <id>http://laijianfeng.org/2018/09/Elasticsearch-6-3-2-启动过程/</id>
    <published>2018-09-01T12:25:45.000Z</published>
    <updated>2018-09-01T12:29:11.559Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文探究Elasticsearch 6.3.2的启动流程</p><h4 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h4><p>使用工具：IDEA，XMind</p><p>关于ES调试环境的搭建，可以参考前面的文章 《<a href="https://mp.weixin.qq.com/s?__biz=MzI1NDU0MTE1NA==&amp;mid=2247483676&amp;idx=1&amp;sn=1d88a883ce21d7dcacd073a8fa85dbfc&amp;chksm=e9c2ed11deb56407879ba0b22a4ef96916f8a9e7931e1efb99df57991966a3dc475eb3e23101&amp;mpshare=1&amp;scene=1&amp;srcid=0901DM6ZqcQqSqyujfIApsDj#rd" target="_blank" rel="noopener">教你编译调试Elasticsearch 6.3.2源码</a>》</p><p>然后通过设置断点，从 <code>org.elasticsearch.bootstrap.ElasticSearch</code> 的入口函数开始，一步一步调试</p><p><img src="http://image.laijianfeng.org/20180901_131938.png" alt="IDEA 2018.2 调试按钮"></p><p>上图为使用 IDEA 2018.2 进行调试的一个截图，左上角84行出红点为一个断点，1、2、3编号的3个按钮是较为常用的按钮，作用如下：</p><ul><li>按钮1：step over，执行到下一行，遇到方法<strong>不进入</strong>方法内部</li><li>按钮2：step into，执行到下一句代码，遇到方法则<strong>进入</strong>方法内部</li><li>按钮3：Run to cursor，执行到下一个断点处，后面没有断点则执行到结束</li></ul><h4 id="通过XMind记录ES启动流程的整个过程"><a href="#通过XMind记录ES启动流程的整个过程" class="headerlink" title="通过XMind记录ES启动流程的整个过程"></a>通过XMind记录ES启动流程的整个过程</h4><p><img src="http://image.laijianfeng.org/ES_startup_process.jpg" alt="ES 6.3.2 启动流程"></p><p>根据上图，作者大概地把ES启动流程分为四个阶段：</p><ul><li>Elasticsearch 解析 Command，加载配置</li><li>Bootstrap 初始化，资源检查</li><li>Node 创建节点</li><li>Bootstrap 启动节点和保活线程</li></ul><h3 id="Elasticsearch-解析-Command，加载配置"><a href="#Elasticsearch-解析-Command，加载配置" class="headerlink" title="Elasticsearch 解析 Command，加载配置"></a>Elasticsearch 解析 Command，加载配置</h3><p>首先可以看一下入口方法 <code>Elasticsearch.main</code>：</p><pre><code>    public static void main(final String[] args) throws Exception {        System.setSecurityManager(new SecurityManager() {            @Override            public void checkPermission(Permission perm) {                // grant all permissions so that we can later set the security manager to the one that we want            }        });        LogConfigurator.registerErrorListener();        final Elasticsearch elasticsearch = new Elasticsearch();        int status = main(args, elasticsearch, Terminal.DEFAULT);        if (status != ExitCodes.OK) {            exit(status);        }    }</code></pre><p>1.1, 创建 SecurityManager 安全管理器</p><blockquote><p>关于 SecurityManager:<br>安全管理器在Java语言中的作用就是<strong>检查操作是否有权限执行</strong>，通过则顺序进行，否则抛出一个异常<br>网上一篇文章：<a href="https://blog.csdn.net/wwwdc1012/article/details/82287474" target="_blank" rel="noopener">Java安全——安全管理器、访问控制器和类装载器</a></p></blockquote><p>1.2, LogConfigurator.registerErrorListener() 注册侦听器</p><p>1.3, 创建Elasticsearch对象</p><p>Elasticsearch 入口类的继承关系如下：</p><p><img src="http://image.laijianfeng.org/20180901_143515.png" alt="Elasticsearch 入口类的继承关系"></p><p>可以看到Elasticsearch继承了EnvironmentAwareCommand，Command，这几个类的功能简要介绍如下：</p><ul><li>Elasticsearch: This class starts elasticsearch.</li><li>EnvironmentAwareCommand: A cli command which requires an <code>org.elasticsearch.env.Environment</code> to use current paths and settings</li><li>Command: An action to execute within a cli.</li></ul><p>可以看出Elasticsearch的一个重要作用是解析命令参数</p><p>执行带 <code>-h</code> 参数的Elasticsearch启动命令</p><p><img src="http://image.laijianfeng.org/20180901_144410.png" alt="带参数的Elasticsearch启动命令"></p><p>可以发现这几个参数与 Cammand 类 和 Elasticsearch 的几个私有变量是对应的</p><p>Elasticsearch的构造函数如下：</p><pre><code>Elasticsearch() {    super(&quot;starts elasticsearch&quot;, () -&gt; {}); // we configure logging later so we override the base class from configuring logging    versionOption = parser.acceptsAll(Arrays.asList(&quot;V&quot;, &quot;version&quot;), &quot;Prints elasticsearch version information and exits&quot;);    daemonizeOption = parser.acceptsAll(Arrays.asList(&quot;d&quot;, &quot;daemonize&quot;), &quot;Starts Elasticsearch in the background&quot;)        .availableUnless(versionOption);    pidfileOption = parser.acceptsAll(Arrays.asList(&quot;p&quot;, &quot;pidfile&quot;), &quot;Creates a pid file in the specified path on start&quot;)        .availableUnless(versionOption).withRequiredArg().withValuesConvertedBy(new PathConverter());    quietOption = parser.acceptsAll(Arrays.asList(&quot;q&quot;, &quot;quiet&quot;), &quot;Turns off standard output/error streams logging in console&quot;)        .availableUnless(versionOption).availableUnless(daemonizeOption);}</code></pre><p>1.4, 接着进入 <code>Command.main</code> 方法</p><p>该方法给当前Runtime类添加一个hook线程，该线程作用是：当Runtime异常关闭时打印异常信息</p><p>1.5, <code>Command.mainWithoutErrorHandling</code> 方法，根据命令行参数，打印或者设置参数，然后执行命令，有异常则抛出所有异常</p><p>1.6, <code>EnvironmentAwareCommand.execute</code>，确保 <code>es.path.data</code>, <code>es.path.home</code>, <code>es.path.logs</code> 等参数已设置，否则从 <code>System.properties</code> 中读取</p><pre><code>putSystemPropertyIfSettingIsMissing(settings, &quot;path.data&quot;, &quot;es.path.data&quot;);putSystemPropertyIfSettingIsMissing(settings, &quot;path.home&quot;, &quot;es.path.home&quot;);putSystemPropertyIfSettingIsMissing(settings, &quot;path.logs&quot;, &quot;es.path.logs&quot;);execute(terminal, options, createEnv(terminal, settings));</code></pre><p>1.7, <code>EnvironmentAwareCommand.createEnv</code>，读取config下的配置文件<code>elasticsearch.yml</code>内容，收集plugins，bin，lib，modules等目录下的文件信息</p><p>createEnv最后返回一个 Environment 对象，执行结果如下</p><p><img src="http://image.laijianfeng.org/20180901_160825.png" alt="EnvironmentAwareCommand.createEnv"></p><p>1.8, <code>Elasticsearch.execute</code> ，读取daemonize， pidFile，quiet 的值，并 确保配置的临时目录(temp)是有效目录</p><p>进入Bootstrap初始化阶段</p><pre><code>Bootstrap.init(!daemonize, pidFile, quiet, initialEnv);</code></pre><h3 id="Bootstrap初始化阶段"><a href="#Bootstrap初始化阶段" class="headerlink" title="Bootstrap初始化阶段"></a>Bootstrap初始化阶段</h3><h4 id="Bootstrap-init"><a href="#Bootstrap-init" class="headerlink" title="Bootstrap.init"></a>Bootstrap.init</h4><p>2.1, 进入 <code>Bootstrap.init</code>, This method is invoked by <code>Elasticsearch#main(String[])</code> to startup elasticsearch.</p><p><code>INSTANCE = new Bootstrap();</code>, 创建一个Bootstrap对象作为类对象，该类构造函数会创建一个用户线程，添加到Runtime Hook中，进行 countDown 操作</p><pre><code> private final CountDownLatch keepAliveLatch = new CountDownLatch(1); /** creates a new instance */    Bootstrap() {        keepAliveThread = new Thread(new Runnable() {            @Override            public void run() {                try {                    keepAliveLatch.await();                } catch (InterruptedException e) {                }            }        }, &quot;elasticsearch[keepAlive/&quot; + Version.CURRENT + &quot;]&quot;);        keepAliveThread.setDaemon(false);        // keep this thread alive (non daemon thread) until we shutdown        Runtime.getRuntime().addShutdownHook(new Thread() {            @Override            public void run() {                keepAliveLatch.countDown();            }        });    }</code></pre><blockquote><p>CountDownLatch是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程执行完后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有框架服务之后执行。<br>CountDownLatch是通过一个计数器来实现的，计数器的初始化值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就相应得减1。当计数器到达0时，表示所有的线程都已完成任务，然后在闭锁上等待的线程就可以恢复执行任务。<br>更多介绍请看文章：<a href="https://blog.csdn.net/wwwdc1012/article/details/82288473" target="_blank" rel="noopener">并发工具类 CountDownLatch</a></p></blockquote><p>2.2, 加载 keystore 安全配置，keystore文件不存在则创建，保存；存在则解密，更新keystore</p><p>2.3, 根据已有的配置信息，创建一个Environment对象</p><p>2.4, LogConfigurator log4j日志配置</p><p>2.5, 检查pid文件是否存在，不存在则创建</p><blockquote><p>关于 pid 文件：<br>(1) <strong>pid文件的内容</strong>：pid文件为文本文件，内容只有一行，记录了该进程的ID，用cat命令可以看到。<br>(2) <strong>pid文件的作用</strong>：防止进程启动多个副本。只有获得pid文件(固定路径固定文件名)写入权限(F_WRLCK)的进程才能正常启动并把自身的PID写入该文件中，其它同一个程序的多余进程则自动退出。</p></blockquote><p>2.6, 检查Lucene版本与实际的Lucene Jar文件的版本是否一致，不一致则抛异常</p><p>2.7, 设置未捕获异常的处理 Thread.setDefaultUncaughtExceptionHandler</p><p>在Thread ApI中提供了UncaughtExceptionHandle，它能检测出某个由于未捕获的异常而终结的情况</p><blockquote><p>朱小厮 <a href="https://blog.csdn.net/u013256816/article/details/50417822" target="_blank" rel="noopener">JAVA多线程之UncaughtExceptionHandler——处理非正常的线程中止</a></p></blockquote><h4 id="INSTANCE-setup-true-environment"><a href="#INSTANCE-setup-true-environment" class="headerlink" title="INSTANCE.setup(true, environment);"></a>INSTANCE.setup(true, environment);</h4><p>3.1，<code>spawner.spawnNativeControllers(environment);</code></p><p>遍历每个模块，生成本机控制类（native Controller）：读取modules文件夹下所有的文件夹中的模块信息，保存为一个 PluginInfo  对象，为合适的模块生成控制类，通过 <code>Files.isRegularFile(spawnPath)</code> 来判断</p><p>尝试为给定模块生成控制器(native Controller)守护程序。    生成的进程将通过其stdin，stdout和stderr流保持与此JVM的连接，但对此包之外的代码不能使用对这些流的引用。</p><p>3.2， <code>initializeNatives(Path tmpFile, boolean mlockAll, boolean systemCallFilter, boolean ctrlHandler)</code>初始化本地资源</p><p>检查用户是否为root用户，是则抛异常;<br>尝试启用 系统调用过滤器 system call filter;<br>如果设置了则进行 mlockall<br>Windows关闭事件监听器<br>init lucene random seed.   </p><p>这个过程中使用到了 Natives 类:<br>Natives类是一个包装类，用于检查调用本机方法所需的类是否在启动时可用。如果它们不可用，则此类将避免调用加载这些类的代码</p><p>3.3, 添加一个Hook： Runtime.getRuntime().addShutdownHook，当ES退出时用于关闭必要的IO流，日志器上下文和配置器等</p><p>3.4, 使用 JarHell 检查重复的 jar 文件</p><p>3.5, 初始化 SecurityManager</p><pre><code>// install SM after natives, shutdown hooks, etc.Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings));</code></pre><h3 id="创建-node-节点"><a href="#创建-node-节点" class="headerlink" title="创建 node 节点"></a>创建 node 节点</h3><pre><code>node = new Node(environment) {    @Override    protected void validateNodeBeforeAcceptingRequests(        final BootstrapContext context,        final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException {        BootstrapChecks.check(context, boundTransportAddress, checks);    }};</code></pre><p>4.1, 这里直接贴一下代码（前半部分）</p><pre><code>    protected Node(final Environment environment, Collection&lt;Class&lt;? extends Plugin&gt;&gt; classpathPlugins) {        final List&lt;Closeable&gt; resourcesToClose = new ArrayList&lt;&gt;(); // register everything we need to release in the case of an error        boolean success = false;        {            // use temp logger just to say we are starting. we can&#39;t use it later on because the node name might not be set            Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(environment.settings()));            logger.info(&quot;initializing ...&quot;);        }        try {            originalSettings = environment.settings();            Settings tmpSettings = Settings.builder().put(environment.settings())                .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build();            // create the node environment as soon as possible, to recover the node id and enable logging            try {                nodeEnvironment = new NodeEnvironment(tmpSettings, environment);                resourcesToClose.add(nodeEnvironment);            } catch (IOException ex) {                throw new IllegalStateException(&quot;Failed to create node environment&quot;, ex);            }            final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings);            final String nodeId = nodeEnvironment.nodeId();            tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId);            final Logger logger = Loggers.getLogger(Node.class, tmpSettings);            // this must be captured after the node name is possibly added to the settings            final String nodeName = NODE_NAME_SETTING.get(tmpSettings);            if (hadPredefinedNodeName == false) {                logger.info(&quot;node name derived from node ID [{}]; set [{}] to override&quot;, nodeId, NODE_NAME_SETTING.getKey());            } else {                logger.info(&quot;node name [{}], node ID [{}]&quot;, nodeName, nodeId);            }            final JvmInfo jvmInfo = JvmInfo.jvmInfo();            logger.info(                &quot;version[{}], pid[{}], build[{}/{}/{}/{}], OS[{}/{}/{}], JVM[{}/{}/{}/{}]&quot;,                Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()),                jvmInfo.pid(),                Build.CURRENT.flavor().displayName(),                Build.CURRENT.type().displayName(),                Build.CURRENT.shortHash(),                Build.CURRENT.date(),                Constants.OS_NAME,                Constants.OS_VERSION,                Constants.OS_ARCH,                Constants.JVM_VENDOR,                Constants.JVM_NAME,                Constants.JAVA_VERSION,                Constants.JVM_VERSION);            logger.info(&quot;JVM arguments {}&quot;, Arrays.toString(jvmInfo.getInputArguments()));            warnIfPreRelease(Version.CURRENT, Build.CURRENT.isSnapshot(), logger);            if (logger.isDebugEnabled()) {                logger.debug(&quot;using config [{}], data [{}], logs [{}], plugins [{}]&quot;,                    environment.configFile(), Arrays.toString(environment.dataFiles()), environment.logsFile(), environment.pluginsFile());            }            this.pluginsService = new PluginsService(tmpSettings, environment.configFile(), environment.modulesFile(), environment.pluginsFile(), classpathPlugins);            this.settings = pluginsService.updatedSettings();            localNodeFactory = new LocalNodeFactory(settings, nodeEnvironment.nodeId());            // create the environment based on the finalized (processed) view of the settings            // this is just to makes sure that people get the same settings, no matter where they ask them from            this.environment = new Environment(this.settings, environment.configFile());            Environment.assertEquivalent(environment, this.environment);            final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings);            final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0]));            resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS));            // adds the context to the DeprecationLogger so that it does not need to be injected everywhere            DeprecationLogger.setThreadContext(threadPool.getThreadContext());            resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext()));            final List&lt;Setting&lt;?&gt;&gt; additionalSettings = new ArrayList&lt;&gt;(pluginsService.getPluginSettings());            final List&lt;String&gt; additionalSettingsFilter = new ArrayList&lt;&gt;(pluginsService.getPluginSettingsFilter());            for (final ExecutorBuilder&lt;?&gt; builder : threadPool.builders()) {                additionalSettings.addAll(builder.getRegisteredSettings());            }            client = new NodeClient(settings, threadPool);    ...</code></pre><p>这里进行的主要操作有:</p><ol><li>生命周期Lifecycle设置为 初始化状态 INITIALIZED</li><li>创建一个 NodeEnvironment 对象保存节点环境信息，如各种数据文件的路径</li><li>读取JVM信息</li><li>创建 PluginsService 对象，创建过程中会读取并加载所有的模块和插件</li><li>创建一个最终的 Environment 对象</li><li>创建线程池 ThreadPool 后面各类对象基本都是通过线程来提供服务，这个线程池可以管理各类线程</li><li>创建 节点客户端 NodeClient</li></ol><p><strong>这里重点介绍 PluginsService 和 ThreadPool 这两个类</strong></p><h4 id="PluginsService"><a href="#PluginsService" class="headerlink" title="PluginsService"></a>PluginsService</h4><p>在构造该类对象是传入的参数如下：</p><p><img src="http://image.laijianfeng.org/20180901_175542.png" alt="PluginsService 构造方法的参数"></p><p>在构造方法中加载所有的模块</p><pre><code>Set&lt;Bundle&gt; seenBundles = new LinkedHashSet&lt;&gt;();List&lt;PluginInfo&gt; modulesList = new ArrayList&lt;&gt;();Set&lt;Bundle&gt; modules = getModuleBundles(modulesDirectory); for (Bundle bundle : modules) {   modulesList.add(bundle.plugin);}seenBundles.addAll(modules);/** Get bundles for plugins installed in the given modules directory. */static Set&lt;Bundle&gt; getModuleBundles(Path modulesDirectory) throws IOException {    return findBundles(modulesDirectory, &quot;module&quot;).stream().flatMap(b -&gt; b.bundles().stream()).collect(Collectors.toSet());}</code></pre><p>其中的 Bundle是一个内部类（a “bundle” is a group of plugins in a single classloader）<br>而 PluginInfo 则是 An in-memory representation of the plugin descriptor. 存在内存中的用来描述一个 plugin 的类</p><p>插件加载的实际代码如下：</p><pre><code>    /**     * Reads the plugin descriptor file.     *     * @param path           the path to the root directory for the plugin     * @return the plugin info     * @throws IOException if an I/O exception occurred reading the plugin descriptor     */    public static PluginInfo readFromProperties(final Path path) throws IOException {        final Path descriptor = path.resolve(ES_PLUGIN_PROPERTIES);        final Map&lt;String, String&gt; propsMap;        {            final Properties props = new Properties();            try (InputStream stream = Files.newInputStream(descriptor)) {                props.load(stream);            }            propsMap = props.stringPropertyNames().stream().collect(Collectors.toMap(Function.identity(), props::getProperty));        }        final String name = propsMap.remove(&quot;name&quot;);        if (name == null || name.isEmpty()) {            throw new IllegalArgumentException(                    &quot;property [name] is missing in [&quot; + descriptor + &quot;]&quot;);        }        final String description = propsMap.remove(&quot;description&quot;);        if (description == null) {            throw new IllegalArgumentException(                    &quot;property [description] is missing for plugin [&quot; + name + &quot;]&quot;);        }        final String version = propsMap.remove(&quot;version&quot;);        if (version == null) {            throw new IllegalArgumentException(                    &quot;property [version] is missing for plugin [&quot; + name + &quot;]&quot;);        }        final String esVersionString = propsMap.remove(&quot;elasticsearch.version&quot;);        if (esVersionString == null) {            throw new IllegalArgumentException(                    &quot;property [elasticsearch.version] is missing for plugin [&quot; + name + &quot;]&quot;);        }        final Version esVersion = Version.fromString(esVersionString);        final String javaVersionString = propsMap.remove(&quot;java.version&quot;);        if (javaVersionString == null) {            throw new IllegalArgumentException(                    &quot;property [java.version] is missing for plugin [&quot; + name + &quot;]&quot;);        }        JarHell.checkVersionFormat(javaVersionString);        final String classname = propsMap.remove(&quot;classname&quot;);        if (classname == null) {            throw new IllegalArgumentException(                    &quot;property [classname] is missing for plugin [&quot; + name + &quot;]&quot;);        }        final String extendedString = propsMap.remove(&quot;extended.plugins&quot;);        final List&lt;String&gt; extendedPlugins;        if (extendedString == null) {            extendedPlugins = Collections.emptyList();        } else {            extendedPlugins = Arrays.asList(Strings.delimitedListToStringArray(extendedString, &quot;,&quot;));        }        final String hasNativeControllerValue = propsMap.remove(&quot;has.native.controller&quot;);        final boolean hasNativeController;        if (hasNativeControllerValue == null) {            hasNativeController = false;        } else {            switch (hasNativeControllerValue) {                case &quot;true&quot;:                    hasNativeController = true;                    break;                case &quot;false&quot;:                    hasNativeController = false;                    break;                default:                    final String message = String.format(                            Locale.ROOT,                            &quot;property [%s] must be [%s], [%s], or unspecified but was [%s]&quot;,                            &quot;has_native_controller&quot;,                            &quot;true&quot;,                            &quot;false&quot;,                            hasNativeControllerValue);                    throw new IllegalArgumentException(message);            }        }        if (esVersion.before(Version.V_6_3_0) &amp;&amp; esVersion.onOrAfter(Version.V_6_0_0_beta2)) {            propsMap.remove(&quot;requires.keystore&quot;);        }        if (propsMap.isEmpty() == false) {            throw new IllegalArgumentException(&quot;Unknown properties in plugin descriptor: &quot; + propsMap.keySet());        }        return new PluginInfo(name, description, version, esVersion, javaVersionString,                              classname, extendedPlugins, hasNativeController);    }</code></pre><p>其中的两个常量的值</p><pre><code>    public static final String ES_PLUGIN_PROPERTIES = &quot;plugin-descriptor.properties&quot;;    public static final String ES_PLUGIN_POLICY = &quot;plugin-security.policy&quot;;</code></pre><p>从以上代码可以看出<strong>模块的加载过程</strong>：</p><ol><li>读取模块的配置文件 <code>plugin-descriptor.properties</code>，解析出内容并存储到 Map 中</li><li>分别校验 <code>name</code>, <code>description</code>, <code>version</code>, <code>elasticsearch.version</code>, <code>java.version</code>, <code>classname</code>, <code>extended.plugins</code>, <code>has.native.controller</code>, <code>requires.keystore</code> 这些配置项，缺失或者不按要求则抛出异常</li><li>根据配置项构造一个 PluginInfo 对象返回</li></ol><p>举例：读取出的 aggs-matrix-stats 模块的配置项信息如下</p><p><img src="http://image.laijianfeng.org/20180901_181500.png" alt="读取插件配置文件并解析文件内容"></p><p>加载插件与加载模块调用的是相同的方法</p><h3 id="ThreadPool-线程池"><a href="#ThreadPool-线程池" class="headerlink" title="ThreadPool 线程池"></a>ThreadPool 线程池</h3><p>线程池的构造方法如下：</p><pre><code>    public ThreadPool(final Settings settings, final ExecutorBuilder&lt;?&gt;... customBuilders) {        super(settings);        assert Node.NODE_NAME_SETTING.exists(settings);        final Map&lt;String, ExecutorBuilder&gt; builders = new HashMap&lt;&gt;();        final int availableProcessors = EsExecutors.numberOfProcessors(settings);        final int halfProcMaxAt5 = halfNumberOfProcessorsMaxFive(availableProcessors);        final int halfProcMaxAt10 = halfNumberOfProcessorsMaxTen(availableProcessors);        final int genericThreadPoolMax = boundedBy(4 * availableProcessors, 128, 512);        builders.put(Names.GENERIC, new ScalingExecutorBuilder(Names.GENERIC, 4, genericThreadPoolMax, TimeValue.timeValueSeconds(30)));        builders.put(Names.INDEX, new FixedExecutorBuilder(settings, Names.INDEX, availableProcessors, 200, true));        builders.put(Names.WRITE, new FixedExecutorBuilder(settings, Names.WRITE, &quot;bulk&quot;, availableProcessors, 200));        builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, availableProcessors, 1000));        builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16));        builders.put(Names.SEARCH, new AutoQueueAdjustingExecutorBuilder(settings,                        Names.SEARCH, searchThreadPoolSize(availableProcessors), 1000, 1000, 1000, 2000));        builders.put(Names.MANAGEMENT, new ScalingExecutorBuilder(Names.MANAGEMENT, 1, 5, TimeValue.timeValueMinutes(5)));        // no queue as this means clients will need to handle rejections on listener queue even if the operation succeeded        // the assumption here is that the listeners should be very lightweight on the listeners side        builders.put(Names.LISTENER, new FixedExecutorBuilder(settings, Names.LISTENER, halfProcMaxAt10, -1));        builders.put(Names.FLUSH, new ScalingExecutorBuilder(Names.FLUSH, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5)));        builders.put(Names.REFRESH, new ScalingExecutorBuilder(Names.REFRESH, 1, halfProcMaxAt10, TimeValue.timeValueMinutes(5)));        builders.put(Names.WARMER, new ScalingExecutorBuilder(Names.WARMER, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5)));        builders.put(Names.SNAPSHOT, new ScalingExecutorBuilder(Names.SNAPSHOT, 1, halfProcMaxAt5, TimeValue.timeValueMinutes(5)));        builders.put(Names.FETCH_SHARD_STARTED, new ScalingExecutorBuilder(Names.FETCH_SHARD_STARTED, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5)));        builders.put(Names.FORCE_MERGE, new FixedExecutorBuilder(settings, Names.FORCE_MERGE, 1, -1));        builders.put(Names.FETCH_SHARD_STORE, new ScalingExecutorBuilder(Names.FETCH_SHARD_STORE, 1, 2 * availableProcessors, TimeValue.timeValueMinutes(5)));        for (final ExecutorBuilder&lt;?&gt; builder : customBuilders) {            if (builders.containsKey(builder.name())) {                throw new IllegalArgumentException(&quot;builder with name [&quot; + builder.name() + &quot;] already exists&quot;);            }            builders.put(builder.name(), builder);        }        this.builders = Collections.unmodifiableMap(builders);        threadContext = new ThreadContext(settings);        final Map&lt;String, ExecutorHolder&gt; executors = new HashMap&lt;&gt;();        for (@SuppressWarnings(&quot;unchecked&quot;) final Map.Entry&lt;String, ExecutorBuilder&gt; entry : builders.entrySet()) {            final ExecutorBuilder.ExecutorSettings executorSettings = entry.getValue().getSettings(settings);            final ExecutorHolder executorHolder = entry.getValue().build(executorSettings, threadContext);            if (executors.containsKey(executorHolder.info.getName())) {                throw new IllegalStateException(&quot;duplicate executors with name [&quot; + executorHolder.info.getName() + &quot;] registered&quot;);            }            logger.debug(&quot;created thread pool: {}&quot;, entry.getValue().formatInfo(executorHolder.info));            executors.put(entry.getKey(), executorHolder);        }        executors.put(Names.SAME, new ExecutorHolder(DIRECT_EXECUTOR, new Info(Names.SAME, ThreadPoolType.DIRECT)));        this.executors = unmodifiableMap(executors);        this.scheduler = Scheduler.initScheduler(settings);        TimeValue estimatedTimeInterval = ESTIMATED_TIME_INTERVAL_SETTING.get(settings);        this.cachedTimeThread = new CachedTimeThread(EsExecutors.threadName(settings, &quot;[timer]&quot;), estimatedTimeInterval.millis());        this.cachedTimeThread.start();    }</code></pre><p>参考着文档来理解这里的代码：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html" target="_blank" rel="noopener">Elasticsearch Reference [6.4] » Modules » Thread Pool</a> 和 <a href="http://cwiki.apachecn.org/pages/viewpage.action?pageId=9405389" target="_blank" rel="noopener">apachecn 线程池</a></p><h4 id="线程池类型-ThreadPoolType"><a href="#线程池类型-ThreadPoolType" class="headerlink" title="线程池类型 ThreadPoolType"></a>线程池类型 ThreadPoolType</h4><p><strong>fixed</strong>（固定）：fixed线程池拥有固定数量的线程来处理请求，在没有空闲线程时请求将被挂在队列中。queue_size参数可以控制在没有空闲线程时，能排队挂起的请求数</p><p><strong>fixed_auto_queue_size</strong>：此类型为实验性的，将被更改或删除，不关注</p><p><strong>scaling</strong>（弹性）：scaling线程池拥有的线程数量是动态的，这个数字介于core和max参数的配置之间变化。keep_alive参数用来控制线程在线程池中空闲的最长时间</p><p><strong>direct</strong>：此类线程是一种不支持关闭的线程,就意味着一旦使用,则会一直存活下去.</p><h4 id="一些重要的线程池"><a href="#一些重要的线程池" class="headerlink" title="一些重要的线程池"></a>一些重要的线程池</h4><p><strong>generic</strong>：用于通用的请求（例如：后台节点发现），线程池类型为 scaling。</p><p><strong>index</strong>：用于index/delete请求，线程池类型为 fixed， 大小的为处理器数量，队列大小为200，最大线程数为 1 + 处理器数量。</p><p><strong>search</strong>：用于count/search/suggest请求。线程池类型为 fixed， 大小的为 int((处理器数量 3) / 2) +1，队列大小为1000。*</p><p><strong>get</strong>：用于get请求。线程池类型为 fixed，大小的为处理器数量，队列大小为1000。</p><p><strong>analyze</strong>：用于analyze请求。线程池类型为 fixed，大小的1，队列大小为16</p><p><strong>write</strong>：用于单个文档的 index/delete/update 请求以及 <strong>bulk 请求</strong>，线程池类型为 fixed，大小的为处理器数量，队列大小为200，最大线程数为 1 + 处理器数量。</p><p><strong>snapshot</strong>：用于snaphost/restore请求。线程池类型为 scaling，线程保持存活时间为5分钟，最大线程数为min(5, (处理器数量)/2)。</p><p><strong>warmer</strong>：用于segment warm-up请求。线程池类型为 scaling，线程保持存活时间为5分钟，最大线程数为min(5, (处理器数量)/2)。</p><p><strong>refresh</strong>：用于refresh请求。线程池类型为 scaling，线程空闲保持存活时间为5分钟，最大线程数为min(10, (处理器数量)/2)。</p><p><strong>listener</strong>：主要用于Java客户端线程监听器被设置为true时执行动作。线程池类型为 scaling，最大线程数为min(10, (处理器数量)/2)。</p><p>ThreadPool 类中除了以上线程队列，还可以看到有 CachedTimeThread（缓存系统时间）、ExecutorService（在当前线程上执行提交的任务）、ThreadContext（线程上下文）、ScheduledThreadPoolExecutor（Java任务调度）等</p><blockquote><p>参考文章：<a href="https://my.oschina.net/u/3145136/blog/848079" target="_blank" rel="noopener">Java并发编程14-ScheduledThreadPoolExecutor详解</a><br><a href="https://www.jianshu.com/p/4b8a257f1b90" target="_blank" rel="noopener">Java线程池原理分析ScheduledThreadPoolExecutor篇</a><br>关于 ScheduledThreadPoolExecutor 更多的细节应该看书或者官方文档</p></blockquote><h4 id="关于线程"><a href="#关于线程" class="headerlink" title="关于线程"></a>关于线程</h4><p>了解了线程池，继续深究ES线程是什么样子的</p><p>在 <code>ScalingExecutorBuilder.build</code> 中可以发现 <code>ExecutorService</code> 对象是由 <code>EsExecutors.newScaling</code> 创建的</p><pre><code>public static EsThreadPoolExecutor newScaling(String name, int min, int max, long keepAliveTime, TimeUnit unit, ThreadFactory threadFactory, ThreadContext contextHolder) {    ExecutorScalingQueue&lt;Runnable&gt; queue = new ExecutorScalingQueue&lt;&gt;();    EsThreadPoolExecutor executor = new EsThreadPoolExecutor(name, min, max, keepAliveTime, unit, queue, threadFactory, new ForceQueuePolicy(), contextHolder);    queue.executor = executor;    return executor;}</code></pre><p>再看看 <code>EsThreadPoolExecutor</code> 这个类的继承关系，其是扩展自Java的线程池 <code>ThreadPoolExecutor</code></p><p><img src="http://image.laijianfeng.org/20180901_192545.png" alt="EsThreadPoolExecutor的继承链"></p><pre><code>    EsThreadPoolExecutor(String name, int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit,            BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, XRejectedExecutionHandler handler,            ThreadContext contextHolder) {        super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, handler);        this.name = name;        this.contextHolder = contextHolder;    }</code></pre><h3 id="回到-Node-节点的创建"><a href="#回到-Node-节点的创建" class="headerlink" title="回到 Node 节点的创建"></a>回到 Node 节点的创建</h3><p>4.2, 创建各种服务类对象 ResourceWatcherService、NetworkService、ClusterService、IngestService、ClusterInfoService、UsageService、MonitorService、CircuitBreakerService、MetaStateService、IndicesService、MetaDataIndexUpgradeService、TemplateUpgradeService、TransportService、ResponseCollectorService、SearchTransportService、NodeService、SearchService、PersistentTasksClusterService</p><p>这些服务类是的功能可以根据名称做一个大概的判断，具体还需要看文档和源码，限于篇幅，在此不做探究</p><p>4.3, ModulesBuilder类加入各种模块 ScriptModule、AnalysisModule、SettingsModule、pluginModule、ClusterModule、IndicesModule、SearchModule、GatewayModule、RepositoriesModule、ActionModule、NetworkModule、DiscoveryModule</p><p>4.4, guice 绑定依赖以及依赖注入</p><blockquote><p>关于 guice 可以参考之前的文章:<br><a href="https://mp.weixin.qq.com/s?__biz=MzI1NDU0MTE1NA==&amp;mid=2247483683&amp;idx=1&amp;sn=0d77085a0234b2c5b7c679e62200e6f5&amp;chksm=e9c2ed2edeb56438010b5f5d487bcb7f0529c85d50ac7c858e1a8e3a9279c15007341170c5ac&amp;mpshare=1&amp;scene=1&amp;srcid=0901SWQiIdjHZ3endUHZqjkP#rd" target="_blank" rel="noopener">Google Guice 快速入门</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzI1NDU0MTE1NA==&amp;mid=2247483691&amp;idx=1&amp;sn=3c7175d318bce6728c2105d27ae6bafe&amp;chksm=e9c2ed26deb56430289edabd15cef1a0cf777c5dfe4f4ad5013655e9d3607958e0fe16ac5436&amp;mpshare=1&amp;scene=1&amp;srcid=0901u1aslOFhg6UmeBkLw8BY#rd" target="_blank" rel="noopener">Elasticsearch 中的 Guice</a></p></blockquote><p>elasticsearch里面的组件基本都进行进行了模块化管理，elasticsearch对guice进行了封装，通过ModulesBuilder类构建es的模块（一般包括的模块在 4.3 中列举了）</p><pre><code>// 依赖绑定modules.add(b -&gt; {        b.bind(Node.class).toInstance(this);        b.bind(NodeService.class).toInstance(nodeService);        b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry);        b.bind(PluginsService.class).toInstance(pluginsService);        b.bind(Client.class).toInstance(client);        b.bind(NodeClient.class).toInstance(client);        b.bind(Environment.class).toInstance(this.environment);        b.bind(ThreadPool.class).toInstance(threadPool);        b.bind(NodeEnvironment.class).toInstance(nodeEnvironment);        b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService);        b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService);        b.bind(BigArrays.class).toInstance(bigArrays);        b.bind(ScriptService.class).toInstance(scriptModule.getScriptService());        b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry());        b.bind(IngestService.class).toInstance(ingestService);        b.bind(UsageService.class).toInstance(usageService);        b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);        b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader);        b.bind(MetaStateService.class).toInstance(metaStateService);        b.bind(IndicesService.class).toInstance(indicesService);        b.bind(SearchService.class).toInstance(searchService);        b.bind(SearchTransportService.class).toInstance(searchTransportService);        b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings,            searchService::createReduceContext));        b.bind(Transport.class).toInstance(transport);        b.bind(TransportService.class).toInstance(transportService);        b.bind(NetworkService.class).toInstance(networkService);        b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService()));        b.bind(MetaDataIndexUpgradeService.class).toInstance(metaDataIndexUpgradeService);        b.bind(ClusterInfoService.class).toInstance(clusterInfoService);        b.bind(GatewayMetaState.class).toInstance(gatewayMetaState);        b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery());        {            RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings());            processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings);            b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService,                    indicesService, recoverySettings));            b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool,                    transportService, recoverySettings, clusterService));        }        httpBind.accept(b);        pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p));        b.bind(PersistentTasksService.class).toInstance(persistentTasksService);        b.bind(PersistentTasksClusterService.class).toInstance(persistentTasksClusterService);        b.bind(PersistentTasksExecutorRegistry.class).toInstance(registry);    });injector = modules.createInjector();</code></pre><h3 id="Bootstrap-启动"><a href="#Bootstrap-启动" class="headerlink" title="Bootstrap 启动"></a>Bootstrap 启动</h3><p>5.1， 通过 <code>injector</code> 获取各个类的对象，调用 <code>start()</code> 方法启动（实际进入各个类的中 <code>doStart</code> 方法）: LifecycleComponent、IndicesService、IndicesClusterStateService、SnapshotsService、SnapshotShardsService、RoutingService、SearchService、MonitorService、NodeConnectionsService、ResourceWatcherService、GatewayService、Discovery、TransportService</p><p>这里简要介绍一下各个服务类的职能：</p><p>IndicesService：索引管理<br>IndicesClusterStateService：跨集群同步<br>SnapshotsService：负责创建快照<br>SnapshotShardsService：此服务在数据和主节点上运行，并控制这些节点上当前快照的分片。 它负责启动和停止分片级别快照<br>RoutingService：侦听集群状态，当它收到ClusterChangedEvent（集群改变事件）将验证集群状态，路由表可能会更新<br>SearchService：搜索服务<br>MonitorService：监控<br>NodeConnectionsService：此组件负责在节点添加到群集状态后连接到节点，并在删除它们时断开连接。 此外，它会定期检查所有连接是否仍处于打开状态，并在需要时还原它们。 请注意，如果节点断开/不响应ping，则此组件不负责从群集中删除节点。 这是由NodesFaultDetection完成的。 主故障检测由链接MasterFaultDetection完成。<br>ResourceWatcherService：通用资源观察器服务<br>GatewayService：网关</p><p>如果该节点是主节点或数据节点，还需要进行相关的职能操作</p><p>5.2, 集群发现与监控等，启动 HttpServerTransport， 绑定服务端口</p><pre><code>validateNodeBeforeAcceptingRequests(new BootstrapContext(settings, onDiskMetadata), transportService.boundAddress(), pluginsService    .filterPlugins(Plugin    .class)    .stream()    .flatMap(p -&gt; p.getBootstrapChecks().stream()).collect(Collectors.toList()));clusterService.addStateApplier(transportService.getTaskManager());// start after transport service so the local disco is knowndiscovery.start(); // start before cluster service so that it can set initial state on ClusterApplierServiceclusterService.start();assert clusterService.localNode().equals(localNodeFactory.getNode())    : &quot;clusterService has a different local node than the factory provided&quot;;transportService.acceptIncomingRequests();discovery.startInitialJoin();// tribe nodes don&#39;t have a master so we shouldn&#39;t register an observer         sfinal TimeValue initialStateTimeout = DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings);if (initialStateTimeout.millis() &gt; 0) {    final ThreadPool thread = injector.getInstance(ThreadPool.class);    ClusterState clusterState = clusterService.state();    ClusterStateObserver observer = new ClusterStateObserver(clusterState, clusterService, null, logger, thread.getThreadContext());    if (clusterState.nodes().getMasterNodeId() == null) {        logger.debug(&quot;waiting to join the cluster. timeout [{}]&quot;, initialStateTimeout);        final CountDownLatch latch = new CountDownLatch(1);        observer.waitForNextChange(new ClusterStateObserver.Listener() {            @Override            public void onNewClusterState(ClusterState state) { latch.countDown(); }            @Override            public void onClusterServiceClose() {                latch.countDown();            }            @Override            public void onTimeout(TimeValue timeout) {                logger.warn(&quot;timed out while waiting for initial discovery state - timeout: {}&quot;,                    initialStateTimeout);                latch.countDown();            }        }, state -&gt; state.nodes().getMasterNodeId() != null, initialStateTimeout);        try {            latch.await();        } catch (InterruptedException e) {            throw new ElasticsearchTimeoutException(&quot;Interrupted while waiting for initial discovery state&quot;);        }    }}if (NetworkModule.HTTP_ENABLED.get(settings)) {    injector.getInstance(HttpServerTransport.class).start();}if (WRITE_PORTS_FILE_SETTING.get(settings)) {    if (NetworkModule.HTTP_ENABLED.get(settings)) {        HttpServerTransport http = injector.getInstance(HttpServerTransport.class);        writePortsFile(&quot;http&quot;, http.boundAddress());    }    TransportService transport = injector.getInstance(TransportService.class);    writePortsFile(&quot;transport&quot;, transport.boundAddress());}</code></pre><p>5.3, 启动保活线程 keepAliveThread.start 进行心跳检测</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>过程很漫长，后面很多类的功能未了解，之后补上</p><p>有理解错误的地方请大家多多指教</p><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;本文探究Elasticsearch 6.3.2的启动流程&lt;/p&gt;
&lt;h4 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Google guava工具类的介绍和使用</title>
    <link href="http://laijianfeng.org/2018/08/Google-guava%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://laijianfeng.org/2018/08/Google-guava工具类的介绍和使用/</id>
    <published>2018-08-30T16:58:57.000Z</published>
    <updated>2018-08-30T16:59:59.729Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>工具类 就是封装平常用的方法，不需要你重复造轮子，节省开发人员时间，提高工作效率。谷歌作为大公司，当然会从日常的工作中提取中很多高效率的方法出来。所以就诞生了guava。</p><p>guava的优点：</p><ul><li>高效设计良好的API，被Google的开发者设计，实现和使用</li><li>遵循高效的java语法实践</li><li>使代码更刻度，简洁，简单</li><li>节约时间，资源，提高生产力</li></ul><p>Guava工程包含了若干被Google的 Java项目广泛依赖 的核心库，例如：</p><ul><li>集合 [collections]</li><li>缓存 [caching]</li><li>原生类型支持 [primitives support]</li><li>并发库 [concurrency libraries]</li><li>通用注解 [common annotations]</li><li>字符串处理 [string processing]</li><li>I/O 等等。</li></ul><p>这里借用龙果学院深入浅出Guava课程的一张图</p><p><img src="http://image.laijianfeng.org/20180830_234930.png" alt="龙果学院深入浅出Guava"></p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>引入gradle依赖（引入Jar包）</p><pre><code>compile &#39;com.google.guava:guava:26.0-jre&#39;</code></pre><h4 id="1-集合的创建"><a href="#1-集合的创建" class="headerlink" title="1.集合的创建"></a>1.集合的创建</h4><pre><code>// 普通Collection的创建List&lt;String&gt; list = Lists.newArrayList();Set&lt;String&gt; set = Sets.newHashSet();Map&lt;String, String&gt; map = Maps.newHashMap();// 不变Collection的创建ImmutableList&lt;String&gt; iList = ImmutableList.of(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;);ImmutableSet&lt;String&gt; iSet = ImmutableSet.of(&quot;e1&quot;, &quot;e2&quot;);ImmutableMap&lt;String, String&gt; iMap = ImmutableMap.of(&quot;k1&quot;, &quot;v1&quot;, &quot;k2&quot;, &quot;v2&quot;);</code></pre><p>创建不可变集合 先理解什么是immutable(不可变)对象</p><ul><li>在多线程操作下，是线程安全的</li><li>所有不可变集合会比可变集合更有效的利用资源</li><li>中途不可改变</li></ul><pre><code>ImmutableList&lt;String&gt; immutableList = ImmutableList.of(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;);</code></pre><p>这声明了一个<strong>不可变</strong>的List集合，List中有数据1，2，3，4。类中的 操作集合的方法（譬如add, set, sort, replace等）都被声明过期，并且抛出异常。 而没用guava之前是需要声明并且加各种包裹集合才能实现这个功能</p><pre><code>  // add 方法  @Deprecated @Override  public final void add(int index, E element) {    throw new UnsupportedOperationException();  }</code></pre><p><strong>当我们需要一个map中包含key为String类型，value为List类型的时候</strong>，以前我们是这样写的</p><pre><code>Map&lt;String,List&lt;Integer&gt;&gt; map = new HashMap&lt;String,List&lt;Integer&gt;&gt;();List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();list.add(1);list.add(2);map.put(&quot;aa&quot;, list);System.out.println(map.get(&quot;aa&quot;));//[1, 2]</code></pre><p>而现在</p><pre><code>Multimap&lt;String,Integer&gt; map = ArrayListMultimap.create();        map.put(&quot;aa&quot;, 1);map.put(&quot;aa&quot;, 2);System.out.println(map.get(&quot;aa&quot;));  //[1, 2]</code></pre><p><strong>其他的黑科技集合</strong></p><pre><code>MultiSet: 无序+可重复   count()方法获取单词的次数  增强了可读性+操作简单创建方式:  Multiset&lt;String&gt; set = HashMultiset.create();Multimap: key-value  key可以重复  创建方式: Multimap&lt;String, String&gt; teachers = ArrayListMultimap.create();BiMap: 双向Map(Bidirectional Map) 键与值都不能重复创建方式:  BiMap&lt;String, String&gt; biMap = HashBiMap.create();Table: 双键的Map Map--&gt; Table--&gt;rowKey+columnKey+value  //和sql中的联合主键有点像创建方式: Table&lt;String, String, Integer&gt; tables = HashBasedTable.create();...等等(guava中还有很多java里面没有给出的集合类型)</code></pre><h4 id="2-将集合转换为特定规则的字符串"><a href="#2-将集合转换为特定规则的字符串" class="headerlink" title="2.将集合转换为特定规则的字符串"></a>2.将集合转换为特定规则的字符串</h4><p>以前我们将list转换为特定规则的字符串是这样写的:</p><pre><code>//use javaList&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;aa&quot;);list.add(&quot;bb&quot;);list.add(&quot;cc&quot;);String str = &quot;&quot;;for(int i=0; i&lt;list.size(); i++){    str = str + &quot;-&quot; +list.get(i);}//str 为-aa-bb-cc//use guavaList&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;aa&quot;);list.add(&quot;bb&quot;);list.add(&quot;cc&quot;);String result = Joiner.on(&quot;-&quot;).join(list);//result为  aa-bb-cc</code></pre><p>把map集合转换为特定规则的字符串</p><pre><code>Map&lt;String, Integer&gt; map = Maps.newHashMap();map.put(&quot;xiaoming&quot;, 12);map.put(&quot;xiaohong&quot;,13);String result = Joiner.on(&quot;,&quot;).withKeyValueSeparator(&quot;=&quot;).join(map);// result为 xiaoming=12,xiaohong=13</code></pre><h4 id="3-将String转换为特定的集合"><a href="#3-将String转换为特定的集合" class="headerlink" title="3.将String转换为特定的集合"></a>3.将String转换为特定的集合</h4><pre><code>//use javaList&lt;String&gt; list = new ArrayList&lt;String&gt;();String a = &quot;1-2-3-4-5-6&quot;;String[] strs = a.split(&quot;-&quot;);for(int i=0; i&lt;strs.length; i++){    list.add(strs[i]);}//use guavaString str = &quot;1-2-3-4-5-6&quot;;List&lt;String&gt; list = Splitter.on(&quot;-&quot;).splitToList(str);//list为  [1, 2, 3, 4, 5, 6]</code></pre><p>如果</p><pre><code>str=&quot;1-2-3-4- 5-  6  &quot;;</code></pre><p>guava还可以使用 <code>omitEmptyStrings().trimResults()</code> 去除空串与空格</p><pre><code>String str = &quot;1-2-3-4-  5-  6   &quot;;  List&lt;String&gt; list = Splitter.on(&quot;-&quot;).omitEmptyStrings().trimResults().splitToList(str);System.out.println(list);</code></pre><p><strong>将String转换为map</strong></p><pre><code>String str = &quot;xiaoming=11,xiaohong=23&quot;;Map&lt;String,String&gt; map = Splitter.on(&quot;,&quot;).withKeyValueSeparator(&quot;=&quot;).split(str);</code></pre><h4 id="4-guava还支持多个字符切割，或者特定的正则分隔"><a href="#4-guava还支持多个字符切割，或者特定的正则分隔" class="headerlink" title="4.guava还支持多个字符切割，或者特定的正则分隔"></a>4.guava还支持多个字符切割，或者特定的正则分隔</h4><pre><code>String input = &quot;aa.dd,,ff,,.&quot;;List&lt;String&gt; result = Splitter.onPattern(&quot;[.|,]&quot;).omitEmptyStrings().splitToList(input);</code></pre><p>关于字符串的操作 都是在Splitter这个类上进行的</p><pre><code>// 判断匹配结果boolean result = CharMatcher.inRange(&#39;a&#39;, &#39;z&#39;).or(CharMatcher.inRange(&#39;A&#39;, &#39;Z&#39;)).matches(&#39;K&#39;); //true// 保留数字文本  CharMatcher.digit() 已过时   retain 保留//String s1 = CharMatcher.digit().retainFrom(&quot;abc 123 efg&quot;); //123String s1 = CharMatcher.inRange(&#39;0&#39;, &#39;9&#39;).retainFrom(&quot;abc 123 efg&quot;); // 123// 删除数字文本  remove 删除// String s2 = CharMatcher.digit().removeFrom(&quot;abc 123 efg&quot;);    //abc  efgString s2 = CharMatcher.inRange(&#39;0&#39;, &#39;9&#39;).removeFrom(&quot;abc 123 efg&quot;); // abc  efg</code></pre><h4 id="5-集合的过滤"><a href="#5-集合的过滤" class="headerlink" title="5. 集合的过滤"></a>5. 集合的过滤</h4><p>我们对于集合的过滤，思路就是迭代，然后再具体对每一个数判断，这样的代码放在程序中，难免会显得很臃肿，虽然功能都有，但是很不好看。</p><p>guava写法</p><pre><code>//按照条件过滤ImmutableList&lt;String&gt; names = ImmutableList.of(&quot;begin&quot;, &quot;code&quot;, &quot;Guava&quot;, &quot;Java&quot;);Iterable&lt;String&gt; fitered = Iterables.filter(names, Predicates.or(Predicates.equalTo(&quot;Guava&quot;), Predicates.equalTo(&quot;Java&quot;)));System.out.println(fitered); // [Guava, Java]//自定义过滤条件   使用自定义回调方法对Map的每个Value进行操作ImmutableMap&lt;String, Integer&gt; m = ImmutableMap.of(&quot;begin&quot;, 12, &quot;code&quot;, 15);        // Function&lt;F, T&gt; F表示apply()方法input的类型，T表示apply()方法返回类型        Map&lt;String, Integer&gt; m2 = Maps.transformValues(m, new Function&lt;Integer, Integer&gt;() {            public Integer apply(Integer input) {                if(input&gt;12){                    return input;                }else{                    return input+1;                }            }        });System.out.println(m2);   //{begin=13, code=15}</code></pre><p>set的交集, 并集, 差集</p><pre><code>HashSet setA = newHashSet(1, 2, 3, 4, 5);  HashSet setB = newHashSet(4, 5, 6, 7, 8);  SetView union = Sets.union(setA, setB);    System.out.println(&quot;union:&quot;);  for (Integer integer : union)      System.out.println(integer);           //union 并集:12345867SetView difference = Sets.difference(setA, setB);  System.out.println(&quot;difference:&quot;);  for (Integer integer : difference)      System.out.println(integer);        //difference 差集:123SetView intersection = Sets.intersection(setA, setB);  System.out.println(&quot;intersection:&quot;);  for (Integer integer : intersection)      System.out.println(integer);  //intersection 交集:45</code></pre><p>map的交集，并集，差集</p><pre><code>HashMap&lt;String, Integer&gt; mapA = Maps.newHashMap();mapA.put(&quot;a&quot;, 1);mapA.put(&quot;b&quot;, 2);mapA.put(&quot;c&quot;, 3);HashMap&lt;String, Integer&gt; mapB = Maps.newHashMap();mapB.put(&quot;b&quot;, 20);mapB.put(&quot;c&quot;, 3);mapB.put(&quot;d&quot;, 4);MapDifference differenceMap = Maps.difference(mapA, mapB);differenceMap.areEqual();Map entriesDiffering = differenceMap.entriesDiffering();Map entriesOnlyLeft = differenceMap.entriesOnlyOnLeft();Map entriesOnlyRight = differenceMap.entriesOnlyOnRight();Map entriesInCommon = differenceMap.entriesInCommon();System.out.println(entriesDiffering);   // {b=(2, 20)}System.out.println(entriesOnlyLeft);    // {a=1}System.out.println(entriesOnlyRight);   // {d=4}System.out.println(entriesInCommon);    // {c=3}</code></pre><h4 id="6-检查参数"><a href="#6-检查参数" class="headerlink" title="6.检查参数"></a>6.检查参数</h4><pre><code>//use javaif(list!=null &amp;&amp; list.size()&gt;0)&#39;&#39;&#39;if(str!=null &amp;&amp; str.length()&gt;0)&#39;&#39;&#39;if(str !=null &amp;&amp; !str.isEmpty())//use guavaif(!Strings.isNullOrEmpty(str))//use javaif (count &lt;= 0) {    throw new IllegalArgumentException(&quot;must be positive: &quot; + count);         }    //use guavaPreconditions.checkArgument(count &gt; 0, &quot;must be positive: %s&quot;, count);  </code></pre><p>免去了很多麻烦！并且会使你的代码看上去更好看。而不是代码里面充斥着 <code>!=null</code>， <code>!=&quot;&quot;</code></p><p><strong>检查是否为空,不仅仅是字符串类型，其他类型的判断，全部都封装在 Preconditions类里，里面的方法全为静态</strong></p><p>其中的一个方法的源码</p><pre><code>@CanIgnoreReturnValuepublic static &lt;T&gt; T checkNotNull(T reference) {    if (reference == null) {      throw new NullPointerException();    }    return reference;}</code></pre><table><thead><tr><th>方法声明（不包括额外参数）</th><th>描述</th><th>检查失败时抛出的异常</th></tr></thead><tbody><tr><td>checkArgument(boolean)</td><td>检查boolean是否为true，用来检查传递给方法的参数。</td><td>IllegalArgumentException</td></tr><tr><td>checkNotNull(T)</td><td>检查value是否为null，该方法直接返回value，因此可以内嵌使用checkNotNull。</td><td>NullPointerException</td></tr><tr><td>checkState(boolean)</td><td>用来检查对象的某些状态。</td><td>IllegalStateException</td></tr><tr><td>checkElementIndex(int index, int size)</td><td>检查index作为索引值对某个列表、字符串或数组是否有效。   index &gt; 0 &amp;&amp; index &lt; size</td><td>IndexOutOfBoundsException</td></tr><tr><td>checkPositionIndexes(int start, int end, int size)</td><td>检查[start,end]表示的位置范围对某个列表、字符串或数组是否有效</td><td>IndexOutOfBoundsException</td></tr></tbody></table><h4 id="7-MoreObjects"><a href="#7-MoreObjects" class="headerlink" title="7. MoreObjects"></a>7. MoreObjects</h4><p>这个方法是在Objects过期后官方推荐使用的替代品，该类最大的好处就是不用大量的重写 <code>toString</code>，用一种很优雅的方式实现重写，或者在某个场景定制使用。</p><pre><code>Person person = new Person(&quot;aa&quot;,11);String str = MoreObjects.toStringHelper(&quot;Person&quot;).add(&quot;age&quot;, person.getAge()).toString();System.out.println(str);  //输出Person{age=11}</code></pre><h4 id="8-强大的Ordering排序器"><a href="#8-强大的Ordering排序器" class="headerlink" title="8.强大的Ordering排序器"></a>8.强大的Ordering排序器</h4><p>排序器[Ordering]是Guava流畅风格比较器[Comparator]的实现，它可以用来为构建复杂的比较器，以完成集合排序的功能。</p><pre><code>natural()    对可排序类型做自然排序，如数字按大小，日期按先后排序usingToString()    按对象的字符串形式做字典排序[lexicographical ordering]from(Comparator)    把给定的Comparator转化为排序器reverse()    获取语义相反的排序器nullsFirst()    使用当前排序器，但额外把null值排到最前面。nullsLast()    使用当前排序器，但额外把null值排到最后面。compound(Comparator)    合成另一个比较器，以处理当前排序器中的相等情况。lexicographical()    基于处理类型T的排序器，返回该类型的可迭代对象Iterable&lt;T&gt;的排序器。onResultOf(Function)    对集合中元素调用Function，再按返回值用当前排序器排序。</code></pre><p>示例</p><pre><code>Person person = new Person(&quot;aa&quot;,14);  //String name  ,Integer agePerson ps = new Person(&quot;bb&quot;,13);Ordering&lt;Person&gt; byOrdering = Ordering.natural().nullsFirst().onResultOf(new Function&lt;Person,String&gt;(){    public String apply(Person person){        return person.age.toString();    }});byOrdering.compare(person, ps);System.out.println(byOrdering.compare(person, ps)); //1      person的年龄比ps大 所以输出1</code></pre><h4 id="9-计算中间代码的运行时间"><a href="#9-计算中间代码的运行时间" class="headerlink" title="9.计算中间代码的运行时间"></a>9.计算中间代码的运行时间</h4><pre><code>Stopwatch stopwatch = Stopwatch.createStarted();for(int i=0; i&lt;100000; i++){    // do some thing}long nanos = stopwatch.elapsed(TimeUnit.MILLISECONDS);System.out.println(nanos);</code></pre><p>TimeUnit 可以指定时间输出精确到多少时间</p><h4 id="10-文件操作"><a href="#10-文件操作" class="headerlink" title="10.文件操作"></a>10.文件操作</h4><p>以前我们写文件读取的时候要定义缓冲区，各种条件判断，各种 <code>$%#$@#</code></p><p>而现在我们只需要使用好guava的api 就能使代码变得简洁，并且不用担心因为写错逻辑而背锅了</p><pre><code>File file = new File(&quot;test.txt&quot;);List&lt;String&gt; list = null;try {    list = Files.readLines(file, Charsets.UTF_8);} catch (Exception e) {}Files.copy(from,to);  //复制文件Files.deleteDirectoryContents(File directory); //删除文件夹下的内容(包括文件与子文件夹)  Files.deleteRecursively(File file); //删除文件或者文件夹  Files.move(File from, File to); //移动文件URL url = Resources.getResource(&quot;abc.xml&quot;); //获取classpath根下的abc.xml文件url</code></pre><p>Files类中还有许多方法可以用，可以多多翻阅</p><h4 id="11-guava缓存"><a href="#11-guava缓存" class="headerlink" title="11.guava缓存"></a>11.guava缓存</h4><p>guava的缓存设计的比较巧妙，可以很精巧的使用。guava缓存创建分为两种，一种是CacheLoader,另一种则是callback方式</p><p>CacheLoader:</p><pre><code>LoadingCache&lt;String,String&gt; cahceBuilder=CacheBuilder                .newBuilder()                .build(new CacheLoader&lt;String, String&gt;(){                    @Override                    public String load(String key) throws Exception {                                String strProValue=&quot;hello &quot;+key+&quot;!&quot;;                                        return strProValue;                    }                });        System.out.println(cahceBuilder.apply(&quot;begincode&quot;));  //hello begincode!System.out.println(cahceBuilder.get(&quot;begincode&quot;)); //hello begincode!System.out.println(cahceBuilder.get(&quot;wen&quot;)); //hello wen!System.out.println(cahceBuilder.apply(&quot;wen&quot;)); //hello wen!System.out.println(cahceBuilder.apply(&quot;da&quot;));//hello da!cahceBuilder.put(&quot;begin&quot;, &quot;code&quot;);System.out.println(cahceBuilder.get(&quot;begin&quot;)); //code</code></pre><p>api中已经把apply声明为过期，声明中推荐使用get方法获取值</p><p>callback方式:</p><pre><code> Cache&lt;String, String&gt; cache = CacheBuilder.newBuilder().maximumSize(1000).build();              String resultVal = cache.get(&quot;code&quot;, new Callable&lt;String&gt;() {                  public String call() {                      String strProValue=&quot;begin &quot;+&quot;code&quot;+&quot;!&quot;;                                    return strProValue;                }              });   System.out.println(&quot;value : &quot; + resultVal); //value : begin code!</code></pre><p>以上只是guava使用的一小部分，guava是个大的工具类，第一版guava是2010年发布的，每一版的更新和迭代都是一种创新。</p><p>jdk的升级很多都是借鉴guava里面的思想来进行的。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>代码可以在 <a href="https://github.com/whirlys/elastic-example/tree/master/guava" target="_blank" rel="noopener">https://github.com/whirlys/elastic-example/tree/master/guava</a> 下载</p><p>细节请翻看 guava 文档 <a href="https://github.com/google/guava/wiki" target="_blank" rel="noopener">https://github.com/google/guava/wiki</a></p><blockquote><p>参考：<br><a href="https://my.oschina.net/u/2551035/blog/802634" target="_blank" rel="noopener">Google guava工具类的介绍和使用</a><br><a href="https://blog.csdn.net/ac_dao_di/article/details/53750028" target="_blank" rel="noopener">Guava工具类学习</a></p></blockquote><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;工具类 就是封装平常用的方法，不需要你重复造轮子，节省开发人员时间，提高工作效率。谷歌作为大公司，当然会从日常的工作中提取中很多高效率的方法
      
    
    </summary>
    
      <category term="后端" scheme="http://laijianfeng.org/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="java" scheme="http://laijianfeng.org/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 中的 Guice</title>
    <link href="http://laijianfeng.org/2018/08/Elasticsearch-%E4%B8%AD%E7%9A%84-Guice/"/>
    <id>http://laijianfeng.org/2018/08/Elasticsearch-中的-Guice/</id>
    <published>2018-08-30T16:39:17.000Z</published>
    <updated>2018-08-30T16:40:14.535Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Elasticsearch 源代码中使用了Guice框架进行依赖注入. 为了方便阅读源码, 此处我先通过模仿ES guice的使用方式简单写了一个基本Demo 方便理解, 之后再来理一下ES的Guice使用. 编写的测试类原理图如下:</p><p><img src="http://image.laijianfeng.org/2778947-7847f61cf5180bdc.webp" alt="ES Guice Demo"></p><p>总共有两个Module，一个是ToolModule，<strong>用于绑定</strong>IAnimal接口、ITool接口以及Map对象. 另一个是HumanModule 用于绑定Person对象。   </p><p>其中Person的构造函数通过 <code>@Inject</code> 注解注入其他实例</p><p>gradle 需要引入的 Jar 包</p><pre><code>compile group: &#39;com.google.inject.extensions&#39;, name: &#39;guice-multibindings&#39;, version: &#39;4.2.0&#39;compile group: &#39;com.google.inject&#39;, name: &#39;guice&#39;, version: &#39;4.2.0&#39;</code></pre><h3 id="1、Demo"><a href="#1、Demo" class="headerlink" title="1、Demo"></a>1、Demo</h3><h4 id="iTool接口与实现类"><a href="#iTool接口与实现类" class="headerlink" title="iTool接口与实现类"></a>iTool接口与实现类</h4><pre><code>public interface ITool {    public void doWork();}</code></pre><pre><code>import com.whirly.guice.example.ITool;public class IToolImpl implements ITool {    @Override    public void doWork() {        System.out.println(&quot;use tool to work&quot;);    }}</code></pre><h4 id="IAnimal-接口与实现类"><a href="#IAnimal-接口与实现类" class="headerlink" title="IAnimal 接口与实现类"></a>IAnimal 接口与实现类</h4><pre><code>public interface IAnimal {    void work();}</code></pre><pre><code>public class IAnimalImpl implements IAnimal {    @Override    public void work() {        System.out.println(&quot;animals can also do work&quot;);    }}</code></pre><h4 id="ToolModule的实现-它绑了三个实例"><a href="#ToolModule的实现-它绑了三个实例" class="headerlink" title="ToolModule的实现, 它绑了三个实例"></a>ToolModule的实现, 它绑了三个实例</h4><pre><code>public class ToolModule extends AbstractModule {    @Override    protected void configure() {        //此处注入的实例可以注入到其他类的构造函数中, 只要那个类使用@Inject进行注入即可        bind(IAnimal.class).to(IAnimalImpl.class);        bind(ITool.class).to(IToolImpl.class);        // 注入Map实例        MapBinder&lt;String, String&gt; mapBinder = MapBinder.newMapBinder(binder(), String.class, String.class);        mapBinder.addBinding(&quot;test1&quot;).toInstance(&quot;test1&quot;);        mapBinder.addBinding(&quot;test2&quot;).toInstance(&quot;test2&quot;);    }}</code></pre><p><code>bind(IAnimal.class).to(IAnimalImpl.class);bind(ITool.class).to(IToolImpl.class);</code>  是将接口与其具体实现绑定起来</p><p><code>MapBinder&lt;String,String&gt; mapBinder =MapBinder.newMapBinder(binder(), String.class, String.class);mapBinder.addBinding(&quot;test1&quot;).toInstance(&quot;test1&quot;);mapBinder.addBinding(&quot;test2&quot;).toInstance(&quot;test2&quot;);</code> 则是完成Map的绑定. </p><p>后面来看看Person类和HumanModule</p><h4 id="Person-类"><a href="#Person-类" class="headerlink" title="Person 类"></a>Person 类</h4><pre><code>public class Person {    private IAnimal iAnimal;    private ITool iTool;    private Map&lt;String, String&gt; map;    @Inject    public Person(IAnimal iAnimal, ITool iTool, Map&lt;String, String&gt; map) {        this.iAnimal = iAnimal;        this.iTool = iTool;        this.map = map;    }    public void startwork() {        iTool.doWork();        iAnimal.work();        for (Map.Entry entry : map.entrySet()) {            System.out.println(&quot;注入的map 是 &quot; + entry.getKey() + &quot; value &quot; + entry.getValue());        }    }}</code></pre><p>Person 类中由 <code>IAnimal</code>、<code>ITool</code> 和 <code>Map&lt;String, String&gt;</code> 这三个接口定义的变量，对象将通过 <code>@Inject</code> 从构造方法中注入进来</p><pre><code>public class HumanModule extends AbstractModule {    @Override    protected void configure() {        bind(Person.class).asEagerSingleton();    }}</code></pre><p>Person类的构造函数是通过注入的方式，注入对象实例的</p><p>最后 <code>CustomModuleBuilder</code> 进行<strong>统一管理所有的Module</strong>，实例化所有Module中的对象. 完成依赖注入。</p><p>这里的CustomModuleBuilder是修改自Elasticsearch中的ModulesBuilder，其原理是一样的。</p><p>就是一个迭代器，<strong>内部封装的是Module集合, 统一管理所有的Module</strong></p><h4 id="CustomModuleBuilder-统一管理-Module"><a href="#CustomModuleBuilder-统一管理-Module" class="headerlink" title="CustomModuleBuilder 统一管理 Module"></a>CustomModuleBuilder 统一管理 Module</h4><pre><code>public class CustomModuleBuilder implements Iterable&lt;Module&gt; {    private final List&lt;Module&gt; modules = new ArrayList&lt;&gt;();    public CustomModuleBuilder add(Module... newModules) {        for (Module module : newModules) {            modules.add(module);        }        return this;    }    @Override    public Iterator&lt;Module&gt; iterator() {        return modules.iterator();    }    public Injector createInjector() {        Injector injector = Guice.createInjector(modules);        return injector;    }}</code></pre><p>这样就可以从Main方法是如何进行使用的</p><h4 id="Main-方法"><a href="#Main-方法" class="headerlink" title="Main 方法"></a>Main 方法</h4><pre><code>public class Main {    public static void main(String[] args) {        CustomModuleBuilder moduleBuilder = new CustomModuleBuilder();        moduleBuilder.add(new ToolModule());        moduleBuilder.add(new HumanModule());        Injector injector = moduleBuilder.createInjector();        Person person = injector.getInstance(Person.class);        person.startwork();    }}</code></pre><p>运行结果</p><pre><code>use tool to workanimals can also do work注入的map 是 test1 value test1注入的map 是 test2 value test2</code></pre><p>通过CustomModuleBuilder 的createInjector获取Injector 对象, 根据Injector 对象取相应的具体实例对象.</p><h3 id="2、ES-中Guice的使用"><a href="#2、ES-中Guice的使用" class="headerlink" title="2、ES 中Guice的使用"></a>2、ES 中Guice的使用</h3><p>ES中TransportClient初始化时的Guice的使用是这样的, 如下图所示</p><p><img src="http://image.laijianfeng.org/2778947-ab2035e865492a2b.png" alt="ES中TransportClient初始化时的Guice的使用（ES版本不是6.3.2）"></p><h4 id="TransportClient的初始化代码"><a href="#TransportClient的初始化代码" class="headerlink" title="TransportClient的初始化代码"></a>TransportClient的初始化代码</h4><p>Elasticsearch 6.3.2 </p><pre><code>private static ClientTemplate buildTemplate(Settings providedSettings, Settings defaultSettings,                                            Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins, HostFailureListener failureListner) {    // 省略 ...    try {        // 省略 ...        // 创建一个迭代器, 然后将各个Module通过add方法加入进去        ModulesBuilder modules = new ModulesBuilder();        // plugin modules must be added here, before others or we can get crazy injection errors...        for (Module pluginModule : pluginsService.createGuiceModules()) {            modules.add(pluginModule);        }        modules.add(b -&gt; b.bind(ThreadPool.class).toInstance(threadPool));        ActionModule actionModule = new ActionModule(true, settings, null, settingsModule.getIndexScopedSettings(),                settingsModule.getClusterSettings(), settingsModule.getSettingsFilter(), threadPool,                pluginsService.filterPlugins(ActionPlugin.class), null, null, null);        modules.add(actionModule);        CircuitBreakerService circuitBreakerService = Node.createCircuitBreakerService(settingsModule.getSettings(),            settingsModule.getClusterSettings());        resourcesToClose.add(circuitBreakerService);        PageCacheRecycler pageCacheRecycler = new PageCacheRecycler(settings);        BigArrays bigArrays = new BigArrays(pageCacheRecycler, circuitBreakerService);        resourcesToClose.add(bigArrays);        modules.add(settingsModule);        NetworkModule networkModule = new NetworkModule(settings, true, pluginsService.filterPlugins(NetworkPlugin.class), threadPool,            bigArrays, pageCacheRecycler, circuitBreakerService, namedWriteableRegistry, xContentRegistry, networkService, null);        final Transport transport = networkModule.getTransportSupplier().get();        final TransportService transportService = new TransportService(settings, transport, threadPool,            networkModule.getTransportInterceptor(),            boundTransportAddress -&gt; DiscoveryNode.createLocal(settings, new TransportAddress(TransportAddress.META_ADDRESS, 0),                UUIDs.randomBase64UUID()), null, Collections.emptySet());        modules.add((b -&gt; {            b.bind(BigArrays.class).toInstance(bigArrays);            b.bind(PluginsService.class).toInstance(pluginsService);            b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService);            b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry);            b.bind(Transport.class).toInstance(transport);            b.bind(TransportService.class).toInstance(transportService);            b.bind(NetworkService.class).toInstance(networkService);        }));        // 注入所有module下的实例        Injector injector = modules.createInjector();        final TransportClientNodesService nodesService =            new TransportClientNodesService(settings, transportService, threadPool, failureListner == null                ? (t, e) -&gt; {} : failureListner);        // construct the list of client actions        final List&lt;ActionPlugin&gt; actionPlugins = pluginsService.filterPlugins(ActionPlugin.class);        final List&lt;GenericAction&gt; clientActions =                actionPlugins.stream().flatMap(p -&gt; p.getClientActions().stream()).collect(Collectors.toList());        // add all the base actions        final List&lt;? extends GenericAction&lt;?, ?&gt;&gt; baseActions =                actionModule.getActions().values().stream().map(ActionPlugin.ActionHandler::getAction).collect(Collectors.toList());        clientActions.addAll(baseActions);        final TransportProxyClient proxy = new TransportProxyClient(settings, transportService, nodesService, clientActions);        List&lt;LifecycleComponent&gt; pluginLifecycleComponents = new ArrayList&lt;&gt;(pluginsService.getGuiceServiceClasses().stream()            .map(injector::getInstance).collect(Collectors.toList()));        resourcesToClose.addAll(pluginLifecycleComponents);        // 启动服务        transportService.start();        transportService.acceptIncomingRequests();        ClientTemplate transportClient = new ClientTemplate(injector, pluginLifecycleComponents, nodesService, proxy, namedWriteableRegistry);        resourcesToClose.clear();        return transportClient;    } finally {        IOUtils.closeWhileHandlingException(resourcesToClose);    }}</code></pre><p>可以看到确实是先通 <code>过ModulesBuilder modules = new ModulesBuilder()</code> 创建一个迭代器, 然后将各个Module通过add方法加入进去, 最后通过 <code>Injector injector = modules.createInjector();</code> 创建Injector对象, <strong>之后便可根据Injector对象去获取实例了</strong>. </p><p>各个Module会绑定自己所需要的实例, 这里以 SettingsModule 举例:</p><pre><code>public class SettingsModule extends AbstractModule {    private final Settings settings;    private final Set&lt;String&gt; settingsFilterPattern = new HashSet&lt;&gt;();    private final Map&lt;String, Setting&lt;?&gt;&gt; nodeSettings = new HashMap&lt;&gt;();    private final Map&lt;String, Setting&lt;?&gt;&gt; indexSettings = new HashMap&lt;&gt;();    private final Logger logger;    private final IndexScopedSettings indexScopedSettings;    private final ClusterSettings clusterSettings;    private final SettingsFilter settingsFilter;    public SettingsModule(Settings settings, Setting&lt;?&gt;... additionalSettings) {        this(settings, Arrays.asList(additionalSettings), Collections.emptyList());    }    @Override    public void configure(Binder binder) {        binder.bind(Settings.class).toInstance(settings);        binder.bind(SettingsFilter.class).toInstance(settingsFilter);        binder.bind(ClusterSettings.class).toInstance(clusterSettings);        binder.bind(IndexScopedSettings.class).toInstance(indexScopedSettings);    }    //...}</code></pre><p>可以看到它绑定了四个,分别是 Settings.class，SettingsFilter.class，ClusterSettings.class，IndexScopedSettings.class</p><p>它们的实例对象都可以通过Injector来获取</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>示例代码可在 <a href="https://github.com/whirlys/elastic-example/tree/master/guice" target="_blank" rel="noopener">https://github.com/whirlys/elastic-example/tree/master/guice</a> 处下载</p><blockquote><p>参考：<br>kason_zhang <a href="https://www.jianshu.com/p/0a1e6267b46f" target="_blank" rel="noopener">Elasticsearch Guice 的使用</a></p></blockquote><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;Elasticsearch 源代码中使用了Guice框架进行依赖注入. 为了方便阅读源码, 此处我先通过模仿ES guice的使用方式简单写
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>鼓舞人心</title>
    <link href="http://laijianfeng.org/2018/08/%E9%BC%93%E8%88%9E%E4%BA%BA%E5%BF%83/"/>
    <id>http://laijianfeng.org/2018/08/鼓舞人心/</id>
    <published>2018-08-27T11:51:31.000Z</published>
    <updated>2018-08-27T11:52:52.757Z</updated>
    
    <content type="html"><![CDATA[<ol><li>所有的伟大，源于一个勇敢的开始</li></ol><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;所有的伟大，源于一个勇敢的开始&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E
      
    
    </summary>
    
      <category term="读书" scheme="http://laijianfeng.org/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>Elasticsearch 分布式特性</title>
    <link href="http://laijianfeng.org/2018/08/Elasticsearch-%E5%88%86%E5%B8%83%E5%BC%8F%E7%89%B9%E6%80%A7/"/>
    <id>http://laijianfeng.org/2018/08/Elasticsearch-分布式特性/</id>
    <published>2018-08-25T10:29:15.000Z</published>
    <updated>2018-08-25T10:32:23.330Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文的主要内容：</p><ul><li>分布式介绍及cerebro</li><li>构建集群</li><li>副本与分片</li><li>集群状态与故障转移</li><li>文档分布式存储</li><li>脑裂问题</li><li>shard详解</li></ul><h3 id="分布式介绍及cerebro"><a href="#分布式介绍及cerebro" class="headerlink" title="分布式介绍及cerebro"></a>分布式介绍及cerebro</h3><p>ES支持集群模式，是一个分布式系统，其好处主要有两个：</p><ul><li>增大系统容量，如内存、磁盘，使得ES集群可以支持PB级的数据</li><li>提高系统可用性，即使部分节点停止服务，整个集群依然可以正常服务</li></ul><p>ES集群由多个ES实例组成</p><ul><li>不同集群通过集群名称来区分，可通过cluster.name进行修改，名称默认为elasticsearch</li><li>每个ES实例本质上是一个JVM进程，且有自己的名字，通过node.name进行修改</li></ul><h4 id="cerebro"><a href="#cerebro" class="headerlink" title="cerebro"></a>cerebro</h4><p>cerebro 是一个ES Web管理工具，项目地址 <a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">https://github.com/lmenezes/cerebro</a> </p><p>其配置文件为 conf/application.conf，启动 cerebro ，默认监听的地址为 0.0.0.0:9000</p><pre><code>bin/cerebro# 也可指定监听ip和端口号bin/cerebro -Dhttp.port=1234 -Dhttp.address=127.0.0.1</code></pre><p>访问 <a href="http://yourhost:9000" target="_blank" rel="noopener">http://yourhost:9000</a> ，填写要监控的 ES 地址：<a href="http://eshost:9200" target="_blank" rel="noopener">http://eshost:9200</a> 即可进入管理界面</p><p><img src="http://image.laijianfeng.org/20180825_150701.png" alt="cerebro管理界面"></p><p><img src="http://image.laijianfeng.org/20180825_151133.png" alt="cerebro 节点信息"></p><p><img src="http://image.laijianfeng.org/20180825_151323.png" alt="cerebro 集群配置"></p><p>在cerebro管理界面中我们可以看到 ES节点、索引、shard的分布、集群参数配置等多种信息</p><h3 id="构建集群"><a href="#构建集群" class="headerlink" title="构建集群"></a>构建集群</h3><p>如果只有一台机器，可以执行下面的命令，每次指定相同的集群名称，不同的节点名称和端口，即可在同一台机器上启动多个ES节点</p><pre><code>bin/elasticsearch -Ecluster.name=my_cluster -Enode.name=node1 -Ehttp.port=9200 -d</code></pre><p>作者的是在 virtualbox 上安装Ubuntu虚拟机，在安装好开发环境，正常启动ES之后，采取复制虚拟机的做法，复制后需要修改虚拟机的UUID，做法可自行上网搜索。</p><p>作者复制了两个，准备构建一个拥有三个ES节点的集群。启动虚拟机后可以进行关闭防火墙，配置hosts以使相互之间能够通过主机名访问，配置ssh免密访问等操作</p><p>分别修改ES节点中的 <code>cluster.name</code> 为相同名称，<code>node.name</code> 为各自的主机名，<code>network.host</code> 为 <code>0.0.0.0</code>，<code>discovery.zen.ping.unicast.hosts</code> 列表中中加入各自的 <code>node.name</code></p><p>在ES主目录下执行命令启动ES</p><pre><code>bin/elasticsearch</code></pre><p>查看日志可见集群搭建完毕</p><h4 id="Cluster-State-集群状态"><a href="#Cluster-State-集群状态" class="headerlink" title="Cluster State 集群状态"></a>Cluster State 集群状态</h4><p>与ES集群相关的数据称为cluster state，主要记录如下信息：</p><ul><li>节点信息，比如节点名称、连接地址等</li><li>索引信息，比如索引名称，配置等</li><li>其他。。</li></ul><h4 id="Master-Node-主节点"><a href="#Master-Node-主节点" class="headerlink" title="Master Node 主节点"></a>Master Node 主节点</h4><ul><li>可以修改cluster state的节点成为master节点，一个集群<strong>只能有一个</strong></li><li>cluster state存储在每个节点上，master维护最新版本并<strong>同步</strong>给其他节点</li><li>master节点是通过集群中所有节点<strong>选举产生</strong>的，可以<strong>被选举</strong>的节点成为master-eligible（候选）节点，相关配置如下：<code>node.master: true</code></li></ul><h4 id="Coordinating-Node"><a href="#Coordinating-Node" class="headerlink" title="Coordinating Node"></a>Coordinating Node</h4><ul><li><strong>处理请求</strong>的节点即为coordinating节点，该节点为所有节点的默认角色，不能取消</li><li>路由请求到正确的节点处理，比如创建索引的请求到master节点</li></ul><h4 id="Data-Node-数据节点"><a href="#Data-Node-数据节点" class="headerlink" title="Data Node 数据节点"></a>Data Node 数据节点</h4><ul><li><strong>存储数据</strong>的节点即为Data节点，默认节点都是data类型，相关配置如下：<code>node.data: true</code></li></ul><h3 id="副本与分片"><a href="#副本与分片" class="headerlink" title="副本与分片"></a>副本与分片</h3><h4 id="提高系统可用性"><a href="#提高系统可用性" class="headerlink" title="提高系统可用性"></a>提高系统可用性</h4><p>提高系统可用性可从两个方面考虑：服务可用性和数据可用性</p><p>服务可用性：</p><ul><li>2个节点的情况下，允许其中1个节点停止服务</li></ul><p>数据可用性</p><ul><li>引入副本（Replication）解决</li><li>每个节点上都有完备的数据</li></ul><h4 id="增大系统容量"><a href="#增大系统容量" class="headerlink" title="增大系统容量"></a>增大系统容量</h4><p>如何将数据分布于所有节点上？</p><ul><li>引入分片（shard）解决问题</li></ul><p>分片是ES支持PB级数据的基石</p><ul><li>分片存储了部分数据，可以分布于任意节点上</li><li>分片数在索引创建时指定且后续不允许再修改，默认为5个</li><li>分片有主分片和副本分片之分，以实现数据的高可用</li><li>副本分片的数据由主分片同步，可以有多个，从而提高读取的吞吐量</li></ul><h4 id="分片的分布"><a href="#分片的分布" class="headerlink" title="分片的分布"></a>分片的分布</h4><p>下图演示的是 3 个节点的集群中test_index的分片分布情况，创建时我们指定了3个分片和副本</p><pre><code>PUT test_index{  &quot;settings&quot;: {    &quot;number_of_replicas&quot;: 1,    &quot;number_of_shards&quot;: 3  }}</code></pre><p><img src="http://image.laijianfeng.org/20180825_164121.png" alt="主副分片的分布"></p><p>大致是均匀分布，实验中如果由于磁盘空间不足导致有分片未分配，为了测试可以将集群设置 <code>cluster.routing.allocation.disk.threshold_enabled</code> 设置为 false</p><ul><li><strong>此时增加节点是否能提高索引的数据容量？</strong></li></ul><p>不能，因为已经设置了分片数为 3 ，shard的数量已经确定，新增的节点无法利用，</p><ul><li><strong>此时增加副本数能否提高索引的读取吞吐量？</strong></li></ul><p>不能，因为新增的副本分片也是分布在这 3 台节点上，利用了同样的资源（CPU，内存，IO等）。如果要增加吞吐量，同时还需要增加节点的数量</p><ul><li><strong>分片数的设定很重要，需要提前规划好</strong><ul><li>过小会导致后续无法通过增加节点实现水平扩容</li><li>过大会导致一个节点上分布过多分片，造成资源浪费，同时会影响查询性能</li><li>shard的数量的确定：一般建议一个shard的数据量不要超过 <code>30G</code>，shard数量最小为 2</li></ul></li></ul><h3 id="Cluster-Health-集群健康"><a href="#Cluster-Health-集群健康" class="headerlink" title="Cluster Health 集群健康"></a>Cluster Health 集群健康</h3><p>通过如下API可以查看集群健康状况，状态status包括以下三种：</p><ul><li>green 健康状态，指所有主副分片都正常分配</li><li>yellow 指所有主分片都正常分配，但有副本分片未正常分配</li><li>red 有主分片未分配</li></ul><pre><code>GET _cluster/health# 结果{  &quot;cluster_name&quot;: &quot;elasticsearch&quot;,  &quot;status&quot;: &quot;yellow&quot;,  &quot;timed_out&quot;: false,  &quot;number_of_nodes&quot;: 1,  &quot;number_of_data_nodes&quot;: 1,  &quot;active_primary_shards&quot;: 115,  &quot;active_shards&quot;: 115,  &quot;relocating_shards&quot;: 0,  &quot;initializing_shards&quot;: 0,  &quot;unassigned_shards&quot;: 111,  &quot;delayed_unassigned_shards&quot;: 0,  &quot;number_of_pending_tasks&quot;: 0,  &quot;number_of_in_flight_fetch&quot;: 0,  &quot;task_max_waiting_in_queue_millis&quot;: 0,  &quot;active_shards_percent_as_number&quot;: 50.88495575221239}</code></pre><h4 id="Failover-故障转移"><a href="#Failover-故障转移" class="headerlink" title="Failover 故障转移"></a>Failover 故障转移</h4><p>集群由 3 个节点组成，名称分别为 master，Hadoop2，Hadoop3， 其中 master 为主节点，集群状态status为 green</p><p><img src="http://image.laijianfeng.org/20180825_174406.png" alt="集群状态green"></p><p><strong>如果此时 master 所在机器宕机导致服务终止，此时集群如何处理？</strong></p><p>Hadoop2 和 Hadoop3 发现 master 无法响应一段时间后会发起 master 主节点选举，比如这里选择 Hadoop2 为 master 节点。由于此时主分片 P0 和 P2 下线，集群状态变为 Red</p><p><img src="http://image.laijianfeng.org/20180825_174933.png" alt="节点master宕机"></p><p>node2 发现主分片 P0 和 P2 未分配，将 R0 和 R2 提升为主分片，此时由于所有主分片都正常分配，集群状态变为 yellow</p><p><img src="http://image.laijianfeng.org/20180825_175028.png" alt="image"></p><p>Hadoop2 为 P0 和 P2 生成新的副本，集群状态变为绿色</p><p><img src="http://image.laijianfeng.org/20180825_175235.png" alt="image"></p><p>最后看看 Hadoop2 打印的日志</p><p><img src="http://image.laijianfeng.org/20180825_175517.png" alt="image"></p><h3 id="文档分布式存储"><a href="#文档分布式存储" class="headerlink" title="文档分布式存储"></a>文档分布式存储</h3><p>文档最终会存储在分片上。文档选择分片需要文档到分片的<strong>映射算法</strong>，目的是使得文档均匀分布在所有分片上，以充分利用资源。</p><p>算法：</p><ul><li>随机选择或者round-robin算法？不可取，因为需要维护文档到分片的映射关系，成本巨大</li><li><strong>根据文档值实时计算对应的分片</strong></li></ul><h4 id="文档到分片的映射算法"><a href="#文档到分片的映射算法" class="headerlink" title="文档到分片的映射算法"></a>文档到分片的映射算法</h4><p>ES通过如下的公式计算文档对应的分片</p><ul><li><code>shard = hash(routing) % number_of_primary_shards</code></li><li>hash算法保证可以将数据均匀地分散在分片中</li><li>routing是一个关键参数，默认是文档id，也可以自行指定</li><li>number_of_primary_shards是主分片数</li></ul><p>该算法与主分片数相关，这也是分片数一旦确定后便不能更改的原因</p><h4 id="文档创建流程"><a href="#文档创建流程" class="headerlink" title="文档创建流程"></a>文档创建流程</h4><ol><li>Client向node3发起创建文档的请求</li><li>node3通过routing计算该文档应该存储在shard1上，查询cluster state后确认主分片P1在node2上，然后转发创建文档的请求到node2</li><li>P1 接收并执行创建文档请求后，将同样的请求发送到副本分片R1</li><li>R1接收并执行创建文档请求后，通知P1成功的结果</li><li>P1接收副本分片结果后，通知node3创建成功</li><li>node3返回结果到Client</li></ol><p><img src="http://image.laijianfeng.org/20180825_181026.png" alt="文档创建流程"></p><h4 id="文档读取流程"><a href="#文档读取流程" class="headerlink" title="文档读取流程"></a>文档读取流程</h4><ol><li>Client向node3发起获取文档1的请求</li><li>node3通过routing计算该文档在shard1上，查询cluster state后获取shard1的主副分片列表，然后以轮询的机制获取一个shard，比如这里是R1，然后转发读取文档的请求到node1</li><li>R1接收并执行读取文档请求后，将结果返回node3</li><li>node3返回结果给client</li></ol><p><img src="http://image.laijianfeng.org/20180825_181511.png" alt="文档读取流程"></p><h4 id="文档批量创建的流程"><a href="#文档批量创建的流程" class="headerlink" title="文档批量创建的流程"></a>文档批量创建的流程</h4><ol><li>client向node3发起批量创建文档的请求（bulk）</li><li>node3通过routing计算所有文档对应的shard，然后按照主shard分配对应执行的操作，同时发送请求到涉及的主shard，比如这里3个主shard都需要参与</li><li>主shard接收并执行请求后，将同样的请求同步到对应的副本shard</li><li>副本shard执行结果后返回到主shard，主shard再返回node3</li><li>node3整合结果后返回client</li></ol><p><img src="http://image.laijianfeng.org/20180825_181725.png" alt="文档批量创建的流程 bulk"></p><h4 id="文档批量读取的流程"><a href="#文档批量读取的流程" class="headerlink" title="文档批量读取的流程"></a>文档批量读取的流程</h4><ol><li>client向node3发起批量获取所有文档的请求（mget）</li><li>node3通过routing计算所有文档对应的shard，然后通过轮询的机制获取要参与shard，按照shard投建mget请求，通过发送请求到涉及shard，比如这里有2个shard需要参与</li><li>R1，R2返回文档结果</li><li>node3返回结果给client</li></ol><p><img src="http://image.laijianfeng.org/20180825_182023.png" alt="文档批量读取的流程 mget"></p><h3 id="脑裂问题"><a href="#脑裂问题" class="headerlink" title="脑裂问题"></a>脑裂问题</h3><p>脑裂问题，英文为split-brain，是分布式系统中的经典网络问题，如下图所示：</p><p>3个节点组成的集群，突然node1的网络和其他两个节点中断<br><img src="http://image.laijianfeng.org/20180807_1234.png" alt="image"></p><p>node2与node3会重新选举master，比如node2成为了新的master，此时会更新cluster state</p><p>node1自己组成集群后，也更新cluster state</p><p>同一个集群有两个master，而且维护不同的cluster state，网络恢复后无法选择正确的master</p><p><img src="http://image.laijianfeng.org/20180807_1235.png" alt="image"></p><p><strong>解决方案</strong>为仅在可选举master-eligible节点数大于等于quorum时才可以进行master选举</p><ul><li><code>quorum = master-eligible节点数/2 + 1</code>，例如3个master-eligible节点时，quorum 为2 </li><li>设定 <code>discovery.zen.minimun_master_nodes</code> 为 <code>quorum</code> 即可避免脑裂问题</li></ul><p><img src="http://image.laijianfeng.org/20180807_1236.png" alt="image"></p><h3 id="倒排索引的不可变更"><a href="#倒排索引的不可变更" class="headerlink" title="倒排索引的不可变更"></a>倒排索引的不可变更</h3><p>倒排索引一旦生成，不能更改<br>其好处如下：</p><ul><li>不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题</li><li>由于文件不再更改，可以充分利用文件系统缓存，只需载入一次，只要内存足够，对该文件的读取都会从内存读取，性能高</li><li>利于生成缓存数据</li><li>利于对文件进行压缩存储，节省磁盘和内存存储空间</li></ul><p>坏处为需要写入新文档时，必须重新构建倒排索引文件，然后替换老文件后，新文档才能被检索，导致文档实时性差</p><h3 id="文档搜索实时性"><a href="#文档搜索实时性" class="headerlink" title="文档搜索实时性"></a>文档搜索实时性</h3><p><strong>解决方案</strong>是新文档直接生成新的倒排索引文件，查询的时候同时查询所有的倒排文件，然后做结果的汇总计算即可</p><p>Lucene便是采用了这种方案，它构建的单个倒排索引称为segment，合在一起称为index，与ES中的Index概念不同，ES中的一个shard对应一个Lucene Index</p><p>Lucene会有一个专门的文件来记录所有的segment信息，称为commit point<br><img src="http://image.laijianfeng.org/20180807_1237.png" alt="image"></p><h4 id="refresh"><a href="#refresh" class="headerlink" title="refresh"></a>refresh</h4><p>segment写入磁盘的过程依然很耗时，可以借助文件系统缓存的特性，现将segment在缓存中创建并开放查询来进一步提升实时性，该过程在ES中被称为<code>refresh</code></p><p>在refresh之前文档会先存储在一个buffer中，refresh时将buffer中的所有文档清空并生成segment</p><p>ES默认每1秒执行一次refresh，因此文档的实时性被提高到1秒，这也是ES被称为 近实时(Near Real Time)的原因<br><img src="http://image.laijianfeng.org/20180807_1238.png" alt="image"></p><h4 id="translog"><a href="#translog" class="headerlink" title="translog"></a>translog</h4><p>如果在内存中的segment还没有写入磁盘前发生了宕机，那么其中的文档就无法恢复了，如何解决这个问题呢？</p><ul><li>ES引入translog机制，写入文档到buffer时，同时将该操作写入translog</li><li>translog文件会即时写入磁盘(fsync)，6.x默认每个请求都会落盘</li></ul><p><img src="http://image.laijianfeng.org/20180807_1345.png" alt="image"></p><h4 id="flush"><a href="#flush" class="headerlink" title="flush"></a>flush</h4><p>flush负责将内存中的segment写入磁盘，主要做成如下的工作：</p><ul><li>将translog写入磁盘</li><li>将index buffer清空，其中的文档生成一个新的segment，相当于一个refresh操作</li><li>更新commit point并写入磁盘</li><li>执行fsync操作，将内存中的segment写入磁盘</li><li>删除旧的translog文件</li></ul><p><img src="http://image.laijianfeng.org/20180807_1239.png" alt="image"></p><p>flush发生的时机主要有如下几种情况：</p><ul><li>间隔时间达到时，默认是30分钟，5.x之前可以通过<code>index.translog.flush_threshold_period</code>修改，之后无法修改</li><li>translog占满时，其大小可以通过<code>index.translog.flush_threshold_size</code>控制，默认是512mb，每个index有自己的translog</li></ul><h4 id="refresh-1"><a href="#refresh-1" class="headerlink" title="refresh"></a>refresh</h4><p>refresh发生的时机主要有如下几种情况：</p><ul><li>间隔时间达到时，通过<code>index.settings.refresh_interval</code>来设定，默认是1秒</li><li><code>index.buffer</code>占满时，其大小通过<code>indices.memory.index_buffer_size</code>设置，默认为JVM heap的10%，所有shard共享</li><li>flush发生时也会发生refresh</li></ul><h4 id="删除与更新文档"><a href="#删除与更新文档" class="headerlink" title="删除与更新文档"></a>删除与更新文档</h4><p>segment一旦生成就不能更改，那么如果你要删除文档该如何操作？</p><ul><li>Lucene专门维护一个<code>.del</code>文件，记录所有已经删除的文档，注意<code>.del</code>上记录的是文档在Lucene内部的id</li><li>在查询结果返回前会过滤掉<code>.del</code>中所有的文档</li></ul><p>要更新文档如何进行呢？</p><ul><li>首先删除文档，然后再创建新文档</li></ul><h4 id="整体视角"><a href="#整体视角" class="headerlink" title="整体视角"></a>整体视角</h4><p>ES Index与Lucene Index的术语对照如下所示：<br><img src="http://image.laijianfeng.org/20180807_1346.png" alt="image"></p><h4 id="Segment-Merging"><a href="#Segment-Merging" class="headerlink" title="Segment Merging"></a>Segment Merging</h4><p>随着segment的增多，由于一次查询的segment数增多，查询速度会变慢<br>ES会定时在后台进行segment merge的操作，减少segment的数量<br>通过force_merge api可以手动强制做segment merge的操作</p><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;本文的主要内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式介绍及cerebro&lt;/li&gt;
&lt;li&gt;构建集群&lt;/li&gt;
&lt;li&gt;副本与分片&lt;/li&gt;

      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>23个最有用的ES检索技巧(Java API实现)</title>
    <link href="http://laijianfeng.org/2018/08/23%E4%B8%AA%E6%9C%80%E6%9C%89%E7%94%A8%E7%9A%84ES%E6%A3%80%E7%B4%A2%E6%8A%80%E5%B7%A7-Java-API%E5%AE%9E%E7%8E%B0/"/>
    <id>http://laijianfeng.org/2018/08/23个最有用的ES检索技巧-Java-API实现/</id>
    <published>2018-08-25T05:19:00.000Z</published>
    <updated>2018-08-25T05:21:32.553Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文是对 <code>23个最有用的Elasticseaerch检索技巧</code> 一文提到的ES检索技巧进行 Java API 的简单实现，但仅限于简单实现，并不考虑包括参数校验，异常处理，日志处理，安全等问题，仅供参考</p><p>代码见 <a href="https://github.com/whirlys/elastic-example/tree/master/UsefullESSearchSkill" target="_blank" rel="noopener">UsefullESSearchSkill</a> ,<strong>原查询语句请对照原文</strong></p><h4 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h4><p>JDK version : 10.0.2<br>gradle version : 4.7<br>Elasticsearch version : 6.3.2<br>IDEA version : 2018.2</p><p>运行前请启动 ES 实例，并修改 <code>application.properties</code> 文件中的ES配置</p><h3 id="类介绍"><a href="#类介绍" class="headerlink" title="类介绍"></a>类介绍</h3><h4 id="实体类-Book"><a href="#实体类-Book" class="headerlink" title="实体类 Book"></a>实体类 Book</h4><p>注意：日期 publish_date 的类型设置为 String 是避免 Java 到 ES 之间复杂的转换工作，在ES中该字段仍然被识别为 date 类型</p><pre><code>public class Book {    public static SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);    private String id;    private String title;    private List&lt;String&gt; authors;    private String summary;    private String publish_date;    private Integer num_reviews;    private String publisher;    ...}</code></pre><h4 id="公共类-Constants"><a href="#公共类-Constants" class="headerlink" title="公共类 Constants"></a>公共类 Constants</h4><p>定义了一些常用的常量</p><pre><code>public class Constants {    // 字段名    public static String ID = &quot;id&quot;;    public static String TITLE = &quot;title&quot;;    public static String AUTHORS = &quot;authors&quot;;    public static String SUMMARY = &quot;summary&quot;;    public static String PUBLISHDATE = &quot;publish_date&quot;;    public static String PUBLISHER = &quot;publisher&quot;;    public static String NUM_REVIEWS = &quot;num_reviews&quot;;    public static String TITLE_KEYWORD = &quot;title.keyword&quot;;    public static String PUBLISHER_KEYWORD = &quot;publisher.keyword&quot;;    // 过滤要返回的字段    public static String[] fetchFieldsTSPD = {ID, TITLE, SUMMARY, PUBLISHDATE};    public static String[] fetchFieldsTA = {ID, TITLE, AUTHORS};    public static String[] fetchFieldsSA = {ID, SUMMARY, AUTHORS};    public static String[] fetchFieldsTSA = {ID, TITLE, SUMMARY, AUTHORS};    public static String[] fetchFieldsTPPD = {ID, TITLE, PUBLISHER, PUBLISHDATE};    public static String[] fetchFieldsTSPN = {ID, TITLE, SUMMARY, PUBLISHER, NUM_REVIEWS};    // 高亮    public static HighlightBuilder highlightS = new HighlightBuilder().field(SUMMARY);}</code></pre><h4 id="公共类-EsConfig"><a href="#公共类-EsConfig" class="headerlink" title="公共类 EsConfig"></a>公共类 EsConfig</h4><p>创建 ES 客户端实例，ES 客户端用于与 ES 集群进行交互</p><pre><code>@Configurationpublic class EsConfig {    @Value(&quot;${elasticsearch.cluster-nodes}&quot;)    private String clusterNodes;    @Value(&quot;${elasticsearch.cluster-name}&quot;)    private String clusterName;    @Bean    public Client client() {        Settings settings = Settings.builder().put(&quot;cluster.name&quot;, clusterName)                .put(&quot;client.transport.sniff&quot;, true).build();        TransportClient client = new PreBuiltTransportClient(settings);        try {            if (clusterNodes != null &amp;&amp; !&quot;&quot;.equals(clusterNodes)) {                for (String node : clusterNodes.split(&quot;,&quot;)) {                    String[] nodeInfo = node.split(&quot;:&quot;);                    client.addTransportAddress(new TransportAddress(InetAddress.getByName(nodeInfo[0]), Integer.parseInt(nodeInfo[1])));                }            }        } catch (UnknownHostException e) {        }        return client;    }}</code></pre><h4 id="数据获取工具类-DataUtil"><a href="#数据获取工具类-DataUtil" class="headerlink" title="数据获取工具类 DataUtil"></a>数据获取工具类 DataUtil</h4><p>这里的数据也就是 <code>23个最有用的ES检索技巧</code> 文中用于实验的4条数据</p><pre><code>public class DataUtil {    public static SimpleDateFormat dateFormater = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);    /**     * 模拟获取数据     */    public static List&lt;Book&gt; batchData() {        List&lt;Book&gt; list = new LinkedList&lt;&gt;();        Book book1 = new Book(&quot;1&quot;, &quot;Elasticsearch: The Definitive Guide&quot;, Arrays.asList(&quot;clinton gormley&quot;, &quot;zachary tong&quot;),                &quot;A distibuted real-time search and analytics engine&quot;, &quot;2015-02-07&quot;, 20, &quot;oreilly&quot;);        Book book2 = new Book(&quot;2&quot;, &quot;Taming Text: How to Find, Organize, and Manipulate It&quot;, Arrays.asList(&quot;grant ingersoll&quot;, &quot;thomas morton&quot;, &quot;drew farris&quot;),                &quot;organize text using approaches such as full-text search, proper name recognition, clustering, tagging, information extraction, and summarization&quot;,                &quot;2013-01-24&quot;, 12, &quot;manning&quot;);        Book book3 = new Book(&quot;3&quot;, &quot;Elasticsearch in Action&quot;, Arrays.asList(&quot;radu gheorge&quot;, &quot;matthew lee hinman&quot;, &quot;roy russo&quot;),                &quot;build scalable search applications using Elasticsearch without having to do complex low-level programming or understand advanced data science algorithms&quot;,                &quot;2015-12-03&quot;, 18, &quot;manning&quot;);        Book book4 = new Book(&quot;4&quot;, &quot;Solr in Action&quot;, Arrays.asList(&quot;trey grainger&quot;, &quot;timothy potter&quot;), &quot;Comprehensive guide to implementing a scalable search engine using Apache Solr&quot;,                &quot;2014-04-05&quot;, 23, &quot;manning&quot;);        list.add(book1);        list.add(book2);        list.add(book3);        list.add(book4);        return list;    }    public static Date parseDate(String dateStr) {        try {            return dateFormater.parse(dateStr);        } catch (ParseException e) {        }        return null;    }</code></pre><h4 id="公共查询工具类-CommonQueryUtils"><a href="#公共查询工具类-CommonQueryUtils" class="headerlink" title="公共查询工具类 CommonQueryUtils"></a>公共查询工具类 CommonQueryUtils</h4><p>对执行完ES查询请求后的数据进行解析</p><pre><code>public class CommonQueryUtils {    public static Gson gson = new GsonBuilder().setDateFormat(&quot;YYYY-MM-dd&quot;).create();    /**     * 处理ES返回的数据，封装     */    public static List&lt;Book&gt; parseResponse(SearchResponse searchResponse) {        List&lt;Book&gt; list = new LinkedList&lt;&gt;();        for (SearchHit hit : searchResponse.getHits().getHits()) {            // 用gson直接解析            Book book = gson.fromJson(hit.getSourceAsString(), Book.class);            list.add(book);        }        return list;    }    /**     * 解析完数据后，构建 Response 对象     */    public static Response&lt;List&lt;Book&gt;&gt; buildResponse(SearchResponse searchResponse) {        // 超时处理        if (searchResponse.isTimedOut()) {            return new Response&lt;&gt;(ResponseCode.ESTIMEOUT);        }        // 处理ES返回的数据        List&lt;Book&gt; list = parseResponse(searchResponse);        // 有shard执行失败        if (searchResponse.getFailedShards() &gt; 0) {            return new Response&lt;&gt;(ResponseCode.FAILEDSHARDS, list);        }        return new Response&lt;&gt;(ResponseCode.OK, list);    }    ...}</code></pre><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><h4 id="BulkTests"><a href="#BulkTests" class="headerlink" title="BulkTests"></a>BulkTests</h4><p>创建索引，以及使用 bulk API 批量插入数据</p><pre><code>@RunWith(SpringRunner.class)@SpringBootTestpublic class BulkTests {    // 在Test中 Autowired需要引入包 org.elasticsearch.plugin:transport-netty4-client:6.3.2，否则异常找不到Transport类    @Autowired    private Client client;    @Value(&quot;${elasticsearch.bookIndex}&quot;)    private String bookIndex;    @Value(&quot;${elasticsearch.bookType}&quot;)    private String bookType;    private Gson gson = new GsonBuilder().setDateFormat(&quot;YYYY-MM-dd&quot;).create();    /**     * 创建索引，设置 settings，设置mappings     */    @Test    public void createIndex() {        int settingShards = 1;        int settingReplicas = 0;        // 判断索引是否存在，存在则删除        IndicesExistsResponse indicesExistsResponse = client.admin().indices().prepareExists(bookIndex).get();        if (indicesExistsResponse.isExists()) {            System.out.println(&quot;索引 &quot; + bookIndex + &quot; 存在！&quot;);            // 删除索引，防止报异常  ResourceAlreadyExistsException[index [bookdb_index/yL05ZfXFQ4GjgOEM5x8tFQ] already exists            DeleteIndexResponse deleteResponse = client.admin().indices().prepareDelete(bookIndex).get();            if (deleteResponse.isAcknowledged()){                System.out.println(&quot;索引&quot; + bookIndex + &quot;已删除&quot;);            }else {                System.out.println(&quot;索引&quot; + bookIndex + &quot;删除失败&quot;);            }        } else {            System.out.println(&quot;索引 &quot; + bookIndex + &quot; 不存在！&quot;);        }        // 设置Settings        CreateIndexResponse response = client.admin().indices().prepareCreate(bookIndex)                .setSettings(Settings.builder()                        .put(&quot;index.number_of_shards&quot;, settingShards)                        .put(&quot;index.number_of_replicas&quot;, settingReplicas))                .get();        // 查看结果        GetSettingsResponse getSettingsResponse = client.admin().indices()                .prepareGetSettings(bookIndex).get();        System.out.println(&quot;索引设置结果&quot;);        for (ObjectObjectCursor&lt;String, Settings&gt; cursor : getSettingsResponse.getIndexToSettings()) {            String index = cursor.key;            Settings settings = cursor.value;            Integer shards = settings.getAsInt(&quot;index.number_of_shards&quot;, null);            Integer replicas = settings.getAsInt(&quot;index.number_of_replicas&quot;, null);            System.out.println(&quot;index:&quot; + index + &quot;, shards:&quot; + shards + &quot;, replicas:&quot; + replicas);            Assert.assertEquals(java.util.Optional.of(settingShards), java.util.Optional.of(shards));            Assert.assertEquals(java.util.Optional.of(settingReplicas), java.util.Optional.of(replicas));        }    }    /**     * Bulk 批量插入数据     */    @Test    public void bulk() {        List&lt;Book&gt; list = DateUtil.batchData();        BulkRequestBuilder bulkRequestBuilder = client.prepareBulk();        // 添加index操作到 bulk 中        list.forEach(book -&gt; {            // 新版的API中使用setSource时，参数的个数必须是偶数，否则需要加上 setSource(json, XContentType.JSON)            bulkRequestBuilder.add(client.prepareIndex(bookIndex, bookType, book.getId()).setSource(gson.toJson(book), XContentType.JSON));        });        BulkResponse responses = bulkRequestBuilder.get();        if (responses.hasFailures()) {            // bulk有失败            for (BulkItemResponse res : responses) {                System.out.println(res.getFailure());            }            Assert.assertTrue(false);        }    }}</code></pre><h3 id="开始查询"><a href="#开始查询" class="headerlink" title="开始查询"></a>开始查询</h3><h4 id="控制类"><a href="#控制类" class="headerlink" title="控制类"></a>控制类</h4><p>查询接口</p><pre><code>@RestController@RequestMapping(&quot;basicmatch&quot;)public class BasicMatchQueryController {    @Autowired    private BasicMatchQueryService basicMatchQueryService;    /**     * 1.1 对 &quot;guide&quot; 执行全文检索     * 测试：http://localhost:8080/basicmatch/multimatch?query=guide     */    @RequestMapping(&quot;multimatch&quot;)    public Response&lt;List&lt;Book&gt;&gt; multiMatch(@RequestParam(value = &quot;query&quot;, required = true) String query) {        return basicMatchQueryService.multiBatch(query);    }    /**     * 1.2 指定特定字段检索     * 测试：http://localhost:8080/basicmatch/match?title=in action&amp;from=0&amp;size=4     */    @RequestMapping(&quot;match&quot;)    public ResponsePage&lt;List&lt;Book&gt;&gt; match(MatchForm form) {        return basicMatchQueryService.match(form);    }    /**     * 2 对 &quot;guide&quot; 执行多字段检索     * 测试：http://localhost:8080/basicmatch/multifield?query=guide     */    @RequestMapping(&quot;multifield&quot;)    public Response&lt;List&lt;Book&gt;&gt; multiField(@RequestParam(value = &quot;query&quot;, required = true) String query) {        return basicMatchQueryService.multiField(query);    }    /**     * 3、 Boosting提升某字段得分的检索( Boosting): 将“摘要”字段的得分提高了3倍     * 测试：http://localhost:8080/basicmatch/multifieldboost?query=elasticsearch guide     */    @RequestMapping(&quot;multifieldboost&quot;)    public Response&lt;List&lt;Book&gt;&gt; multiFieldboost(@RequestParam(value = &quot;query&quot;, required = true) String query) {        return basicMatchQueryService.multiFieldboost(query);    }    /**     * 4、Bool检索( Bool Query)     * 测试：http://localhost:8080/basicmatch/bool?shouldTitles=Elasticsearch&amp;shouldTitles=Solr&amp;mustAuthors=clinton gormely&amp;mustNotAuthors=radu gheorge     */    @RequestMapping(&quot;bool&quot;)    public Response&lt;List&lt;Book&gt;&gt; bool(@ModelAttribute BoolForm form) {        return basicMatchQueryService.bool(form);    }    /**     * 5、 Fuzzy 模糊检索( Fuzzy Queries)     */    @RequestMapping(&quot;fuzzy&quot;)    public Response&lt;List&lt;Book&gt;&gt; fuzzy(String query) {        return basicMatchQueryService.fuzzy(query);    }    /**     * 6、 Wildcard Query 通配符检索     * 测试：http://localhost:8080/basicmatch/wildcard?pattern=t*     */    @RequestMapping(&quot;wildcard&quot;)    public Response&lt;List&lt;Book&gt;&gt; wildcard(String pattern) {        return basicMatchQueryService.wildcard(Constants.AUTHORS, pattern);    }    /**     * 7、正则表达式检索( Regexp Query)     * 测试：http://localhost:8080/basicmatch/regexp     */    @RequestMapping(&quot;regexp&quot;)    public Response&lt;List&lt;Book&gt;&gt; regexp(String regexp) {        // 由于Tomcat的原因，直接接收有特殊字符的 正则表达式 会异常，所以这里写死，不过多探究        // 若        regexp = &quot;t[a-z]*y&quot;;        return basicMatchQueryService.regexp(Constants.AUTHORS, regexp);    }    /**     * 8、匹配短语检索( Match Phrase Query)     * 测试：http://localhost:8080/basicmatch/phrase?query=search engine     */    @RequestMapping(&quot;phrase&quot;)    public Response&lt;List&lt;Book&gt;&gt; phrase(String query) {        return basicMatchQueryService.phrase(query);    }    /**     * 9、匹配词组前缀检索     * 测试：http://localhost:8080/basicmatch/phraseprefix?query=search en     */    @RequestMapping(&quot;phraseprefix&quot;)    public Response&lt;List&lt;Book&gt;&gt; phrasePrefix(String query) {        return basicMatchQueryService.phrasePrefix(query);    }    /**     * 10、字符串检索（ Query String）     * 测试：http://localhost:8080/basicmatch/querystring?query=(saerch~1 algorithm~1) AND (grant ingersoll)  OR (tom morton)     */    @RequestMapping(&quot;querystring&quot;)    public Response&lt;List&lt;Book&gt;&gt; queryString(String query) {        return basicMatchQueryService.queryString(query);    }    /**     * 11、简化的字符串检索 （Simple Query String）     * 测试：http://localhost:8080/basicmatch/simplequerystring?query=(saerch~1 algorithm~1) AND (grant ingersoll)  OR (tom morton)     */    @RequestMapping(&quot;simplequerystring&quot;)    public Response&lt;List&lt;Book&gt;&gt; simplequerystring(String query) {        // 这里写死，仅为测试        query = &quot;(saerch~1 algorithm~1) + (grant ingersoll)  | (tom morton)&quot;;        return basicMatchQueryService.simpleQueryString(query);    }    /**     * 12、Term=检索（指定字段检索）     * 测试：http://localhost:8080/basicmatch/term?query=manning     */    @RequestMapping(&quot;term&quot;)    public Response&lt;List&lt;Book&gt;&gt; term(String query) {        return basicMatchQueryService.term(query);    }    /**     * 13、Term排序检索-（Term Query - Sorted）     * 测试：http://localhost:8080/basicmatch/termsort?query=manning     */    @RequestMapping(&quot;termsort&quot;)    public Response&lt;List&lt;Book&gt;&gt; termsort(String query) {        return basicMatchQueryService.termsort(query);    }    /**     * 14、范围检索（Range query）     * 测试：http://localhost:8080/basicmatch/range?startDate=2015-01-01&amp;endDate=2015-12-31     */    @RequestMapping(&quot;range&quot;)    public Response&lt;List&lt;Book&gt;&gt; range(String startDate, String endDate) {        return basicMatchQueryService.range(startDate, endDate);    }    /**     * 15. 过滤检索     * 测试：http://localhost:8080/basicmatch/filter?query=elasticsearch&amp;gte=20     */    @RequestMapping(&quot;filter&quot;)    public Response&lt;List&lt;Book&gt;&gt; filter(String query, Integer gte, Integer lte) {        return basicMatchQueryService.filter(query, gte, lte);    }    /**     * 17、 Function 得分：Field值因子（ Function Score: Field Value Factor）     * 测试：http://localhost:8080/basicmatch/fieldvaluefactor?query=search engine     */    @RequestMapping(&quot;fieldvaluefactor&quot;)    public Response&lt;List&lt;Book&gt;&gt; fieldValueFactor(String query) {        return basicMatchQueryService.fieldValueFactor(query);    }    /**     * 18、 Function 得分：衰减函数( Function Score: Decay Functions )     * 测试：http://localhost:8080/basicmatch/decay?query=search engines&amp;origin=2014-06-15     */    @RequestMapping(&quot;decay&quot;)    public Response&lt;List&lt;Book&gt;&gt; decay(String query, @RequestParam(value = &quot;origin&quot;, defaultValue = &quot;2014-06-15&quot;) String origin) {        return basicMatchQueryService.decay(query, origin);    }    /**     * 19、Function得分：脚本得分（ Function Score: Script Scoring ）     * 测试：ES需要配置允许groovy脚本运行才可以     */    @RequestMapping(&quot;script&quot;)    public Response&lt;List&lt;Book&gt;&gt; script(String query, @RequestParam(value = &quot;threshold&quot;, defaultValue = &quot;2015-07-30&quot;) String threshold) {        return basicMatchQueryService.script(query, threshold);    }}</code></pre><h3 id="服务类"><a href="#服务类" class="headerlink" title="服务类"></a>服务类</h3><pre><code>@Servicepublic class BasicMatchQueryService {    @Autowired    private Client client;    @Value(&quot;${elasticsearch.bookIndex}&quot;)    private String bookIndex;    @Value(&quot;${elasticsearch.bookType}&quot;)    private String bookType;    /**     * 进行ES查询，执行请求前后打印出 查询语句 和 查询结果     */    private SearchResponse requestGet(String queryName, SearchRequestBuilder requestBuilder) {        System.out.println(queryName + &quot; 构建的查询：&quot; + requestBuilder.toString());        SearchResponse searchResponse = requestBuilder.get();        System.out.println(queryName + &quot; 搜索结果：&quot; + searchResponse.toString());        return searchResponse;    }    ...}</code></pre><h4 id="1-1-对-“guide”-执行全文检索-Multi-Match-Query"><a href="#1-1-对-“guide”-执行全文检索-Multi-Match-Query" class="headerlink" title="1.1 对 “guide” 执行全文检索 Multi Match Query"></a>1.1 对 “guide” 执行全文检索 Multi Match Query</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; multiBatch(String query) {        MultiMatchQueryBuilder queryBuilder = new MultiMatchQueryBuilder(query);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(queryBuilder);        SearchResponse searchResponse = requestGet(&quot;multiBatch&quot;, requestBuilder);        return CommonQueryUtils.buildResponse(searchResponse);    }</code></pre><h4 id="1-2-在标题字段-title-中搜索带有-“in-action”-字样的图书"><a href="#1-2-在标题字段-title-中搜索带有-“in-action”-字样的图书" class="headerlink" title="1.2 在标题字段(title)中搜索带有 “in action” 字样的图书"></a>1.2 在标题字段(title)中搜索带有 “in action” 字样的图书</h4><pre><code>    public ResponsePage&lt;List&lt;Book&gt;&gt; match(MatchForm form) {        MatchQueryBuilder matchQueryBuilder = new MatchQueryBuilder(&quot;title&quot;, form.getTitle());        // 高亮        HighlightBuilder highlightBuilder = new HighlightBuilder().field(&quot;title&quot;).fragmentSize(200);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(matchQueryBuilder)                .setFrom(form.getFrom()).setSize(form.getSize())                .highlighter(highlightBuilder)                // 设置 _source 要返回的字段                .setFetchSource(Constants.fetchFieldsTSPD, null);        ...    }</code></pre><h4 id="多字段检索-Multi-field-Search"><a href="#多字段检索-Multi-field-Search" class="headerlink" title="多字段检索 (Multi-field Search)"></a>多字段检索 (Multi-field Search)</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; multiField(String query) {        MultiMatchQueryBuilder queryBuilder = new MultiMatchQueryBuilder(query).field(&quot;title&quot;).field(&quot;summary&quot;);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(queryBuilder);        ...    }</code></pre><h4 id="3、-Boosting提升某字段得分的检索-Boosting-将“摘要”字段的得分提高了3倍"><a href="#3、-Boosting提升某字段得分的检索-Boosting-将“摘要”字段的得分提高了3倍" class="headerlink" title="3、 Boosting提升某字段得分的检索( Boosting),将“摘要”字段的得分提高了3倍"></a>3、 Boosting提升某字段得分的检索( Boosting),将“摘要”字段的得分提高了3倍</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; multiFieldboost(String query) {        MultiMatchQueryBuilder queryBuilder = new MultiMatchQueryBuilder(query).field(&quot;title&quot;).field(&quot;summary&quot;, 3);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(queryBuilder);        ...    }</code></pre><h4 id="4、Bool检索-Bool-Query"><a href="#4、Bool检索-Bool-Query" class="headerlink" title="4、Bool检索( Bool Query)"></a>4、Bool检索( Bool Query)</h4><pre><code>    /**     * 在标题中搜索一本名为 &quot;Elasticsearch&quot; 或 &quot;Solr&quot; 的书，     * AND由 &quot;clinton gormley&quot; 创作，但NOT由 &quot;radu gheorge&quot; 创作     */    public Response&lt;List&lt;Book&gt;&gt; bool(BoolForm form) {        BoolQueryBuilder boolQuery = new BoolQueryBuilder();        // 搜索标题 should        BoolQueryBuilder shouldTitleBool = new BoolQueryBuilder();        form.getShouldTitles().forEach(title -&gt; {            shouldTitleBool.should().add(new MatchQueryBuilder(&quot;title&quot;, title));        });        boolQuery.must().add(shouldTitleBool);        // match 作者        form.getMustAuthors().forEach(author -&gt; {            boolQuery.must().add(new MatchQueryBuilder(&quot;authors&quot;, author));        });        // not match 作者        form.getMustNotAuthors().forEach(author -&gt; {            boolQuery.mustNot().add(new MatchQueryBuilder(&quot;authors&quot;, author));        });        ...    }</code></pre><h4 id="5、-Fuzzy-模糊检索-Fuzzy-Queries"><a href="#5、-Fuzzy-模糊检索-Fuzzy-Queries" class="headerlink" title="5、 Fuzzy 模糊检索( Fuzzy Queries)"></a>5、 Fuzzy 模糊检索( Fuzzy Queries)</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; fuzzy(String query) {        MultiMatchQueryBuilder queryBuilder = new MultiMatchQueryBuilder(query)                .field(&quot;title&quot;).field(&quot;summary&quot;)                .fuzziness(Fuzziness.AUTO);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(queryBuilder)                .setFetchSource(Constants.fetchFieldsTSPD, null)                .setSize(2);        ...    }</code></pre><h4 id="6、-Wildcard-Query-通配符检索"><a href="#6、-Wildcard-Query-通配符检索" class="headerlink" title="6、 Wildcard Query 通配符检索"></a>6、 Wildcard Query 通配符检索</h4><pre><code>    /**     * 要查找具有以 &quot;t&quot; 字母开头的作者的所有记录     */    public Response&lt;List&lt;Book&gt;&gt; wildcard(String fieldName, String pattern) {        WildcardQueryBuilder wildcardQueryBuilder = new WildcardQueryBuilder(fieldName, pattern);        HighlightBuilder highlightBuilder = new HighlightBuilder().field(Constants.AUTHORS, 200);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setTypes(bookType).setQuery(wildcardQueryBuilder)                .setFetchSource(Constants.fetchFieldsTA, null)                .highlighter(highlightBuilder);    }</code></pre><h4 id="7、正则表达式检索-Regexp-Query"><a href="#7、正则表达式检索-Regexp-Query" class="headerlink" title="7、正则表达式检索( Regexp Query)"></a>7、正则表达式检索( Regexp Query)</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; regexp(String fieldName, String regexp) {        RegexpQueryBuilder queryBuilder = new RegexpQueryBuilder(fieldName, regexp);        HighlightBuilder highlightBuilder = new HighlightBuilder().field(Constants.AUTHORS);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex)                .setQuery(queryBuilder).setTypes(bookType).highlighter(highlightBuilder)                .setFetchSource(Constants.fetchFieldsTA, null);    }</code></pre><h4 id="8、匹配短语检索-Match-Phrase-Query"><a href="#8、匹配短语检索-Match-Phrase-Query" class="headerlink" title="8、匹配短语检索( Match Phrase Query)"></a>8、匹配短语检索( Match Phrase Query)</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; phrase(String query) {        MultiMatchQueryBuilder queryBuilder = new MultiMatchQueryBuilder(query)                .field(Constants.TITLE).field(Constants.SUMMARY)                .type(MultiMatchQueryBuilder.Type.PHRASE).slop(3);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder)                .setFetchSource(Constants.fetchFieldsTSPD, null);    }</code></pre><h4 id="9、匹配词组前缀检索"><a href="#9、匹配词组前缀检索" class="headerlink" title="9、匹配词组前缀检索"></a>9、匹配词组前缀检索</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; phrasePrefix(String query) {        MatchPhrasePrefixQueryBuilder queryBuilder = new MatchPhrasePrefixQueryBuilder(Constants.SUMMARY, query)                .slop(3).maxExpansions(10);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSPD, null);    }</code></pre><h4 id="10、字符串检索（-Query-String）"><a href="#10、字符串检索（-Query-String）" class="headerlink" title="10、字符串检索（ Query String）"></a>10、字符串检索（ Query String）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; queryString(String query) {        QueryStringQueryBuilder queryBuilder = new QueryStringQueryBuilder(query);        queryBuilder.field(Constants.SUMMARY, 2).field(Constants.TITLE)                .field(Constants.AUTHORS).field(Constants.PUBLISHER);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSA, null);    }</code></pre><h4 id="11、简化的字符串检索-（Simple-Query-String）"><a href="#11、简化的字符串检索-（Simple-Query-String）" class="headerlink" title="11、简化的字符串检索 （Simple Query String）"></a>11、简化的字符串检索 （Simple Query String）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; simpleQueryString(String query) {        SimpleQueryStringBuilder queryBuilder = new SimpleQueryStringBuilder(query);        queryBuilder.field(Constants.SUMMARY, 2).field(Constants.TITLE)                .field(Constants.AUTHORS).field(Constants.PUBLISHER);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSA, null)                .highlighter(Constants.highlightS);    }</code></pre><h4 id="12、Term-Terms检索（指定字段检索）"><a href="#12、Term-Terms检索（指定字段检索）" class="headerlink" title="12、Term/Terms检索（指定字段检索）"></a>12、Term/Terms检索（指定字段检索）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; term(String query) {        TermQueryBuilder termQueryBuilder = new TermQueryBuilder(Constants.PUBLISHER, query);        // terms 查询        /*String[] values = {&quot;manning&quot;, &quot;oreilly&quot;};        TermsQueryBuilder termsQueryBuilder = new TermsQueryBuilder(Constants.PUBLISHER, values);*/        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(termQueryBuilder)                .setFetchSource(Constants.fetchFieldsTPPD, null);    }</code></pre><h4 id="13、Term排序检索-（Term-Query-Sorted）"><a href="#13、Term排序检索-（Term-Query-Sorted）" class="headerlink" title="13、Term排序检索-（Term Query - Sorted）"></a>13、Term排序检索-（Term Query - Sorted）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; termsort(String query) {        TermQueryBuilder termQueryBuilder = new TermQueryBuilder(Constants.PUBLISHER, query);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(termQueryBuilder)                .addSort(Constants.PUBLISHER_KEYWORD, SortOrder.DESC)                .addSort(Constants.TITLE_KEYWORD, SortOrder.ASC)                .setFetchSource(Constants.fetchFieldsTPPD, null);    }</code></pre><h4 id="14、范围检索（Range-query）"><a href="#14、范围检索（Range-query）" class="headerlink" title="14、范围检索（Range query）"></a>14、范围检索（Range query）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; range(String startDate, String endDate) {        RangeQueryBuilder queryBuilder = new RangeQueryBuilder(Constants.PUBLISHDATE)                .gte(startDate).lte(endDate);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder)                .setFetchSource(Constants.fetchFieldsTPPD, null);    }</code></pre><h4 id="15-过滤检索"><a href="#15-过滤检索" class="headerlink" title="15. 过滤检索"></a>15. 过滤检索</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; filter(String query, Integer gte, Integer lte) {        BoolQueryBuilder queryBuilder = new BoolQueryBuilder();        queryBuilder.must().add(new MultiMatchQueryBuilder(query).field(Constants.TITLE).field(Constants.SUMMARY));        if (gte != null || lte != null) {            RangeQueryBuilder rangeQueryBuilder = new RangeQueryBuilder(Constants.NUM_REVIEWS);            if (gte != null) {                rangeQueryBuilder.gte(gte);            }            if (lte != null) {                rangeQueryBuilder.lte(lte);            }            queryBuilder.filter().add(rangeQueryBuilder);        }        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSPN, null);    }</code></pre><h4 id="17、-Function-得分：Field值因子（-Function-Score-Field-Value-Factor）"><a href="#17、-Function-得分：Field值因子（-Function-Score-Field-Value-Factor）" class="headerlink" title="17、 Function 得分：Field值因子（ Function Score: Field Value Factor）"></a>17、 Function 得分：Field值因子（ Function Score: Field Value Factor）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; fieldValueFactor(String query) {        // query        MultiMatchQueryBuilder multiMatchQueryBuilder = new MultiMatchQueryBuilder(query)                .field(Constants.TITLE).field(Constants.SUMMARY);        // fieldValueFactor        FieldValueFactorFunctionBuilder fieldValueFactor = ScoreFunctionBuilders.fieldValueFactorFunction(Constants.NUM_REVIEWS)                .factor(2).modifier(FieldValueFactorFunction.Modifier.LOG1P);        // functionscore        FunctionScoreQueryBuilder queryBuilder = QueryBuilders.functionScoreQuery(multiMatchQueryBuilder, fieldValueFactor);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSPN, null);    }</code></pre><h4 id="18、-Function-得分：衰减函数-Function-Score-Decay-Functions"><a href="#18、-Function-得分：衰减函数-Function-Score-Decay-Functions" class="headerlink" title="18、 Function 得分：衰减函数( Function Score: Decay Functions )"></a>18、 Function 得分：衰减函数( Function Score: Decay Functions )</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; decay(String query, String origin) {        MultiMatchQueryBuilder multiMatchQueryBuilder = new MultiMatchQueryBuilder(query)                .field(Constants.TITLE).field(Constants.SUMMARY);        ExponentialDecayFunctionBuilder exp = ScoreFunctionBuilders.exponentialDecayFunction(Constants.PUBLISHDATE, origin, &quot;30d&quot;, &quot;7d&quot;);        FunctionScoreQueryBuilder queryBuilder = QueryBuilders.functionScoreQuery(multiMatchQueryBuilder, exp).boostMode(CombineFunction.REPLACE);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSPN, null);    }</code></pre><h4 id="19、Function得分：脚本得分（-Function-Score-Script-Scoring-）"><a href="#19、Function得分：脚本得分（-Function-Score-Script-Scoring-）" class="headerlink" title="19、Function得分：脚本得分（ Function Score: Script Scoring ）"></a>19、Function得分：脚本得分（ Function Score: Script Scoring ）</h4><pre><code>    public Response&lt;List&lt;Book&gt;&gt; script(String query, String threshold) {        MultiMatchQueryBuilder multiMatchQueryBuilder = new MultiMatchQueryBuilder(query)                .field(Constants.TITLE).field(Constants.SUMMARY);        // 参数        Map&lt;String, Object&gt; params = new HashMap&lt;&gt;();        params.put(&quot;threshold&quot;, threshold);        // 脚本        String scriptStr = &quot;publish_date = doc[&#39;publish_date&#39;].value; num_reviews = doc[&#39;num_reviews&#39;].value; if (publish_date &gt; Date.parse(&#39;yyyy-MM-dd&#39;, threshold).getTime()) { return log(2.5 + num_reviews) }; return log(1 + num_reviews);&quot;;        Script script = new Script(ScriptType.INLINE, &quot;painless&quot;, scriptStr, params);        ScriptScoreFunctionBuilder scriptScoreFunctionBuilder = ScoreFunctionBuilders.scriptFunction(script);        FunctionScoreQueryBuilder queryBuilder = QueryBuilders.functionScoreQuery(multiMatchQueryBuilder, scriptScoreFunctionBuilder);        SearchRequestBuilder requestBuilder = client.prepareSearch(bookIndex).setTypes(bookType)                .setQuery(queryBuilder).setFetchSource(Constants.fetchFieldsTSPN, null);    }</code></pre><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;本文是对 &lt;code&gt;23个最有用的Elasticseaerch检索技巧&lt;/code&gt; 一文提到的ES检索技巧进行 Java API 的简单
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>教你编译调试Elasticsearch 6.3.2源码</title>
    <link href="http://laijianfeng.org/2018/08/%E6%95%99%E4%BD%A0%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95Elasticsearch-6-3-2%E6%BA%90%E7%A0%81/"/>
    <id>http://laijianfeng.org/2018/08/教你编译调试Elasticsearch-6-3-2源码/</id>
    <published>2018-08-23T03:39:34.000Z</published>
    <updated>2018-08-30T13:10:53.919Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>想深入理解 Elasticsearch，阅读它的源码是很有必要的，一来可以了解它内部的具体实现，有助于调优，二来可以了解优秀开源项目的代码架构，提高我们的代码架构能力等</p><p>阅读Elasticsearch源码的第一步是搭建调试环境，然后作者在这个过程中遇到很多麻烦，在网上找不到想要的答案，历经千辛最后一一解决，所以记录下，帮助有需要的童鞋</p><h4 id="软件环境"><a href="#软件环境" class="headerlink" title="软件环境"></a>软件环境</h4><ul><li>操作系统：win7</li><li>Elasticsearch 源码版本: 6.3.2</li><li>JDK版本： 10.0.2</li><li>Gradle版本： 4.7</li><li>Intellij Idea版本： 2018.2</li></ul><h3 id="环境准备及工程导入"><a href="#环境准备及工程导入" class="headerlink" title="环境准备及工程导入"></a>环境准备及工程导入</h3><h4 id="1-安装JDK"><a href="#1-安装JDK" class="headerlink" title="1.安装JDK"></a>1.安装JDK</h4><p>Elasticsearch 6.3.3需要JDK1.9编译，否则后面步骤会报错。</p><p>Java SE Downloads 地址：<br><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p><p>作者装的是 JDK 10.0.2</p><h4 id="2-下载Elasticsearch源码，并且切换到6-3-2分支"><a href="#2-下载Elasticsearch源码，并且切换到6-3-2分支" class="headerlink" title="2.下载Elasticsearch源码，并且切换到6.3.2分支"></a>2.下载Elasticsearch源码，并且切换到6.3.2分支</h4><p>Elasticsearch github源码托管地址：<br><a href="https://github.com/elastic/elasticsearch.git" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch.git</a></p><pre><code>git checkout v6.3.2</code></pre><p>也可直接下载源码包，地址在 <a href="https://github.com/elastic/elasticsearch/releases" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch/releases</a></p><h4 id="3-下载gradle的安装包"><a href="#3-下载gradle的安装包" class="headerlink" title="3.下载gradle的安装包"></a>3.下载gradle的安装包</h4><p>查看 <code>elasticsearch\gradle\wrapper\gradle-wrapper.properties</code> 发现如下配置：</p><pre><code>distributionUrl=https://services.gradle.org/distributions/gradle-4.5-all.zip</code></pre><p>Elasticsearch 6.3.2需要安装gradle-4.5，官方下载地址：<br><a href="https://services.gradle.org/distributions/gradle-4.5-all.zip" target="_blank" rel="noopener">https://services.gradle.org/distributions/gradle-4.5-all.zip</a></p><blockquote><p>注意：由于国内网速问题，为了加快速度，进行第4步操作</p></blockquote><h4 id="4-拷贝文件"><a href="#4-拷贝文件" class="headerlink" title="4.拷贝文件"></a>4.拷贝文件</h4><p>将下载的gradle-4.5-all.zip包放到 <code>elasticsearch\gradle\wrapper</code> 目录下，<br>确保和 <code>elasticsearch\gradle\wrapper\gradle-wrapper.properties</code> 在同级目录，<br>然后修改 <code>elasticsearch\gradle\wrapper\gradle-wrapper.properties</code> 配置如下：</p><pre><code>distributionUrl=gradle-4.5-all.zip</code></pre><h4 id="5-修改源码Maven仓库地址"><a href="#5-修改源码Maven仓库地址" class="headerlink" title="5.修改源码Maven仓库地址"></a>5.修改源码Maven仓库地址</h4><p>国内下载国外仓库的jar包速度慢，需要替换Maven地址，设置为本地或者国内可用的Maven仓库。</p><p>需要修改下列文件的 maven URL 配置：</p><ul><li>elasticsearch\benchmarks\build.gradle</li><li>elasticsearch\client\benchmark\build.gradle</li></ul><p>修改源码中上面build.gradle文件里面的<code>repositories-maven-url</code>的值，<br>配置为可用的仓库地址，譬如修改为阿里云maven地址 <code>http://maven.aliyun.com/nexus/content/groups/public/</code>，修改示例如下：</p><pre><code>buildscript {    repositories {        maven {            url &#39;http://maven.aliyun.com/nexus/content/groups/public/&#39;        }    }    dependencies {        classpath &#39;com.github.jengelman.gradle.plugins:shadow:2.0.2&#39;    }}</code></pre><h4 id="6-修改全局Maven仓库地址"><a href="#6-修改全局Maven仓库地址" class="headerlink" title="6.修改全局Maven仓库地址"></a>6.修改全局Maven仓库地址</h4><p>在<code>USER_HOME/.gradle/</code>下面创建新文件 <code>init.gradle</code>，输入下面的内容并保存。</p><pre><code>allprojects{    repositories {        def REPOSITORY_URL = &#39;http://maven.aliyun.com/nexus/content/groups/public/&#39;        all {            ArtifactRepository repo -&gt;    if (repo instanceof MavenArtifactRepository) {                def url = repo.url.toString()                if (url.startsWith(&#39;https://repo.maven.org/maven2&#39;) || url.startsWith(&#39;https://jcenter.bintray.com/&#39;)) {                    project.logger.lifecycle &quot;Repository ${repo.url} replaced by $REPOSITORY_URL.&quot;                    remove repo                }            }        }        maven {            url REPOSITORY_URL        }    }}</code></pre><p>其中<code>USER_HOME/.gradle/</code>是自己的gradle安装目录，示例值：<code>C:\Users\Administrator\.gradle</code>，<br>如果没有<code>.gradle</code>目录，可用自己创建，或者先执行第7步，等gradle安装后再回来修改。<br>上面脚本把url匹配到的仓库都替换成了阿里云的仓库，<br>如果有未匹配到的导致编译失败，可用自己仿照着添加匹配条件。</p><h4 id="7-gradle编译源码"><a href="#7-gradle编译源码" class="headerlink" title="7.gradle编译源码"></a>7.gradle编译源码</h4><p>windows运行cmd，进入DOS命令行，然后切换到elasticsearch源码的根目录，执行如下命令，把elasticsearch编译为 idea 工程：</p><pre><code>gradlew idea</code></pre><p>编译失败则按照错误信息解决问题，可用使用如下命令帮助定位问题：</p><pre><code>gradlew idea -infogradlew idea -debug</code></pre><p>一般是Maven仓库地址不可用导致jar包无法下载，从而编译失败，此时请参考步骤5和6修改相关的仓库地址。</p><p>编译成功后打印日志：</p><pre><code>BUILD SUCCESSFUL in 1m 23s</code></pre><h4 id="8-idea-导入elasticsearch工程"><a href="#8-idea-导入elasticsearch工程" class="headerlink" title="8. idea 导入elasticsearch工程"></a>8. idea 导入elasticsearch工程</h4><p>idea 中 <code>File -&gt; New Project From Existing Sources</code> 选择你下载的 Elasticsearch 根目录，然后点 <code>open</code> ，之后 <code>Import project from external model -&gt; Gradle</code> , 选中 <code>Use auto-import</code>, 然后就可以了</p><p>导入进去后，gradle 又会编译一遍，需要等一会，好了之后如下：</p><p><img src="http://image.laijianfeng.org/20180822_142155.png" alt="IDEA导入Elasticsearch6.3.2之后"></p><h3 id="运行，开始-solve-error-模式"><a href="#运行，开始-solve-error-模式" class="headerlink" title="运行，开始 solve error 模式"></a>运行，开始 solve error 模式</h3><blockquote><p>前面的步骤都挺顺利，接下来遇到的 ERROR &amp; EXCEPTION 让作者耗费了好几天，心力交瘁，好在最终运行成功   </p></blockquote><p>在 <code>elasticsearch/server/src/main/org/elasticsearch/bootstrap</code> 下找到Elasticsearch的启动类 <code>Elasticsearch.java</code>，打开文件，右键 <code>Run Elasticsearch.main()</code>，运行main方法</p><p><strong>1、 报错如下：</strong></p><pre><code>ERROR: the system property [es.path.conf] must be set</code></pre><p>这是需要配置 es.path.conf 参数，我们先在 elasticsearch 源码目录下新建一个 home 目录，然后在 <a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="noopener"><code>https://www.elastic.co/downloads/elasticsearch</code></a> 下载一个同版本号的 Elasticsearch6.3.2 发行版，解压，将 config 目录拷贝到 home 目录中</p><p>然后打开 <code>Edit Configurations</code>，在 <code>VM options</code> 加入如下配置：</p><p><img src="http://image.laijianfeng.org/20180822_144736.png" alt="Edit Configurations"></p><pre><code>-Des.path.conf=D:\elasticsearch-6.3.2\home\config</code></pre><p>再次运行 <code>Run Elasticsearch.main()</code></p><p><strong>2、报错如下：</strong></p><pre><code>Exception in thread &quot;main&quot; java.lang.IllegalStateException: path.home is not configured    at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:103)...</code></pre><p>需要配置 <code>path.home</code> 这个参数，在 VM options 中添加如下配置：</p><pre><code>-Des.path.home=D:\elasticsearch-6.3.2</code></pre><p>再次RUN</p><p><strong>3、报错如下：</strong></p><pre><code>2018-08-22 15:07:17,094 main ERROR Could not register mbeans java.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;)...Caused by: java.nio.file.NoSuchFileException: D:\elasticsearch-6.3.2\modules\aggs-matrix-stats\plugin-descriptor.properties...</code></pre><p>在 VM options 中把 path.home 的值修改为如下：</p><pre><code>-Des.path.home=D:\elasticsearch-6.3.2\home</code></pre><p>然后把 ES6.3.2 发行版中的 <code>modules</code> 文件夹复制到 <code>home</code> 目录下，然后再次RUN</p><p><strong>4、报错如下：</strong></p><pre><code>2018-08-22 15:12:29,876 main ERROR Could not register mbeans java.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;)...</code></pre><p>在 <code>VM options</code> 中加入</p><pre><code>-Dlog4j2.disable.jmx=true</code></pre><p>1、2、3、4 的配置最终如下：</p><p><img src="http://image.laijianfeng.org/20180823_013945.png" alt="image"></p><p>再次RUN</p><p><strong>5、报错如下：</strong></p><pre><code>[2018-08-23T00:53:17,003][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader    at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?]    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?]    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?]    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?]    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?]    at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?]    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?]    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?]    at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?]    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?]    at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader    at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?]    at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?]    at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?]    ... 15 more</code></pre><p>这个问题其实不算真正的问题，但是说起来挺好笑，为了解决这个问题耗费了作者好几天，心力交瘁，当最后发现问题所在的时候，哭笑不得 ~_~  正是所谓的 <code>踏破铁鞋无觅处，得来全不费工夫</code> </p><p><strong>解决方法：</strong> 打开 IDEA <code>Edit Configurations</code> ，给 <code>Include dependencies with Provided scope</code> 打上勾即可解决，很简单吧！！</p><p><img src="http://image.laijianfeng.org/20180823_011036.png" alt="image"></p><p>继续RUN，又来一个 Exception</p><p><strong>6、报错如下：</strong></p><pre><code>[2018-08-23T01:13:38,551][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;)    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?]    at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?]    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?]    at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?]    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;)    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) ~[?:?]    at java.security.AccessController.checkPermission(AccessController.java:895) ~[?:?]    at java.lang.SecurityManager.checkPermission(SecurityManager.java:335) ~[?:?]    at java.lang.SecurityManager.checkCreateClassLoader(SecurityManager.java:397) ~[?:?]...Exception: java.security.AccessControlException thrown from the UncaughtExceptionHandler in thread &quot;Thread-2&quot;</code></pre><p>这个问题也找了挺久，最终才发现解决方法（两种）：</p><p><strong>第一种：</strong> 在 <code>home/config</code> 目录下新建 <code>java.policy</code> 文件，填入下面内容</p><pre><code>grant {    permission java.lang.RuntimePermission &quot;createClassLoader&quot;;};</code></pre><p>然后在 <code>VM options</code> 加入 <code>java.security.policy</code> 的设置，指向该文件即可</p><pre><code>-Djava.security.policy=D:\elasticsearch-6.3.2\home\config\java.policy</code></pre><p><strong>第二种：</strong> 就是在 <code>%JAVA_HOME%/conf/security</code> 目录下（JDK10是这个路径，之前的版本不确定），我的目录是 <code>C:\Program Files\Java\jdk-10.0.2\conf\security</code>，打开 <code>java.policy</code> 文件，在 <code>grant</code> 中加入下面这句，赋予权限</p><pre><code>permission java.lang.RuntimePermission &quot;createClassLoader&quot;;</code></pre><p>效果如下：</p><p><img src="http://image.laijianfeng.org/20180823_013422.png" alt="java.policy"><br><img src="http://image.laijianfeng.org/20180823_012205.png" alt="createClassLoader"></p><p>再RUN，这次可终于运行起来了！！！</p><p>来看一下效果，浏览器访问 <code>http://localhost:9200/</code></p><p><img src="http://image.laijianfeng.org/20180823_012641.png" alt="image1"></p><p>浏览器访问 <code>http://localhost:9200/_cat/health?v</code></p><p><img src="http://image.laijianfeng.org/20180823_012827.png" alt="image"></p><p>一切正常，终于可以愉快的 DEBUG 源码啦！！！</p><h3 id="另一种源码调试方式：远程调试"><a href="#另一种源码调试方式：远程调试" class="headerlink" title="另一种源码调试方式：远程调试"></a>另一种源码调试方式：远程调试</h3><p>如果上面第五个报错之后解决不了无法继续进行，可以选择这种方式:</p><p>在 Elasticsearch 源码目录下打开 CMD，输入下面的命令启动一个 debug 实例</p><pre><code>gradlew run --debug-jvm</code></pre><p>如果启动失败可能需要先执行 <code>gradlew clean</code> 再 <code>gradlew run --debug-jvm</code> 或者 先退出 IDEA</p><p><img src="http://image.laijianfeng.org/20180823_111933.png" alt="image"></p><p>在 IDEA 中打开 <code>Edit Configurations</code>，添加 remote</p><p><img src="http://image.laijianfeng.org/20180823_104521.png" alt="image"></p><p>配置 host 和 port</p><p><img src="http://image.laijianfeng.org/20180823_110657.png" alt="image"></p><p>点击 debug，浏览器访问 <code>http://localhost:9200/</code>，即可看到ES返回的信息</p><p>随机调试一下， 打开 <code>elasticsearch/server/src/main/org/elasticsearch/rest/action/cat</code> 下的 <code>RestHealthAction</code> 类，在第 54 行出设置一个断点，然后浏览器访问 <code>http://localhost:9200/_cat/health</code>，可以看到断点已经捕获到该请求了</p><p><img src="http://image.laijianfeng.org/20180823_113341.png" alt="image"></p><p>运行成功，可以开始设置断点进行其他调试</p><h3 id="其他可能遇到的问题"><a href="#其他可能遇到的问题" class="headerlink" title="其他可能遇到的问题"></a>其他可能遇到的问题</h3><p><strong>1. 错误信息如下</strong></p><pre><code>JAVA8_HOME required to run tasks gradle</code></pre><p>配置环境变量 <code>JAVA8_HOME</code>，值为 JDK8 的安装目录</p><p><strong>2. 错误信息如下</strong></p><pre><code>[2018-08-22T13:07:23,197][INFO ][o.e.t.TransportService   ] [EFQliuV] publish_address {10.100.99.118:9300}, bound_addresses {[::]:9300}[2018-08-22T13:07:23,211][INFO ][o.e.b.BootstrapChecks    ] [EFQliuV] bound or publishing to a non-loopback address, enforcing bootstrap checksERROR: [1] bootstrap checks failed[1]: initial heap size [268435456] not equal to maximum heap size [4273995776]; this can cause resize pauses and prevents mlockall from locking the entire heap[2018-08-22T13:07:23,219][INFO ][o.e.n.Node               ] [EFQliuV] stopping ...2018-08-22 13:07:23,269 Thread-2 ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property &#39;log4j2.debug&#39; to show Log4j2 internal initialization logging.Disconnected from the target VM, address: &#39;127.0.0.1:5272&#39;, transport: &#39;socket&#39;</code></pre><p>在 <code>Edit Configurations</code> 的 <code>VM options</code> 加入下面配置</p><pre><code>-Xms2g -Xmx2g </code></pre><blockquote><p>参考文档：</p><ol><li><a href="https://www.jianshu.com/p/61dfe6fb6625" target="_blank" rel="noopener">Eclipse导入Elasticsearch源码</a></li><li><a href="https://www.felayman.com/articles/2017/11/10/1510291087246.html" target="_blank" rel="noopener">Elasticsearch源码分析—环境准备(一)</a></li><li><a href="http://www.54tianzhisheng.cn/2018/08/05/es-code01/" target="_blank" rel="noopener">渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建</a></li><li><a href="http://www.54tianzhisheng.cn/2018/08/14/idea-remote-debug-elasticsearch" target="_blank" rel="noopener">教你如何在 IDEA 远程 Debug ElasticSearch</a></li></ol></blockquote><hr><p>打开微信扫一扫，关注【小旋锋】微信公众号，及时接收博文推送</p><p><img src="http://image.laijianfeng.org/%E5%B0%8F%E6%97%8B%E9%94%8B%E7%9A%84%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7_%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg" alt="小旋锋的微信公众号"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;想深入理解 Elasticsearch，阅读它的源码是很有必要的，一来可以了解它内部的具体实现，有助于调优，二来可以了解优秀开源项目的代码架
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>20180818音乐会</title>
    <link href="http://laijianfeng.org/2018/08/20180818%E9%9F%B3%E4%B9%90%E4%BC%9A/"/>
    <id>http://laijianfeng.org/2018/08/20180818音乐会/</id>
    <published>2018-08-18T14:44:24.000Z</published>
    <updated>2018-08-18T15:02:13.767Z</updated>
    
    <content type="html"><![CDATA[<div style="text-align: center"><br><br><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=116837&auto=1&height=66"></iframe><br><br></div><p>感受幸福，感受快乐，感受…悲伤….</p><p><img src="http://image.laijianfeng.org/20180818224224.jpg" alt="丹麦钢琴家克里斯蒂娜·比约克独奏音乐会"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div style=&quot;text-align: center&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//
      
    
    </summary>
    
      <category term="生活杂记" scheme="http://laijianfeng.org/categories/%E7%94%9F%E6%B4%BB%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Lucene初体验</title>
    <link href="http://laijianfeng.org/2018/08/Lucene%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <id>http://laijianfeng.org/2018/08/Lucene初体验/</id>
    <published>2018-08-18T08:23:02.000Z</published>
    <updated>2018-08-18T08:27:00.998Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文的简要内容：</p><ol><li>Lucene简介</li><li>体验Lucene Demo</li><li>Lucene 核心类介绍</li><li>Lucene 索引文件格式</li></ol><h3 id="Lucene简介"><a href="#Lucene简介" class="headerlink" title="Lucene简介"></a>Lucene简介</h3><p>Lucene是目前最流行的Java开源搜索引擎类库,最新版本为7.4.0。Lucene通常用于全文检索,Lucene具有简单高效跨平台等特点,因此有不少搜索引擎都是基于Lucene构建的,例如:Elasticsearch,Solr等等。</p><p>现代搜索引擎的两大核心就是索引和搜索，建立索引的过程就是对源数据进行处理，例如过滤掉一些特殊字符或词语，单词大小写转换，分词，建立倒排索引等支持后续高效准确的搜索。而搜索则是直接提供给用户的功能，尽管面向的用户不同，诸如百度，谷歌等互联网公司以及各种企业都提供了各自的搜索引擎。搜索过程需要对搜索关键词进行分词等处理，然后再引擎内部构建查询，还要根据相关度对搜索结果进行排序，最终把命中结果展示给用户。</p><p>Lucene只是一个提供索引和查询的<strong>类库</strong>，并不是一个应用，程序员需要根据自己的应用场景进行如数据获取、数据预处理、用户界面提供等工作。</p><p>搜索程序的典型组件如下所示：</p><p><img src="http://image.laijianfeng.org/20180818_132723.jpg" alt="搜索程序的典型组件"></p><p>下图为Lucene与应用程序的关系:</p><p><img src="http://image.laijianfeng.org/fig001.jpg" alt="Lucene与应用程序的关系"></p><h3 id="体验Lucene-Demo"><a href="#体验Lucene-Demo" class="headerlink" title="体验Lucene Demo"></a>体验Lucene Demo</h3><p>接下来先来看一个简单的demo</p><blockquote><p>note:<br>代码在 <a href="https://github.com/whirlys/elastic-example/tree/master/startlucene/src/main/java/startlucene" target="_blank" rel="noopener">start Lucene</a></p></blockquote><h4 id="引入-Maven-依赖"><a href="#引入-Maven-依赖" class="headerlink" title="引入 Maven 依赖"></a>引入 Maven 依赖</h4><pre><code>    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;        &lt;lucene.version&gt;7.4.0&lt;/lucene.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.11&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;            &lt;version&gt;${lucene.version}&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt;            &lt;version&gt;${lucene.version}&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;</code></pre><h4 id="索引类-IndexFiles-java"><a href="#索引类-IndexFiles-java" class="headerlink" title="索引类 IndexFiles.java"></a>索引类 IndexFiles.java</h4><pre><code>import org.apache.lucene.analysis.*;import org.apache.lucene.analysis.standard.*;import org.apache.lucene.document.*;import org.apache.lucene.index.*;import org.apache.lucene.store.*;import java.io.*;import java.nio.charset.*;import java.nio.file.*;import java.nio.file.attribute.*;public class IndexFiles {    public static void main(String[] args) {        String indexPath = &quot;D:/lucene_test/index&quot;; // 建立索引文件的目录        String docsPath = &quot;D:/lucene_test/docs&quot;; // 读取文本文件的目录        Path docDir = Paths.get(docsPath);        IndexWriter writer = null;        try {            // 存储索引数据的目录            Directory dir = FSDirectory.open(Paths.get(indexPath));            // 创建分析器            Analyzer analyzer = new StandardAnalyzer();            IndexWriterConfig iwc = new IndexWriterConfig(analyzer);            iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);            writer = new IndexWriter(dir, iwc);            indexDocs(writer, docDir);            writer.close();        } catch (IOException e) {            e.printStackTrace();        }    }    private static void indexDocs(final IndexWriter writer, Path path) throws IOException {        if (Files.isDirectory(path)) {            Files.walkFileTree(path, new SimpleFileVisitor&lt;Path&gt;() {                @Override                public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) {                    try {                        indexDoc(writer, file);                    } catch (IOException ignore) {                        // 不索引那些不能读取的文件,忽略该异常                    }                    return FileVisitResult.CONTINUE;                }            });        } else {            indexDoc(writer, path);        }    }    private static void indexDoc(IndexWriter writer, Path file) throws IOException {        try (InputStream stream = Files.newInputStream(file)) {            // 创建一个新的空文档            Document doc = new Document();            // 添加字段            Field pathField = new StringField(&quot;path&quot;, file.toString(), Field.Store.YES);            doc.add(pathField);            Field contentsField = new TextField(&quot;contents&quot;,                    new BufferedReader(new InputStreamReader(stream, StandardCharsets.UTF_8)));            doc.add(contentsField);            System.out.println(&quot;adding &quot; + file);            // 写文档            writer.addDocument(doc);        }    }}</code></pre><h4 id="查询类-SearchFiles-java"><a href="#查询类-SearchFiles-java" class="headerlink" title="查询类 SearchFiles.java"></a>查询类 SearchFiles.java</h4><pre><code>import org.apache.lucene.analysis.*;import org.apache.lucene.analysis.standard.*;import org.apache.lucene.document.*;import org.apache.lucene.index.*;import org.apache.lucene.queryparser.classic.*;import org.apache.lucene.search.*;import org.apache.lucene.store.*;import java.io.*;import java.nio.charset.*;import java.nio.file.*;public class SearchFiles {    public static void main(String[] args) throws Exception {        String indexPath = &quot;D:/lucene_test/index&quot;; // 建立索引文件的目录        String field = &quot;contents&quot;;        IndexReader reader = DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));        IndexSearcher searcher = new IndexSearcher(reader);        Analyzer analyzer = new StandardAnalyzer();        BufferedReader in = null;        in = new BufferedReader(new InputStreamReader(System.in, StandardCharsets.UTF_8));        QueryParser parser = new QueryParser(field, analyzer);        System.out.println(&quot;Enter query:&quot;);        // 从Console读取要查询的语句        String line = in.readLine();        if (line == null || line.length() == -1) {            return;        }        line = line.trim();        if (line.length() == 0) {            return;        }        Query query = parser.parse(line);        System.out.println(&quot;Searching for:&quot; + query.toString(field));        doPagingSearch(searcher, query);        in.close();        reader.close();    }    private static void doPagingSearch(IndexSearcher searcher, Query query) throws IOException {        // TopDocs保存搜索结果        TopDocs results = searcher.search(query, 10);        ScoreDoc[] hits = results.scoreDocs;        int numTotalHits = Math.toIntExact(results.totalHits);        System.out.println(numTotalHits + &quot; total matching documents&quot;);        for (ScoreDoc hit : hits) {            Document document = searcher.doc(hit.doc);            System.out.println(&quot;文档:&quot; + document.get(&quot;path&quot;));            System.out.println(&quot;相关度:&quot; + hit.score);            System.out.println(&quot;================================&quot;);        }    }}</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>首先创建文件夹 <code>D:\lucene_test</code>，在 <code>lucene_test</code> 下再创建 <code>docs</code> 文件夹，用来存储要索引的测试文件</p><p>在 <code>docs</code> 下创建3个文件 test1.txt, test2.txt, test3.txt，分别写入 hello world、 hello lucene、 hello elasticsearch</p><p>运行索引类 IndexFiles.java，可看到Console输出</p><pre><code>adding D:\lucene_test\docs\test1.txtadding D:\lucene_test\docs\test2.txtadding D:\lucene_test\docs\test3.txt</code></pre><p><img src="http://image.laijianfeng.org/20180818_135449.png" alt="Lucene的索引文件"></p><p>运行查询类 SearchFiles.java，搜索 hello ，三个文件相关度一样</p><pre><code>Enter query:helloSearching for:hello3 total matching documents文档:D:\lucene_test\docs\test1.txt相关度:0.13353139================================文档:D:\lucene_test\docs\test2.txt相关度:0.13353139================================文档:D:\lucene_test\docs\test3.txt相关度:0.13353139================================</code></pre><p>搜索 hello lucene，test2.txt的相关度比其他两个高</p><pre><code>Enter query:hello luceneSearching for:hello lucene3 total matching documents文档:D:\lucene_test\docs\test2.txt相关度:1.1143606================================文档:D:\lucene_test\docs\test1.txt相关度:0.13353139================================文档:D:\lucene_test\docs\test3.txt相关度:0.13353139================================</code></pre><h3 id="Lucene-核心类介绍"><a href="#Lucene-核心类介绍" class="headerlink" title="Lucene 核心类介绍"></a>Lucene 核心类介绍</h3><h4 id="核心索引类"><a href="#核心索引类" class="headerlink" title="核心索引类"></a>核心索引类</h4><p>IndexWriter</p><pre><code>进行索引写操作的一个中心组件不能进行读取和搜索</code></pre><p>Directory</p><pre><code>Directory代表Lucene索引的存放位置常用的实现：    FSDerectory:表示一个存储在文件系统中的索引的位置    RAMDirectory:表示一个存储在内存当中的索引的位置作用：    IndexWriter通过获取Directory的一个具体实现，在Directory指向的位置中操作索引</code></pre><p>Analyzer</p><pre><code>Analyzer，分析器，相当于筛子，对内容进行过滤，分词，转换等作用：把过滤之后的数据交给indexWriter进行索引</code></pre><p>Document</p><pre><code>用来存放文档（数据），该文档为非结构化数据中抓取的相关数据通过Field(域)组成Document，类似于mysql中的一个个字段组成的一条记录</code></pre><p>Field</p><pre><code>Document中的一个字段</code></pre><h4 id="核心搜索类"><a href="#核心搜索类" class="headerlink" title="核心搜索类"></a>核心搜索类</h4><p>IndexSearcher</p><pre><code>IndexSearcher在建立好的索引上进行搜索它只能以 只读 的方式打开一个索引，所以可以有多个IndexSearcher的实例在一个索引上进行操作</code></pre><p>Term</p><pre><code>Term是搜索的基本单元，一个Term由 key:value 组成（类似于mysql中的  字段名称=查询的内容）例子： Query query = new TermQuery(new Term(&quot;filename&quot;, &quot;lucene&quot;));</code></pre><p>Query</p><pre><code>Query是一个抽象类，用来将用户输入的查询字符串封装成Lucene能够识别的Query</code></pre><p>TermQuery</p><pre><code>Query子类，Lucene支持的最基本的一个查询类例子：TermQuery termQuery = new TermQuery(new Term(&quot;filename&quot;, &quot;lucene&quot;));</code></pre><p>BooleanQuery</p><pre><code>BooleanQUery，布尔查询,是一个组合Query（多个查询条件的组合）BooleanQuery是可以嵌套的栗子：BooleanQuery query = new BooleanQuery();BooleanQuery query2 = new BooleanQuery();TermQuery termQuery1 = new TermQuery(new Term(&quot;fileName&quot;, &quot;lucene&quot;));TermQuery termQuery2 = new TermQuery(new Term(&quot;fileName&quot;, &quot;name&quot;));query2.add(termQuery1, Occur.SHOULD);query.add(termQuery2, Occur.SHOULD);query.add(query2, Occur.SHOULD);;        //BooleanQuery是可以嵌套的Occur枚举：    MUST    SHOULD    FILTER    MUST_NOT</code></pre><p>NumericRangeQuery</p><pre><code>数字区间查询栗子：Query newLongRange = NumericRangeQuery.newLongRange(&quot;fileSize&quot;,0l, 100l, true, true);</code></pre><p>PrefixQuery</p><pre><code>前缀查询，查询分词中含有指定字符开头的内容栗子：PrefixQuery query = new PrefixQuery(new Term(&quot;fileName&quot;,&quot;hell&quot;));</code></pre><p>PhraseQuery</p><pre><code>短语查询栗子1：    PhraseQuery query = new PhraseQuery();    query.add(new Term(&quot;fileName&quot;,&quot;lucene&quot;));</code></pre><p>FuzzyQuery</p><pre><code>模糊查询栗子：FuzzyQuery query = new FuzzyQuery(new Term(&quot;fileName&quot;,&quot;lucene&quot;));</code></pre><p>WildcardQuery</p><pre><code>通配符查询：* ：任意字符（0或多个）? : 一个字符栗子：WildcardQuery query = new WildcardQuery(new Term(&quot;fileName&quot;,&quot;*&quot;));</code></pre><p>RegexQuery</p><pre><code>正则表达式查询栗子：搜索含有最少1个字符，最多6个字符的RegexQuery query = new RegexQuery(new Term(&quot;fileName&quot;,&quot;[a-z]{1,6}&quot;));</code></pre><p>MultiFieldQueryParser</p><pre><code>查询多个field栗子：String[] fields = {&quot;fileName&quot;,&quot;fileContent&quot;};MultiFieldQueryParser queryParser = new MultiFieldQueryParser(fields, new StandardAnalyzer());Query query = queryParser.parse(&quot;fileName:lucene AND filePath:a&quot;);</code></pre><p>TopDocs</p><pre><code>TopDocs类是一个简单的指针容器,指针一般指向前N个排名的搜索结果,搜索结果即匹配条件的文档TopDocs会记录前N个结果中每个结果的int docID和浮点数型分数(反映相关度)栗子：    TermQuery searchingBooks = new TermQuery(new Term(&quot;subject&quot;,&quot;search&quot;));     Directory dir = TestUtil.getBookIndexDirectory();    IndexSearcher searcher = new IndexSearcher(dir);    TopDocs matches = searcher.search(searchingBooks, 10);</code></pre><h3 id="Lucene-6-0-索引文件格式"><a href="#Lucene-6-0-索引文件格式" class="headerlink" title="Lucene 6.0 索引文件格式"></a>Lucene 6.0 索引文件格式</h3><h4 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h4><p>谈到倒排索引，那么首先看看正排是什么样子的呢？假设文档1包含【中文、英文、日文】，文档2包含【英文、日文、韩文】，文档3包含【韩文，中文】，那么根据文档去查找内容的话</p><pre><code>文档1-&gt;【中文、英文、日文】文档2-&gt;【英文、日文、韩文】文档3-&gt;【韩文，中文】</code></pre><p>反过来，根据内容去查找文档</p><pre><code>中文-&gt;【文档1、文档3】英文-&gt;【文档1、文档2】日文-&gt;【文档1、文档2】韩文-&gt;【文档2、文档3】</code></pre><p>这就是倒排索引，而Lucene擅长的也正在于此</p><h4 id="段（Segments）"><a href="#段（Segments）" class="headerlink" title="段（Segments）"></a>段（Segments）</h4><p>Lucene的索引可能是由多个子索引或Segments组成。每个Segment是一个完全独立的索引，可以单独用于搜索，索引涉及</p><ol><li>为新添加的documents创建新的segments</li><li>合并已经存在的segments</li></ol><p>搜索可能涉及多个segments或多个索引，每个索引可能由一组segments组成</p><h4 id="文档编号"><a href="#文档编号" class="headerlink" title="文档编号"></a>文档编号</h4><p>Lucene通过一个整型的文档编号指向每个文档，第一个被加入索引的文档编号为0，后续加入的文档编号依次递增。<br>注意文档编号是可能发生变化的，所以在Lucene外部存储这些值时需要格外小心。</p><h4 id="索引结构概述"><a href="#索引结构概述" class="headerlink" title="索引结构概述"></a>索引结构概述</h4><p>每个segment索引包括信息</p><ul><li>Segment info：包含有关segment的元数据，例如文档编号，使用的文件</li><li>Field names：包含索引中使用的字段名称集合</li><li>Stored Field values：对于每个document，它包含属性-值对的列表，其中属性是字段名称。这些用于存储有关文档的辅助信息，例如其标题、url或访问数据库的标识符</li><li>Term dictionary：包含所有文档的所有索引字段中使用的所有terms的字典。字典还包括包含term的文档编号，以及指向term的频率和接近度的指针</li><li>Term Frequency data：对于字典中的每个term，包含该term的所有文档的数量以及该term在该文档中的频率，除非省略频率（IndexOptions.DOCS）</li><li>Term Proximity data：对于字典中的每个term，term在每个文档中出现的位置。注意，如果所有文档中的所有字段都省略位置数据，则不会存在</li><li>Normalization factors：对于每个文档中的每个字段，存储一个值，该值将乘以该字段上的匹配的分数</li><li>Term Vectors：对于每个文档中的每个字段，可以存储term vector，term vector由term文本和term频率组成</li><li>Per-document values：与存储的值类似，这些也以文档编号作为key，但通常旨在被加载到主存储器中以用于快速访问。存储的值通常用于汇总来自搜索的结果，而每个文档值对于诸如评分因子是有用的</li><li>Live documents：一个可选文件，指示哪些文档是活动的</li><li>Point values：可选的文件对，记录索引字段尺寸，以实现快速数字范围过滤和大数值（例如BigInteger、BigDecimal（1D）、地理形状交集（2D，3D））</li></ul><h4 id="文件命名"><a href="#文件命名" class="headerlink" title="文件命名"></a>文件命名</h4><p>属于一个段的所有文件具有相同的名称和不同的扩展名。当使用复合索引文件，这些文件（除了段信息文件、锁文件和已删除的文档文件）将压缩成单个.cfs文件。当任何索引文件被保存到目录时，它被赋予一个从未被使用过的文件名字</p><p><img src="http://image.laijianfeng.org/20180818_161352.png" alt="复合索引文件"></p><h4 id="文件扩展名摘要"><a href="#文件扩展名摘要" class="headerlink" title="文件扩展名摘要"></a>文件扩展名摘要</h4><table><thead><tr><th>名称</th><th>文件扩展名</th><th>简短描述</th></tr></thead><tbody><tr><td>Segments File</td><td>segments_N</td><td>保存了一个提交点（a commit point）的信息</td></tr><tr><td>Lock File</td><td>write.lock</td><td>防止多个IndexWriter同时写到一份索引文件中</td></tr><tr><td>Segment Info</td><td>.si</td><td>保存了索引段的元数据信息</td></tr><tr><td>Compound File</td><td>.cfs，.cfe</td><td>一个可选的虚拟文件，把所有索引信息都存储到复合索引文件中</td></tr><tr><td>Fields</td><td>.fnm</td><td>保存fields的相关信息</td></tr><tr><td>Field Index</td><td>.fdx</td><td>保存指向field data的指针</td></tr><tr><td>Field Data</td><td>.fdt</td><td>文档存储的字段的值</td></tr><tr><td>Term Dictionary</td><td>.tim</td><td>term词典，存储term信息</td></tr><tr><td>Term Index</td><td>.tip</td><td>到Term Dictionary的索引</td></tr><tr><td>Frequencies</td><td>.doc</td><td>由包含每个term以及频率的docs列表组成</td></tr><tr><td>Positions</td><td>.pos</td><td>存储出现在索引中的term的位置信息</td></tr><tr><td>Payloads</td><td>.pay</td><td>存储额外的per-position元数据信息，例如字符偏移和用户payloads</td></tr><tr><td>Norms</td><td>.nvd，.nvm</td><td>.nvm文件保存索引字段加权因子的元数据，.nvd文件保存索引字段加权数据</td></tr><tr><td>Per-Document Values</td><td>.dvd，.dvm</td><td>.dvm文件保存索引文档评分因子的元数据，.dvd文件保存索引文档评分数据</td></tr><tr><td>Term Vector Index</td><td>.tvx</td><td>将偏移存储到文档数据文件中</td></tr><tr><td>Term Vector Documents</td><td>.tvd</td><td>包含有term vectors的每个文档信息</td></tr><tr><td>Term Vector Fields</td><td>.tvf</td><td>字段级别有关term vectors的信息</td></tr><tr><td>Live Documents</td><td>.liv</td><td>哪些是有效文件的信息</td></tr><tr><td>Point values</td><td>.dii，.dim</td><td>保留索引点，如果有的话</td></tr></tbody></table><h4 id="锁文件"><a href="#锁文件" class="headerlink" title="锁文件"></a>锁文件</h4><p>默认情况下，存储在索引目录中的锁文件名为 <code>write.lock</code>。如果锁目录与索引目录不同，则锁文件将命名为“XXXX-write.lock”，其中XXXX是从索引目录的完整路径导出的唯一前缀。此锁文件确保每次只有一个写入程序在修改索引。</p><blockquote><p>参考：</p><ol><li><a href="http://sndragon.com/2018/05/03/Lucene%E5%88%9D%E8%AF%86%E5%8F%8A%E6%A0%B8%E5%BF%83%E7%B1%BB%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">Lucene初识及核心类介绍</a></li><li><a href="http://qguofeng.oschina.io/2018/03/17/Lucene/2018-03-17-Lucene%E6%A0%B8%E5%BF%83%E7%B1%BB/" target="_blank" rel="noopener">Lucene核心类</a></li><li><a href="http://codepub.cn/2016/12/05/Lucene-6-0-index-file-format/" target="_blank" rel="noopener">Lucene 6.0 索引文件格式</a></li><li>Lucene实战.pdf</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;本文的简要内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Lucene简介&lt;/li&gt;
&lt;li&gt;体验Lucene Demo&lt;/li&gt;
&lt;li&gt;Lucene 
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="lucene" scheme="http://laijianfeng.org/tags/lucene/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 6.x Mapping设置</title>
    <link href="http://laijianfeng.org/2018/08/Elasticsearch-6-x-Mapping%E8%AE%BE%E7%BD%AE/"/>
    <id>http://laijianfeng.org/2018/08/Elasticsearch-6-x-Mapping设置/</id>
    <published>2018-08-16T13:14:33.000Z</published>
    <updated>2018-08-16T13:19:49.268Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html" target="_blank" rel="noopener">Mapping</a></h3><p>类似于数据库中的表结构定义，主要作用如下：</p><ul><li>定义Index下字段名（Field Name）</li><li>定义字段的类型，比如数值型，字符串型、布尔型等</li><li>定义倒排索引的相关配置，比如是否索引、记录postion等   </li></ul><p>需要注意的是，在索引中定义太多字段可能会导致索引膨胀，出现内存不足和难以恢复的情况，下面有几个设置：</p><ul><li>index.mapping.total_fields.limit：一个索引中能定义的字段的最大数量，默认是 1000</li><li>index.mapping.depth.limit：字段的最大深度，以内部对象的数量来计算，默认是20</li><li>index.mapping.nested_fields.limit：索引中嵌套字段的最大数量，默认是50</li></ul><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html" target="_blank" rel="noopener">数据类型</a></h3><h4 id="核心数据类型"><a href="#核心数据类型" class="headerlink" title="核心数据类型"></a>核心数据类型</h4><ul><li>字符串 - text<ul><li>用于全文索引，该类型的字段将通过分词器进行分词，最终用于构建索引</li></ul></li><li>字符串 - keyword<ul><li>不分词，只能搜索该字段的完整的值，只用于 filtering </li></ul></li><li>数值型 <ul><li>long：有符号64-bit integer：-2^63 ~ 2^63 - 1 </li><li>integer：有符号32-bit integer，-2^31 ~ 2^31 - 1 </li><li>short：有符号16-bit integer，-32768 ~ 32767</li><li>byte： 有符号8-bit integer，-128 ~ 127</li><li>double：64-bit IEEE 754 浮点数</li><li>float：32-bit IEEE 754 浮点数</li><li>half_float：16-bit IEEE 754 浮点数</li><li>scaled_float</li></ul></li><li>布尔 - boolean<ul><li>值：false, “false”, true, “true”</li></ul></li><li>日期 - date<ul><li>由于Json没有date类型，所以es通过识别字符串是否符合format定义的格式来判断是否为date类型</li><li>format默认为：<code>strict_date_optional_time||epoch_millis</code> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html" target="_blank" rel="noopener">format</a></li></ul></li><li>二进制 - binary<ul><li>该类型的字段把值当做经过 base64 编码的字符串，默认不存储，且不可搜索</li></ul></li><li><p>范围类型</p><ul><li>范围类型表示值是一个范围，而不是一个具体的值</li><li>譬如 age 的类型是 integer_range，那么值可以是  {“gte” : 10, “lte” : 20}；搜索 “term” : {“age”: 15} 可以搜索该值；搜索 “range”: {“age”: {“gte”:11, “lte”: 15}} 也可以搜索到</li><li>range参数 relation 设置匹配模式<ul><li>INTERSECTS ：默认的匹配模式，只要搜索值与字段值有交集即可匹配到</li><li>WITHIN：字段值需要完全包含在搜索值之内，也就是字段值是搜索值的子集才能匹配</li><li>CONTAINS：与WITHIN相反，只搜索字段值包含搜索值的文档</li></ul></li><li>integer_range</li><li>float_range</li><li>long_range</li><li>double_range</li><li>date_range：64-bit 无符号整数，时间戳（单位：毫秒）</li><li><p>ip_range：IPV4 或 IPV6 格式的字符串</p><p>​</p></li></ul></li></ul><pre><code># 创建range索引PUT range_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;properties&quot;: {        &quot;expected_attendees&quot;: {          &quot;type&quot;: &quot;integer_range&quot;        },        &quot;time_frame&quot;: {          &quot;type&quot;: &quot;date_range&quot;,           &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot;        }      }    }  }}# 插入一个文档PUT range_index/_doc/1{  &quot;expected_attendees&quot; : {     &quot;gte&quot; : 10,    &quot;lte&quot; : 20  },  &quot;time_frame&quot; : {     &quot;gte&quot; : &quot;2015-10-31 12:00:00&quot;,     &quot;lte&quot; : &quot;2015-11-05&quot;  }}# 12在 10~20的范围内，可以搜索到文档1GET range_index/_search{  &quot;query&quot; : {    &quot;term&quot; : {      &quot;expected_attendees&quot; : {        &quot;value&quot;: 12      }    }  }}# within可以搜索到文档# 可以修改日期，然后分别对比CONTAINS，WITHIN，INTERSECTS的区别GET range_index/_search{  &quot;query&quot; : {    &quot;range&quot; : {      &quot;time_frame&quot; : {         &quot;gte&quot; : &quot;2015-11-02&quot;,        &quot;lte&quot; : &quot;2015-11-03&quot;,        &quot;relation&quot; : &quot;within&quot;       }    }  }}</code></pre><h4 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h4><ul><li>数组类型 Array<ul><li>字符串数组 [ “one”, “two” ]</li><li>整数数组 [ 1, 2 ]</li><li>数组的数组  [ 1, [ 2, 3 ]]，相当于 [ 1, 2, 3 ]</li><li>Object对象数组 [ { “name”: “Mary”, “age”: 12 }, { “name”: “John”, “age”: 10 }]</li><li>同一个数组只能存同类型的数据，不能混存，譬如 [ 10, “some string” ] 是错误的</li><li>数组中的 null 值将被 null_value 属性设置的值代替或者被忽略</li><li>空数组 [] 被当做 missing field 处理</li></ul></li><li><p>对象类型 Object</p><ul><li>对象类型可能有内部对象</li><li><p>被索引的形式为：manager.name.first</p><p>​</p></li></ul></li></ul><pre><code># tags字符串数组，lists 对象数组PUT my_index/_doc/1{  &quot;message&quot;: &quot;some arrays in this document...&quot;,  &quot;tags&quot;:  [ &quot;elasticsearch&quot;, &quot;wow&quot; ],   &quot;lists&quot;: [     {      &quot;name&quot;: &quot;prog_list&quot;,      &quot;description&quot;: &quot;programming list&quot;    },    {      &quot;name&quot;: &quot;cool_list&quot;,      &quot;description&quot;: &quot;cool stuff list&quot;    }  ]}</code></pre><ul><li>嵌套类型 Nested<ul><li>nested 类型是一种对象类型的特殊版本，它允许索引对象数组，<strong>独立地索引每个对象</strong></li></ul></li></ul><h4 id="嵌套类型与Object类型的区别"><a href="#嵌套类型与Object类型的区别" class="headerlink" title="嵌套类型与Object类型的区别"></a>嵌套类型与Object类型的区别</h4><p>通过例子来说明:</p><ol><li>插入一个文档，不设置mapping，此时 user 字段被自动识别为<strong>对象数组</strong></li></ol><pre><code>DELETE my_indexPUT my_index/_doc/1{  &quot;group&quot; : &quot;fans&quot;,  &quot;user&quot; : [     {      &quot;first&quot; : &quot;John&quot;,      &quot;last&quot; :  &quot;Smith&quot;    },    {      &quot;first&quot; : &quot;Alice&quot;,      &quot;last&quot; :  &quot;White&quot;    }  ]}</code></pre><ol start="2"><li>查询 user.first为 Alice，user.last 为 Smith的文档，理想中应该找不到匹配的文档</li><li>结果是查到了文档1，为什么呢？</li></ol><pre><code>GET my_index/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: [        { &quot;match&quot;: { &quot;user.first&quot;: &quot;Alice&quot; }},        { &quot;match&quot;: { &quot;user.last&quot;:  &quot;Smith&quot; }}      ]    }  }}</code></pre><ol start="4"><li>是由于Object对象类型在内部被转化成如下格式的文档：<pre><code>{&quot;group&quot; :        &quot;fans&quot;,&quot;user.first&quot; : [ &quot;alice&quot;, &quot;john&quot; ],&quot;user.last&quot; :  [ &quot;smith&quot;, &quot;white&quot; ]}</code></pre></li><li>user.first 和 user.last 扁平化为多值字段，alice 和 white 的<strong>关联关系丢失了</strong>。导致这个文档错误地匹配对 alice 和 smith 的查询</li><li>如果最开始就把user设置为 nested 嵌套对象呢？</li></ol><pre><code>DELETE my_indexPUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;properties&quot;: {        &quot;user&quot;: {          &quot;type&quot;: &quot;nested&quot;         }      }    }  }}PUT my_index/_doc/1{  &quot;group&quot;: &quot;fans&quot;,  &quot;user&quot;: [    {      &quot;first&quot;: &quot;John&quot;,      &quot;last&quot;: &quot;Smith&quot;    },    {      &quot;first&quot;: &quot;Alice&quot;,      &quot;last&quot;: &quot;White&quot;    }  ]}</code></pre><ol start="7"><li>再来进行查询，可以发现以下第一个查不到文档，第二个查询到文档1，符合我们预期</li></ol><pre><code>GET my_index/_search{  &quot;query&quot;: {    &quot;nested&quot;: {      &quot;path&quot;: &quot;user&quot;,      &quot;query&quot;: {        &quot;bool&quot;: {          &quot;must&quot;: [            { &quot;match&quot;: { &quot;user.first&quot;: &quot;Alice&quot; }},            { &quot;match&quot;: { &quot;user.last&quot;:  &quot;Smith&quot; }}           ]        }      }    }  }}GET my_index/_search{  &quot;query&quot;: {    &quot;nested&quot;: {      &quot;path&quot;: &quot;user&quot;,      &quot;query&quot;: {        &quot;bool&quot;: {          &quot;must&quot;: [            { &quot;match&quot;: { &quot;user.first&quot;: &quot;Alice&quot; }},            { &quot;match&quot;: { &quot;user.last&quot;:  &quot;White&quot; }}           ]        }      },      &quot;inner_hits&quot;: {         &quot;highlight&quot;: {          &quot;fields&quot;: {            &quot;user.first&quot;: {}          }        }      }    }  }}</code></pre><ol start="8"><li><p>nested对象将数组中每个对象作为独立隐藏文档来索引，这意味着每个嵌套对象都可以独立被搜索</p></li><li><p>需要注意的是：</p></li></ol><ul><li>使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html" target="_blank" rel="noopener">nested 查询</a>来搜索</li><li>使用 nested 和 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-reverse-nested-aggregation.html" target="_blank" rel="noopener">reverse_nested</a> 聚合来分析</li><li>使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#nested-sorting" target="_blank" rel="noopener">nested sorting</a> 来排序</li><li>使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html#nested-inner-hits" target="_blank" rel="noopener">nested inner hits</a> 来检索和高亮</li></ul><h4 id="地理位置数据类型"><a href="#地理位置数据类型" class="headerlink" title="地理位置数据类型"></a>地理位置数据类型</h4><ul><li>geo_point<ul><li>地理位置，其值可以有如下四中表现形式：<ul><li>object对象：”location”: {“lat”: 41.12, “lon”: -71.34}</li><li>字符串：”location”: “41.12,-71.34”</li><li><a href="http://geohash.gofreerange.com/" target="_blank" rel="noopener">geohash</a>：”location”: “drm3btev3e86” </li><li>数组：”location”: [ -71.34, 41.12 ] </li></ul></li><li>查询的时候通过 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-bounding-box-query.html" target="_blank" rel="noopener">Geo Bounding Box Query </a> 进行查询</li></ul></li><li>geo_shape</li></ul><h4 id="专用数据类型"><a href="#专用数据类型" class="headerlink" title="专用数据类型"></a>专用数据类型</h4><ul><li>记录IP地址 ip</li><li>实现自动补全 completion</li><li>记录分词数 token_count</li><li>记录字符串hash值 murmur3</li><li>Percolator</li></ul><pre><code># ip类型，存储IPPUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;properties&quot;: {        &quot;ip_addr&quot;: {          &quot;type&quot;: &quot;ip&quot;        }      }    }  }}PUT my_index/_doc/1{  &quot;ip_addr&quot;: &quot;192.168.1.1&quot;}GET my_index/_search{  &quot;query&quot;: {    &quot;term&quot;: {      &quot;ip_addr&quot;: &quot;192.168.0.0/16&quot;    }  }}</code></pre><h4 id="多字段特性-multi-fields"><a href="#多字段特性-multi-fields" class="headerlink" title="多字段特性 multi-fields"></a>多字段特性 multi-fields</h4><ul><li>允许对同一个字段采用不同的配置，比如分词，常见例子如对人名实现拼音搜索，只需要在人名中新增一个<strong>子字段</strong>为 pinyin 即可</li><li>通过参数 fields 设置</li></ul><h4 id="设置Mapping"><a href="#设置Mapping" class="headerlink" title="设置Mapping"></a>设置Mapping</h4><p><img src="http://image.laijianfeng.org/20180804_024134.png" alt="image"></p><pre><code>GET my_index/_mapping# 结果{  &quot;my_index&quot;: {    &quot;mappings&quot;: {      &quot;doc&quot;: {        &quot;properties&quot;: {          &quot;age&quot;: {            &quot;type&quot;: &quot;integer&quot;          },          &quot;created&quot;: {            &quot;type&quot;: &quot;date&quot;          },          &quot;name&quot;: {            &quot;type&quot;: &quot;text&quot;          },          &quot;title&quot;: {            &quot;type&quot;: &quot;text&quot;          }        }      }    }  }}</code></pre><h3 id="Mapping参数"><a href="#Mapping参数" class="headerlink" title="Mapping参数"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-params.html" target="_blank" rel="noopener">Mapping参数</a></h3><h4 id="analyzer"><a href="#analyzer" class="headerlink" title="analyzer"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html" target="_blank" rel="noopener">analyzer</a></h4><ul><li>分词器，默认为standard analyzer，当该字段被索引和搜索时对字段进行分词处理</li></ul><h4 id="boost"><a href="#boost" class="headerlink" title="boost"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-boost.html" target="_blank" rel="noopener">boost</a></h4><ul><li>字段权重，默认为1.0</li></ul><h4 id="dynamic"><a href="#dynamic" class="headerlink" title="dynamic"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic.html" target="_blank" rel="noopener">dynamic</a></h4><ul><li>Mapping中的字段类型一旦设定后，禁止直接修改，原因是：Lucene实现的倒排索引生成后不允许修改</li><li>只能新建一个索引，然后reindex数据</li><li>默认允许新增字段</li><li><p>通过dynamic参数来控制字段的新增：</p><ul><li>true（默认）允许自动新增字段</li><li>false 不允许自动新增字段，但是文档可以正常写入，但无法对新增字段进行查询等操作</li><li><p>strict 文档不能写入，报错</p><p>​</p></li></ul></li></ul><pre><code>PUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;dynamic&quot;: false,       &quot;properties&quot;: {        &quot;user&quot;: {           &quot;properties&quot;: {            &quot;name&quot;: {              &quot;type&quot;: &quot;text&quot;            },            &quot;social_networks&quot;: {               &quot;dynamic&quot;: true,              &quot;properties&quot;: {}            }          }        }      }    }  }}</code></pre><p>定义后my_index这个索引下不能自动新增字段，但是在user.social_networks下可以自动新增子字段</p><h4 id="copy-to"><a href="#copy-to" class="headerlink" title="copy_to"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/copy-to.html" target="_blank" rel="noopener">copy_to</a></h4><ul><li>将该字段复制到目标字段，实现类似_all的作用</li><li>不会出现在_source中，只用来搜索</li></ul><pre><code>DELETE my_indexPUT my_index{  &quot;mappings&quot;: {    &quot;doc&quot;: {      &quot;properties&quot;: {        &quot;first_name&quot;: {          &quot;type&quot;: &quot;text&quot;,          &quot;copy_to&quot;: &quot;full_name&quot;         },        &quot;last_name&quot;: {          &quot;type&quot;: &quot;text&quot;,          &quot;copy_to&quot;: &quot;full_name&quot;         },        &quot;full_name&quot;: {          &quot;type&quot;: &quot;text&quot;        }      }    }  }}PUT my_index/doc/1{  &quot;first_name&quot;: &quot;John&quot;,  &quot;last_name&quot;: &quot;Smith&quot;}GET my_index/_search{  &quot;query&quot;: {    &quot;match&quot;: {      &quot;full_name&quot;: {         &quot;query&quot;: &quot;John Smith&quot;,        &quot;operator&quot;: &quot;and&quot;      }    }  }}</code></pre><h4 id="index"><a href="#index" class="headerlink" title="index"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-index.html" target="_blank" rel="noopener">index</a></h4><ul><li>控制当前字段是否索引，默认为true，即记录索引，false不记录，即不可搜索</li></ul><h4 id="index-options"><a href="#index-options" class="headerlink" title="index_options"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-options.html" target="_blank" rel="noopener">index_options</a></h4><ul><li>index_options参数控制将哪些信息添加到倒排索引，以用于搜索和突出显示，可选的值有：docs，freqs，positions，offsets</li><li>docs：只索引 doc id</li><li>freqs：索引 doc id 和词频，平分时可能要用到词频</li><li>positions：索引 doc id、词频、位置，做 proximity or phrase queries 时可能要用到位置信息</li><li>offsets：索引doc id、词频、位置、开始偏移和结束偏移，高亮功能需要用到offsets</li></ul><h4 id="fielddata"><a href="#fielddata" class="headerlink" title="fielddata"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/fielddata.html" target="_blank" rel="noopener">fielddata</a></h4><ul><li>是否预加载 fielddata，默认为false</li><li>Elasticsearch第一次查询时完整加载这个字段所有 Segment 中的倒排索引到内存中</li><li>如果我们有一些 5 GB 的索引段，并希望加载 10 GB 的 fielddata 到内存中，这个过程可能会要数十秒</li><li>将 fielddate 设置为 true ,将载入 fielddata 的代价转移到索引刷新的时候，而不是查询时，从而大大提高了搜索体验</li><li>参考：<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/preload-fielddata.html" target="_blank" rel="noopener">预加载 fielddata</a></li></ul><h4 id="eager-global-ordinals"><a href="#eager-global-ordinals" class="headerlink" title="eager_global_ordinals"></a>eager_global_ordinals</h4><ul><li>是否预构建全局序号，默认false</li><li>参考：<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/preload-fielddata.html#global-ordinals" target="_blank" rel="noopener">预构建全局序号（Eager global ordinals）</a></li></ul><h4 id="doc-values"><a href="#doc-values" class="headerlink" title="doc_values"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/doc-values.html" target="_blank" rel="noopener">doc_values</a></h4><ul><li>参考：<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/docvalues-and-fielddata.html" target="_blank" rel="noopener">Doc Values and Fielddata</a></li></ul><h4 id="fields"><a href="#fields" class="headerlink" title="fields"></a>fields</h4><ul><li>该参数的目的是为了实现 multi-fields</li><li>一个字段，多种数据类型</li><li>譬如：一个字段 city 的数据类型为 text ，用于全文索引，可以通过 fields 为该字段定义 keyword 类型，用于排序和聚合</li></ul><pre><code># 设置 mappingPUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;properties&quot;: {        &quot;city&quot;: {          &quot;type&quot;: &quot;text&quot;,          &quot;fields&quot;: {            &quot;raw&quot;: {               &quot;type&quot;:  &quot;keyword&quot;            }          }        }      }    }  }}# 插入两条数据PUT my_index/_doc/1{  &quot;city&quot;: &quot;New York&quot;}PUT my_index/_doc/2{  &quot;city&quot;: &quot;York&quot;}# 查询，city用于全文索引 match，city.raw用于排序和聚合GET my_index/_search{  &quot;query&quot;: {    &quot;match&quot;: {      &quot;city&quot;: &quot;york&quot;     }  },  &quot;sort&quot;: {    &quot;city.raw&quot;: &quot;asc&quot;   },  &quot;aggs&quot;: {    &quot;Cities&quot;: {      &quot;terms&quot;: {        &quot;field&quot;: &quot;city.raw&quot;       }    }  }}</code></pre><h4 id="format"><a href="#format" class="headerlink" title="format"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html" target="_blank" rel="noopener">format</a></h4><ul><li>由于JSON没有date类型，Elasticsearch预先通过format参数定义时间格式，将匹配的字符串识别为date类型，转换为时间戳（单位：毫秒）</li><li>format默认为：<code>strict_date_optional_time||epoch_millis</code></li><li>Elasticsearch内建的时间格式:</li></ul><table><thead><tr><th>名称</th><th>格式</th></tr></thead><tbody><tr><td>epoch_millis</td><td>时间戳（单位：毫秒）</td></tr><tr><td>epoch_second</td><td>时间戳（单位：秒）</td></tr><tr><td>date_optional_time</td><td></td></tr><tr><td>basic_date</td><td>yyyyMMdd</td></tr><tr><td>basic_date_time</td><td>yyyyMMdd’T’HHmmss.SSSZ</td></tr><tr><td>basic_date_time_no_millis</td><td>yyyyMMdd’T’HHmmssZ</td></tr><tr><td>basic_ordinal_date</td><td>yyyyDDD</td></tr><tr><td>basic_ordinal_date_time</td><td>yyyyDDD’T’HHmmss.SSSZ</td></tr><tr><td>basic_ordinal_date_time_no_millis</td><td>yyyyDDD’T’HHmmssZ</td></tr><tr><td>basic_time</td><td>HHmmss.SSSZ</td></tr><tr><td>basic_time_no_millis</td><td>HHmmssZ</td></tr><tr><td>basic_t_time</td><td>‘T’HHmmss.SSSZ</td></tr><tr><td>basic_t_time_no_millis</td><td>‘T’HHmmssZ</td></tr></tbody></table><ul><li>上述名称加前缀 <code>strict_</code> 表示为严格格式</li><li>更多的查看文档</li></ul><h4 id="properties"><a href="#properties" class="headerlink" title="properties"></a>properties</h4><ul><li>用于_doc，object和nested类型的字段定义<strong>子字段</strong></li></ul><pre><code>PUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {       &quot;properties&quot;: {        &quot;manager&quot;: {           &quot;properties&quot;: {            &quot;age&quot;:  { &quot;type&quot;: &quot;integer&quot; },            &quot;name&quot;: { &quot;type&quot;: &quot;text&quot;  }          }        },        &quot;employees&quot;: {           &quot;type&quot;: &quot;nested&quot;,          &quot;properties&quot;: {            &quot;age&quot;:  { &quot;type&quot;: &quot;integer&quot; },            &quot;name&quot;: { &quot;type&quot;: &quot;text&quot;  }          }        }      }    }  }}PUT my_index/_doc/1 {  &quot;region&quot;: &quot;US&quot;,  &quot;manager&quot;: {    &quot;name&quot;: &quot;Alice White&quot;,    &quot;age&quot;: 30  },  &quot;employees&quot;: [    {      &quot;name&quot;: &quot;John Smith&quot;,      &quot;age&quot;: 34    },    {      &quot;name&quot;: &quot;Peter Brown&quot;,      &quot;age&quot;: 26    }  ]}</code></pre><h4 id="normalizer"><a href="#normalizer" class="headerlink" title="normalizer"></a>normalizer</h4><ul><li>与 analyzer 类似，只不过 analyzer 用于 text 类型字段，分词产生多个 token，而 normalizer 用于 keyword 类型，只产生一个 token（整个字段的值作为一个token，而不是分词拆分为多个token）</li><li>定义一个自定义 normalizer，使用大写uppercase过滤器</li></ul><pre><code>PUT test_index_4{  &quot;settings&quot;: {    &quot;analysis&quot;: {      &quot;normalizer&quot;: {        &quot;my_normalizer&quot;: {          &quot;type&quot;: &quot;custom&quot;,          &quot;char_filter&quot;: [],          &quot;filter&quot;: [&quot;uppercase&quot;, &quot;asciifolding&quot;]        }      }    }  },  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;properties&quot;: {        &quot;foo&quot;: {          &quot;type&quot;: &quot;keyword&quot;,          &quot;normalizer&quot;: &quot;my_normalizer&quot;        }      }    }  }}# 插入数据POST test_index_4/_doc/1{  &quot;foo&quot;: &quot;hello world&quot;}POST test_index_4/_doc/2{  &quot;foo&quot;: &quot;Hello World&quot;}POST test_index_4/_doc/3{  &quot;foo&quot;: &quot;hello elasticsearch&quot;}# 搜索hello，结果为空，而不是3条！！ GET test_index_4/_search{  &quot;query&quot;: {    &quot;match&quot;: {      &quot;foo&quot;: &quot;hello&quot;    }  }}# 搜索 hello world，结果2条，1 和 2GET test_index_4/_search{  &quot;query&quot;: {    &quot;match&quot;: {      &quot;foo&quot;: &quot;hello world&quot;    }  }}</code></pre><h4 id="其他字段"><a href="#其他字段" class="headerlink" title="其他字段"></a>其他字段</h4><ul><li>coerce<ul><li>强制类型转换，把json中的值转为ES中字段的数据类型，譬如：把字符串”5”转为integer的5</li><li>coerce默认为 true</li><li>如果coerce设置为 false，当json的值与es字段类型不匹配将会 rejected</li><li>通过 “settings”: { “index.mapping.coerce”: false } 设置索引的 coerce</li></ul></li><li>enabled<ul><li>是否索引，默认为 true</li><li>可以在_doc和字段两个粒度进行设置</li></ul></li><li>ignore_above<ul><li>设置能被索引的字段的长度</li><li>超过这个长度，该字段将不被索引，所以无法搜索，但聚合的terms可以看到</li></ul></li><li>null_value<ul><li>该字段定义遇到null值时的处理策略，默认为Null，即空值，此时ES会忽略该值</li><li>通过设定该值可以设定字段为 null 时的默认值</li></ul></li><li>ignore_malformed<ul><li>当数据类型不匹配且 coerce 强制转换时,默认情况会抛出异常,并拒绝整个文档的插入</li><li>若设置该参数为 true，则忽略该异常，并强制赋值，但是不会被索引，其他字段则照常</li></ul></li><li>norms<ul><li>norms 存储各种标准化因子，为后续查询计算文档对该查询的匹配分数提供依据</li><li>norms 参数对<strong>评分</strong>很有用，但需要占用大量的磁盘空间</li><li>如果不需要计算字段的评分，可以取消该字段 norms 的功能</li></ul></li><li>position_increment_gap<ul><li>与 proximity queries（近似查询）和 phrase queries（短语查询）有关</li><li>默认值 100</li></ul></li><li>search_analyzer<ul><li>搜索分词器，查询时使用</li><li>默认与 analyzer 一样</li></ul></li><li>similarity<ul><li>设置相关度算法，ES5.x 和 ES6.x 默认的算法为 BM25</li><li>另外也可选择 classic 和 boolean</li></ul></li><li>store<ul><li>store 的意思是：是否在 _source 之外在独立存储一份，默认值为 false</li><li>es在存储数据的时候把json对象存储到”_source”字段里，”_source”把所有字段保存为一份文档存储（读取需要1次IO），要取出某个字段则通过 source filtering 过滤</li><li>当字段比较多或者内容比较多，并且不需要取出所有字段的时候，可以把特定字段的store设置为true单独存储（读取需要1次IO），同时在_source设置exclude</li><li>关于该字段的理解，参考： <a href="https://blog.csdn.net/helllochun/article/details/52136954" target="_blank" rel="noopener">es设置mapping store属性</a></li></ul></li><li>term_vector<ul><li>与倒排索引相关</li></ul></li></ul><h3 id="Dynamic-Mapping"><a href="#Dynamic-Mapping" class="headerlink" title="Dynamic Mapping"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-mapping.html" target="_blank" rel="noopener">Dynamic Mapping</a></h3><p>ES是依靠JSON文档的字段类型来实现自动识别字段类型，支持的类型如下：</p><table><thead><tr><th>JSON 类型</th><th>ES 类型</th></tr></thead><tbody><tr><td>null</td><td>忽略</td></tr><tr><td>boolean</td><td>boolean</td></tr><tr><td>浮点类型</td><td>float</td></tr><tr><td>整数</td><td>long</td></tr><tr><td>object</td><td>object</td></tr><tr><td>array</td><td>由第一个非 null 值的类型决定</td></tr><tr><td>string</td><td>匹配为日期则设为date类型（默认开启）；<br>匹配为数字则设置为 float或long类型（默认关闭）；<br>设为text类型，并附带keyword的子字段</td></tr></tbody></table><p>举栗子</p><pre><code>POST my_index/doc{  &quot;username&quot;:&quot;whirly&quot;,  &quot;age&quot;:22,  &quot;birthday&quot;:&quot;1995-01-01&quot;}GET my_index/_mapping# 结果{  &quot;my_index&quot;: {    &quot;mappings&quot;: {      &quot;doc&quot;: {        &quot;properties&quot;: {          &quot;age&quot;: {            &quot;type&quot;: &quot;long&quot;          },          &quot;birthday&quot;: {            &quot;type&quot;: &quot;date&quot;          },          &quot;username&quot;: {            &quot;type&quot;: &quot;text&quot;,            &quot;fields&quot;: {              &quot;keyword&quot;: {                &quot;type&quot;: &quot;keyword&quot;,                &quot;ignore_above&quot;: 256              }            }          }        }      }    }  }}</code></pre><h4 id="日期的自动识别"><a href="#日期的自动识别" class="headerlink" title="日期的自动识别"></a>日期的自动识别</h4><ul><li>dynamic_date_formats 参数为自动识别的日期格式，默认为 [ “strict_date_optional_time”,”yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z”]</li><li>date_detection可以关闭日期自动识别机制</li></ul><pre><code># 自定义日期识别格式PUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;dynamic_date_formats&quot;: [&quot;MM/dd/yyyy&quot;]    }  }}# 关闭日期自动识别机制PUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;date_detection&quot;: false    }  }}</code></pre><h4 id="数字的自动识别"><a href="#数字的自动识别" class="headerlink" title="数字的自动识别"></a>数字的自动识别</h4><ul><li>字符串是数字时，默认不会自动识别为整形，因为字符串中出现数字完全是合理的</li><li>numeric_detection 参数可以开启字符串中数字的自动识别</li></ul><h3 id="Dynamic-templates"><a href="#Dynamic-templates" class="headerlink" title="Dynamic templates"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-templates.html" target="_blank" rel="noopener">Dynamic templates</a></h3><p>允许根据ES自动识别的数据类型、字段名等来动态设定字段类型，可以实现如下效果：</p><ul><li>所有字符串类型都设定为keyword类型，即不分词</li><li>所有以message开头的字段都设定为text类型，即分词</li><li>所有以long_开头的字段都设定为long类型</li><li>所有自动匹配为double类型的都设定为float类型，以节省空间</li></ul><h4 id="Dynamic-templates-API"><a href="#Dynamic-templates-API" class="headerlink" title="Dynamic templates API"></a>Dynamic templates API</h4><pre><code>&quot;dynamic_templates&quot;: [    {      &quot;my_template_name&quot;: {         ...  match conditions ...         &quot;mapping&quot;: { ... }       }    },    ...]</code></pre><p>匹配规则一般有如下几个参数：</p><ul><li>match_mapping_type 匹配ES自动识别的字段类型，如boolean，long，string等</li><li>match, unmatch 匹配字段名</li><li>match_pattern 匹配正则表达式</li><li>path_match, path_unmatch 匹配路径</li></ul><pre><code># double类型的字段设定为float以节省空间PUT my_index{  &quot;mappings&quot;: {    &quot;_doc&quot;: {      &quot;dynamic_templates&quot;: [        {          &quot;integers&quot;: {            &quot;match_mapping_type&quot;: &quot;double&quot;,            &quot;mapping&quot;: {              &quot;type&quot;: &quot;float&quot;            }          }        }      ]    }  }}</code></pre><h5 id="自定义Mapping的建议"><a href="#自定义Mapping的建议" class="headerlink" title="自定义Mapping的建议"></a>自定义Mapping的建议</h5><ol><li>写入一条文档到ES的临时索引中，获取ES自动生成的Mapping</li><li>修改步骤1得到的Mapping，自定义相关配置</li><li>使用步骤2的Mapping创建实际所需索引</li></ol><h3 id="Index-Template-索引模板"><a href="#Index-Template-索引模板" class="headerlink" title="Index Template 索引模板"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/indices-templates.html" target="_blank" rel="noopener">Index Template 索引模板</a></h3><ul><li>索引模板，主要用于在新建索引时自动应用预先设定的配置，简化索引创建的操作步骤<ul><li>可以设定索引的setting和mapping</li><li>可以有多个模板，根据order设置，order大的覆盖小的配置</li></ul></li><li>索引模板API，endpoint为 _template</li></ul><pre><code># 创建索引模板，匹配 test-index-map 开头的索引PUT _template/template_1{  &quot;index_patterns&quot;: [&quot;test-index-map*&quot;],  &quot;order&quot;: 2,  &quot;settings&quot;: {    &quot;number_of_shards&quot;: 1  },  &quot;mappings&quot;: {    &quot;doc&quot;: {      &quot;_source&quot;: {        &quot;enabled&quot;: false      },      &quot;properties&quot;: {        &quot;name&quot;: {          &quot;type&quot;: &quot;keyword&quot;        },        &quot;created_at&quot;: {          &quot;type&quot;: &quot;date&quot;,          &quot;format&quot;: &quot;YYYY/MM/dd HH:mm:ss&quot;        }      }    }  }}# 插入一个文档POST test-index-map_1/doc{  &quot;name&quot; : &quot;小旋锋&quot;,  &quot;created_at&quot;: &quot;2018/08/16 20:11:11&quot;}# 获取该索引的信息，可以发现 settings 和 mappings 和索引模板里设置的一样GET test-index-map_1# 删除DELETE /_template/template_1# 查询GET /_template/template_1</code></pre><blockquote><p>参考文档： </p><ol><li>elasticsearch 官方文档</li><li><a href="https://coding.imooc.com/class/181.html" target="_blank" rel="noopener">慕课网 Elastic Stack从入门到实践</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Mapping&quot;&gt;&lt;a href=&quot;#Mapping&quot; class=&quot;headerlink&quot; title=&quot;Mapping&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>倒排索引与分词</title>
    <link href="http://laijianfeng.org/2018/08/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%86%E8%AF%8D/"/>
    <id>http://laijianfeng.org/2018/08/倒排索引与分词/</id>
    <published>2018-08-15T15:50:57.000Z</published>
    <updated>2018-08-16T13:22:20.517Z</updated>
    
    <content type="html"><![CDATA[<h3 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h3><ul><li>正排索引：文档id到单词的关联关系</li><li>倒排索引：单词到文档id的关联关系</li></ul><p>示例：<br>对以下三个文档去除停用词后构造倒排索引<br><img src="http://image.laijianfeng.org/20180803_223406.png" alt="image"></p><h4 id="倒排索引-查询过程"><a href="#倒排索引-查询过程" class="headerlink" title="倒排索引-查询过程"></a>倒排索引-查询过程</h4><p>查询包含“搜索引擎”的文档</p><ol><li>通过倒排索引获得“搜索引擎”对应的文档id列表，有1，3</li><li>通过正排索引查询1和3的完整内容</li><li>返回最终结果</li></ol><h4 id="倒排索引-组成"><a href="#倒排索引-组成" class="headerlink" title="倒排索引-组成"></a>倒排索引-组成</h4><ul><li>单词词典（Term Dictionary）</li><li>倒排列表（Posting List）</li></ul><h4 id="单词词典（Term-Dictionary）"><a href="#单词词典（Term-Dictionary）" class="headerlink" title="单词词典（Term Dictionary）"></a>单词词典（Term Dictionary）</h4><p>单词词典的实现一般用B+树，B+树构造的可视化过程网址: <a href="https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html" target="_blank" rel="noopener">B+ Tree Visualization</a></p><blockquote><p>关于B树和B+树</p><ol><li><a href="https://zh.wikipedia.org/wiki/B%E6%A0%91" target="_blank" rel="noopener">维基百科-B树</a></li><li><a href="https://zh.wikipedia.org/wiki/B%2B%E6%A0%91" target="_blank" rel="noopener">维基百科-B+树</a></li><li><a href="https://www.cnblogs.com/nullzx/p/8729425.html" target="_blank" rel="noopener">B树和B+树的插入、删除图文详解</a></li></ol></blockquote><p><img src="http://image.laijianfeng.org/20180803_225118.png" alt="image"></p><h4 id="倒排列表（Posting-List）"><a href="#倒排列表（Posting-List）" class="headerlink" title="倒排列表（Posting List）"></a>倒排列表（Posting List）</h4><ul><li>倒排列表记录了单词对应的文档集合，有倒排索引项（Posting）组成</li><li>倒排索引项主要包含如下信息：<ol><li>文档id用于获取原始信息</li><li>单词频率（TF，Term Frequency），记录该单词在该文档中出现的次数，用于后续相关性算分</li><li>位置（Posting），记录单词在文档中的分词位置（多个），用于做词语搜索（Phrase Query）</li><li>偏移（Offset），记录单词在文档的开始和结束位置，用于高亮显示</li></ol></li></ul><p><img src="http://image.laijianfeng.org/20180803_225931.png" alt="image"></p><p>B+树<strong>内部结点存索引，叶子结点存数据</strong>，这里的 单词词典就是B+树索引，倒排列表就是数据，整合在一起后如下所示</p><p><img src="http://image.laijianfeng.org/20180803_232214.png" alt="image"></p><p>ES存储的是一个JSON格式的文档，其中包含多个字段，每个字段会有自己的倒排索引</p><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><p>分词是将文本转换成一系列单词（Term or Token）的过程，也可以叫文本分析，在ES里面称为Analysis</p><p><img src="http://image.laijianfeng.org/20180803_232909.png" alt="image"></p><h4 id="分词器"><a href="#分词器" class="headerlink" title="分词器"></a>分词器</h4><p>分词器是ES中专门处理分词的组件，英文为Analyzer，它的组成如下：</p><ul><li>Character Filters：针对原始文本进行处理，比如去除html标签</li><li>Tokenizer：将原始文本按照一定规则切分为单词</li><li>Token Filters：针对Tokenizer处理的单词进行再加工，比如转小写、删除或增新等处理</li></ul><p>分词器调用顺序<br><img src="http://image.laijianfeng.org/20180803_234047.png" alt="image"></p><h3 id="Analyze-API"><a href="#Analyze-API" class="headerlink" title="Analyze API"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html" target="_blank" rel="noopener">Analyze API</a></h3><p>ES提供了一个可以测试分词的API接口，方便验证分词效果，endpoint是_analyze</p><ul><li>可以直接指定analyzer进行测试</li></ul><p><img src="http://image.laijianfeng.org/20180803_234732.png" alt="image"></p><ul><li>可以直接指定索引中的字段进行测试</li></ul><pre><code>POST test_index/doc{  &quot;username&quot;: &quot;whirly&quot;,  &quot;age&quot;:22}POST test_index/_analyze{  &quot;field&quot;: &quot;username&quot;,  &quot;text&quot;: [&quot;hello world&quot;]}</code></pre><ul><li>可以自定义分词器进行测试</li></ul><pre><code>POST _analyze{  &quot;tokenizer&quot;: &quot;standard&quot;,  &quot;filter&quot;: [&quot;lowercase&quot;],  &quot;text&quot;: [&quot;Hello World&quot;]}</code></pre><h3 id="预定义的分词器"><a href="#预定义的分词器" class="headerlink" title="预定义的分词器"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html" target="_blank" rel="noopener">预定义的分词器</a></h3><p>ES自带的分词器有如下：</p><ul><li>Standard Analyzer<ul><li>默认分词器</li><li>按词切分，支持多语言</li><li>小写处理</li></ul></li><li>Simple Analyzer<ul><li>按照非字母切分</li><li>小写处理</li></ul></li><li>Whitespace Analyzer<ul><li>空白字符作为分隔符</li></ul></li><li>Stop Analyzer<ul><li>相比Simple Analyzer多了去除请用词处理</li><li>停用词指语气助词等修饰性词语，如the, an, 的， 这等</li></ul></li><li>Keyword Analyzer<ul><li>不分词，直接将输入作为一个单词输出</li></ul></li><li>Pattern Analyzer<ul><li>通过正则表达式自定义分隔符</li><li>默认是\W+，即非字词的符号作为分隔符</li></ul></li><li>Language Analyzer<ul><li>提供了30+种常见语言的分词器</li></ul></li></ul><p>示例：停用词分词器</p><pre><code>POST _analyze{  &quot;analyzer&quot;: &quot;stop&quot;,  &quot;text&quot;: [&quot;The 2 QUICK Brown Foxes jumped over the lazy dog&#39;s bone.&quot;]}</code></pre><p>结果</p><pre><code>{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;quick&quot;,      &quot;start_offset&quot;: 6,      &quot;end_offset&quot;: 11,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;brown&quot;,      &quot;start_offset&quot;: 12,      &quot;end_offset&quot;: 17,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 2    },    {      &quot;token&quot;: &quot;foxes&quot;,      &quot;start_offset&quot;: 18,      &quot;end_offset&quot;: 23,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 3    },    {      &quot;token&quot;: &quot;jumped&quot;,      &quot;start_offset&quot;: 24,      &quot;end_offset&quot;: 30,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 4    },    {      &quot;token&quot;: &quot;over&quot;,      &quot;start_offset&quot;: 31,      &quot;end_offset&quot;: 35,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 5    },    {      &quot;token&quot;: &quot;lazy&quot;,      &quot;start_offset&quot;: 40,      &quot;end_offset&quot;: 44,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 7    },    {      &quot;token&quot;: &quot;dog&quot;,      &quot;start_offset&quot;: 45,      &quot;end_offset&quot;: 48,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 8    },    {      &quot;token&quot;: &quot;s&quot;,      &quot;start_offset&quot;: 49,      &quot;end_offset&quot;: 50,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 9    },    {      &quot;token&quot;: &quot;bone&quot;,      &quot;start_offset&quot;: 51,      &quot;end_offset&quot;: 55,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 10    }  ]}</code></pre><h3 id="中文分词"><a href="#中文分词" class="headerlink" title="中文分词"></a>中文分词</h3><ul><li>难点<ul><li>中文分词指的是将一个汉字序列切分为一个一个的单独的词。在英文中，单词之间以空格作为自然分界词，汉语中词没有一个形式上的分界符</li><li>上下文不同，分词结果迥异，比如交叉歧义问题</li></ul></li><li>常见分词系统<ul><li><a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">IK</a>：实现中英文单词的切分，可自定义词库，支持热更新分词词典</li><li><a href="https://github.com/sing1ee/elasticsearch-jieba-plugin" target="_blank" rel="noopener">jieba</a>：支持分词和词性标注，支持繁体分词，自定义词典，并行分词等</li><li><a href="https://github.com/hankcs/HanLP" target="_blank" rel="noopener">Hanlp</a>：由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用</li><li><a href="https://github.com/microbun/elasticsearch-thulac-plugin" target="_blank" rel="noopener">THUAC</a>：中文分词和词性标注</li></ul></li></ul><h4 id="安装ik中文分词插件"><a href="#安装ik中文分词插件" class="headerlink" title="安装ik中文分词插件"></a>安装ik中文分词插件</h4><pre><code># 在Elasticsearch安装目录下执行命令，然后重启esbin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip# 如果由于网络慢，安装失败，可以先下载好zip压缩包，将下面命令改为实际的路径，执行，然后重启esbin/elasticsearch-plugin install file:///path/to/elasticsearch-analysis-ik-6.3.0.zip</code></pre><ul><li>ik测试 - ik_smart</li></ul><pre><code>POST _analyze{  &quot;analyzer&quot;: &quot;ik_smart&quot;,  &quot;text&quot;: [&quot;公安部：各地校车将享最高路权&quot;]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;公安部&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 3,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 0    },    {      &quot;token&quot;: &quot;各地&quot;,      &quot;start_offset&quot;: 4,      &quot;end_offset&quot;: 6,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;校车&quot;,      &quot;start_offset&quot;: 6,      &quot;end_offset&quot;: 8,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 2    },    {      &quot;token&quot;: &quot;将&quot;,      &quot;start_offset&quot;: 8,      &quot;end_offset&quot;: 9,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 3    },    {      &quot;token&quot;: &quot;享&quot;,      &quot;start_offset&quot;: 9,      &quot;end_offset&quot;: 10,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 4    },    {      &quot;token&quot;: &quot;最高&quot;,      &quot;start_offset&quot;: 10,      &quot;end_offset&quot;: 12,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 5    },    {      &quot;token&quot;: &quot;路&quot;,      &quot;start_offset&quot;: 12,      &quot;end_offset&quot;: 13,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 6    },    {      &quot;token&quot;: &quot;权&quot;,      &quot;start_offset&quot;: 13,      &quot;end_offset&quot;: 14,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 7    }  ]}</code></pre><ul><li>ik测试 - ik_max_word</li></ul><pre><code>POST _analyze{  &quot;analyzer&quot;: &quot;ik_max_word&quot;,  &quot;text&quot;: [&quot;公安部：各地校车将享最高路权&quot;]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;公安部&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 3,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 0    },    {      &quot;token&quot;: &quot;公安&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 2,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;部&quot;,      &quot;start_offset&quot;: 2,      &quot;end_offset&quot;: 3,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 2    },    {      &quot;token&quot;: &quot;各地&quot;,      &quot;start_offset&quot;: 4,      &quot;end_offset&quot;: 6,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 3    },    {      &quot;token&quot;: &quot;校车&quot;,      &quot;start_offset&quot;: 6,      &quot;end_offset&quot;: 8,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 4    },    {      &quot;token&quot;: &quot;将&quot;,      &quot;start_offset&quot;: 8,      &quot;end_offset&quot;: 9,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 5    },    {      &quot;token&quot;: &quot;享&quot;,      &quot;start_offset&quot;: 9,      &quot;end_offset&quot;: 10,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 6    },    {      &quot;token&quot;: &quot;最高&quot;,      &quot;start_offset&quot;: 10,      &quot;end_offset&quot;: 12,      &quot;type&quot;: &quot;CN_WORD&quot;,      &quot;position&quot;: 7    },    {      &quot;token&quot;: &quot;路&quot;,      &quot;start_offset&quot;: 12,      &quot;end_offset&quot;: 13,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 8    },    {      &quot;token&quot;: &quot;权&quot;,      &quot;start_offset&quot;: 13,      &quot;end_offset&quot;: 14,      &quot;type&quot;: &quot;CN_CHAR&quot;,      &quot;position&quot;: 9    }  ]}</code></pre><ul><li><p>ik两种分词模式ik_max_word 和 ik_smart 什么区别?</p><ul><li><p>ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；</p></li><li><p>ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。</p></li></ul></li></ul><h3 id="自定义分词"><a href="#自定义分词" class="headerlink" title="自定义分词"></a>自定义分词</h3><p>当自带的分词无法满足需求时，可以自定义分词，通过定义Character Filters、Tokenizer和Token Filters实现</p><h4 id="Character-Filters"><a href="#Character-Filters" class="headerlink" title="Character Filters"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-charfilters.html" target="_blank" rel="noopener">Character Filters</a></h4><ul><li>在Tokenizer之前对原始文本进行处理，比如增加、删除或替换字符等</li><li>自带的如下:<ul><li>HTML Strip Character Filter：去除HTML标签和转换HTML实体</li><li>Mapping Character Filter：进行字符替换操作</li><li>Pattern Replace Character Filter：进行正则匹配替换</li></ul></li><li>会影响后续tokenizer解析的position和offset信息</li></ul><h4 id="Character-Filters测试"><a href="#Character-Filters测试" class="headerlink" title="Character Filters测试"></a>Character Filters测试</h4><pre><code>POST _analyze{  &quot;tokenizer&quot;: &quot;keyword&quot;,  &quot;char_filter&quot;: [&quot;html_strip&quot;],  &quot;text&quot;: [&quot;&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;&quot;]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;&quot;&quot;I&#39;m so happy!&quot;&quot;&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 32,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 0    }  ]}</code></pre><h4 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html" target="_blank" rel="noopener">Tokenizers</a></h4><ul><li>将原始文本按照一定规则切分为单词（term or token）</li><li>自带的如下：<ul><li>standard 按照单词进行分割</li><li>letter 按照非字符类进行分割</li><li>whitespace 按照空格进行分割</li><li>UAX URL Email 按照standard进行分割，但不会分割邮箱和URL</li><li>Ngram 和 Edge NGram 连词分割</li><li>Path Hierarchy 按照文件路径进行分割</li></ul></li></ul><h4 id="Tokenizers-测试"><a href="#Tokenizers-测试" class="headerlink" title="Tokenizers 测试"></a>Tokenizers 测试</h4><pre><code>POST _analyze{  &quot;tokenizer&quot;: &quot;path_hierarchy&quot;,  &quot;text&quot;: [&quot;/path/to/file&quot;]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;/path&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 5,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 0    },    {      &quot;token&quot;: &quot;/path/to&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 8,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 0    },    {      &quot;token&quot;: &quot;/path/to/file&quot;,      &quot;start_offset&quot;: 0,      &quot;end_offset&quot;: 13,      &quot;type&quot;: &quot;word&quot;,      &quot;position&quot;: 0    }  ]}</code></pre><h4 id="Token-Filters"><a href="#Token-Filters" class="headerlink" title="Token Filters"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html" target="_blank" rel="noopener">Token Filters</a></h4><ul><li>对于tokenizer输出的单词（term）进行增加、删除、修改等操作</li><li>自带的如下：<ul><li>lowercase 将所有term转为小写</li><li>stop 删除停用词</li><li>Ngram 和 Edge NGram 连词分割</li><li>Synonym 添加近义词的term</li></ul></li></ul><h4 id="Token-Filters测试"><a href="#Token-Filters测试" class="headerlink" title="Token Filters测试"></a>Token Filters测试</h4><pre><code>POST _analyze{  &quot;text&quot;: [    &quot;a Hello World!&quot;  ],  &quot;tokenizer&quot;: &quot;standard&quot;,  &quot;filter&quot;: [    &quot;stop&quot;,    &quot;lowercase&quot;,    {      &quot;type&quot;: &quot;ngram&quot;,      &quot;min_gram&quot;: 4,      &quot;max_gram&quot;: 4    }  ]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;hell&quot;,      &quot;start_offset&quot;: 2,      &quot;end_offset&quot;: 7,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;ello&quot;,      &quot;start_offset&quot;: 2,      &quot;end_offset&quot;: 7,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;worl&quot;,      &quot;start_offset&quot;: 8,      &quot;end_offset&quot;: 13,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 2    },    {      &quot;token&quot;: &quot;orld&quot;,      &quot;start_offset&quot;: 8,      &quot;end_offset&quot;: 13,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 2    }  ]}</code></pre><h4 id="自定义分词-1"><a href="#自定义分词-1" class="headerlink" title="自定义分词"></a>自定义分词</h4><p>自定义分词需要在索引配置中设定 char_filter、tokenizer、filter、analyzer等</p><p>自定义分词示例:</p><ul><li>分词器名称：my_custom\</li><li>过滤器将token转为大写</li></ul><pre><code>PUT test_index_1{  &quot;settings&quot;: {    &quot;analysis&quot;: {      &quot;analyzer&quot;: {        &quot;my_custom_analyzer&quot;: {          &quot;type&quot;:      &quot;custom&quot;,          &quot;tokenizer&quot;: &quot;standard&quot;,          &quot;char_filter&quot;: [            &quot;html_strip&quot;          ],          &quot;filter&quot;: [            &quot;uppercase&quot;,            &quot;asciifolding&quot;          ]        }      }    }  }}</code></pre><h4 id="自定义分词器测试"><a href="#自定义分词器测试" class="headerlink" title="自定义分词器测试"></a>自定义分词器测试</h4><pre><code>POST test_index_1/_analyze{  &quot;analyzer&quot;: &quot;my_custom_analyzer&quot;,  &quot;text&quot;: [&quot;&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;&quot;]}# 结果{  &quot;tokens&quot;: [    {      &quot;token&quot;: &quot;I&#39;M&quot;,      &quot;start_offset&quot;: 3,      &quot;end_offset&quot;: 11,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 0    },    {      &quot;token&quot;: &quot;SO&quot;,      &quot;start_offset&quot;: 12,      &quot;end_offset&quot;: 14,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 1    },    {      &quot;token&quot;: &quot;HAPPY&quot;,      &quot;start_offset&quot;: 18,      &quot;end_offset&quot;: 27,      &quot;type&quot;: &quot;&lt;ALPHANUM&gt;&quot;,      &quot;position&quot;: 2    }  ]}</code></pre><h4 id="分词使用说明"><a href="#分词使用说明" class="headerlink" title="分词使用说明"></a>分词使用说明</h4><p>分词会在如下两个时机使用：</p><ul><li>创建或更新文档时(Index Time)，会对相应的文档进行分词处理</li><li>查询时（Search Time），会对查询语句进行分词<ul><li>查询时通过analyzer指定分词器</li><li>通过index mapping设置search_analyzer实现</li><li>一般不需要特别指定查询时分词器，直接使用索引分词器即可，否则会出现无法匹配的情况</li></ul></li></ul><h4 id="分词使用建议"><a href="#分词使用建议" class="headerlink" title="分词使用建议"></a>分词使用建议</h4><ul><li>明确字段是否需要分词，不需要分词的字段就将type设置为keyword，可以节省空间和提高写性能</li><li>善用_analyze API，查看文档的分词结果</li></ul><blockquote><p>更多内容请访问我的个人网站： <a href="http://laijianfeng.org">http://laijianfeng.org</a><br>参考文档： </p><ol><li>elasticsearch 官方文档</li><li><a href="https://coding.imooc.com/class/181.html" target="_blank" rel="noopener">慕课网 Elastic Stack从入门到实践</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;倒排索引&quot;&gt;&lt;a href=&quot;#倒排索引&quot; class=&quot;headerlink&quot; title=&quot;倒排索引&quot;&gt;&lt;/a&gt;倒排索引&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;正排索引：文档id到单词的关联关系&lt;/li&gt;
&lt;li&gt;倒排索引：单词到文档id的关联关系&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch初体验</title>
    <link href="http://laijianfeng.org/2018/08/ElasticSearch%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <id>http://laijianfeng.org/2018/08/ElasticSearch初体验/</id>
    <published>2018-08-15T11:37:25.000Z</published>
    <updated>2018-08-15T11:43:46.489Z</updated>
    
    <content type="html"><![CDATA[<h3 id="需要明白的问题"><a href="#需要明白的问题" class="headerlink" title="需要明白的问题"></a>需要明白的问题</h3><ol><li>什么是倒排索引？它的组成是什么？</li><li>常见的相关性算分方法有哪些？</li><li>为什么查询语句没有返回预期的文档？</li><li>常用的数据类型有哪些？Text和Keyword的区别是什么？</li><li>集群是如何搭建起来的？是如何实现故障转移的？</li><li>Shard具体是由什么组成的？</li></ol><h2 id="Elastic-Stack"><a href="#Elastic-Stack" class="headerlink" title="Elastic Stack"></a>Elastic Stack</h2><p>构建在开源基础之上, Elastic Stack 让您能够安全可靠地获取任何来源、任何格式的数据，并且能够实时地对数据进行搜索、分析和可视化</p><p><strong>Elasticsearch</strong> 是基于 JSON 的分布式搜索和分析引擎，专为实现水平扩展、高可用和管理便捷性而设计。</p><p><strong>Kibana</strong> 能够以图表的形式呈现数据，并且具有可扩展的用户界面，供您全方位配置和管理 Elastic Stack。</p><p><strong>Logstash</strong> 是动态数据收集管道，拥有可扩展的插件生态系统，能够与 Elasticsearch 产生强大的协同作用。</p><p><strong>Beats</strong> 是轻量型采集器的平台，从边缘机器向 Logstash 和 Elasticsearch 发送数据。</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_basic_concepts.html" target="_blank" rel="noopener">基础概念</a></h3><ul><li>文档 Document ：用户存储在ES中的数据文档</li><li>索引 Index ：由具有一些相同字段的文档的集合</li><li>类型 Type :  允许将不同类型的文档存储在同一索引中，6.0开始官方不允许在一个index下建立多个type，统一type名称：doc</li><li>节点 Node ：一个Elasticsearch的运行实例，是集群的构成单元，存储部分或全部数据，并参与集群的索引和搜索功能</li><li>集群 Cluster ：由一个或多个节点组成的集合，共同保存所有的数据，对外提供服务（包括跨所有节点的联合索引和搜索功能等）</li><li>分片 Shards ：分片是为了解决存储大规模数据的问题，将数据切分分别存储到不同的分片中</li><li>副本 Replicas ：副本可以在分片或节点发生故障时提高可用性，而且由于可以在所有副本上进行并行搜索，所以也可以提高集群的吞吐量</li><li>近实时 Near Realtime(NRT)：从索引文档到可搜索文档的时间有一点延迟（通常为一秒）</li></ul><blockquote><p>note:</p><ol><li>在创建索引的时候如果没有配置索引Mapping，一个索引默认有5个shard和1个副本，一个索引总共有10个shard（算上副本shard）</li><li>Elasticsearch 的shard实际上是一个Lucene索引，截止Lucene-5843，一个Lucene索引限制的最大文档数为2,147,483,519 (= Integer.MAX_VALUE - 128)</li></ol></blockquote><h3 id="安装Elasticsearch-amp-Kibana"><a href="#安装Elasticsearch-amp-Kibana" class="headerlink" title="安装Elasticsearch &amp; Kibana"></a>安装Elasticsearch &amp; Kibana</h3><p>ES和Kibana的安装很简单，前提需要先安装好Java8，然后执行以下命令即可</p><h5 id="elasticsearch单节点最简安装"><a href="#elasticsearch单节点最简安装" class="headerlink" title="elasticsearch单节点最简安装"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html" target="_blank" rel="noopener">elasticsearch单节点最简安装</a></h5><pre><code># 在Ubuntu16.04上安装，方式有很多种，选择二进制压缩包的方式安装# 1. 在普通用户家目录下，下载压缩包curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.tar.gz# 2. 解压tar -xvf elasticsearch-6.3.2.tar.gz# 3. 移动至/opt目录下sudo mv elasticsearch-6.3.2 /opt# 4. 修改配置文件elasticsearch.yml中的 network.host 值为 0.0.0.0，其他的配置参考官方文档cd /opt/elasticsearch-6.3.2vi config/elasticsearch.yml# 5. 启动单节点，然后浏览器访问host:9200即可看到ES集群信息bin/elasticsearch</code></pre><p><img src="http://image.laijianfeng.org/20180815_153244.png" alt="image"></p><h5 id="kibana最简安装"><a href="#kibana最简安装" class="headerlink" title="kibana最简安装"></a><a href="https://www.elastic.co/guide/en/kibana/current/targz.html" target="_blank" rel="noopener">kibana最简安装</a></h5><pre><code>wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gzshasum -a 512 kibana-6.3.2-linux-x86_64.tar.gz tar -xzf kibana-6.3.2-linux-x86_64.tar.gzsudo mv kibana-6.3.2-linux-x86_64 /optcd /opt/kibana-6.3.2-linux-x86_64# 修改 config/kibana.yml中 server.host: 0.0.0.0# 启动Kibana，访问 host:5601即可进入kibana界面</code></pre><p><img src="http://image.laijianfeng.org/20180815_153435.png" alt="image"></p><h3 id="交互方式-Rest-API"><a href="#交互方式-Rest-API" class="headerlink" title="交互方式 Rest API"></a>交互方式 Rest API</h3><p>Elasticsearch集群对外提供RESTful API</p><ul><li>Curl命令行</li><li>Kibana Devtools</li><li>Java API</li><li>其他各种API，如Python API等</li></ul><blockquote><p>note:<br>我们后面主要使用 Kibana Devtools 这种交互方式</p></blockquote><p><img src="http://image.laijianfeng.org/20180815_154136.png" alt="image"></p><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html" target="_blank" rel="noopener">数据类型</a></h3><ul><li>字符串： text（分词）, keyword（不分词）</li><li>数值型： long, integer, byte, double, float, half_float, scaled_float</li><li>布尔： boolean</li><li>日期： date</li><li>二进制： binary</li><li>范围类型： integer_range, float_range, long_range, double_range, date_range</li><li>复杂数据类型： Array, Object, Nested</li><li>地理： geo_point， geo_shape</li><li>专业： ip，completion， token_count， murmur3， Percolator， join</li><li>组合的</li></ul><h3 id="探索ES集群"><a href="#探索ES集群" class="headerlink" title="探索ES集群"></a>探索ES集群</h3><p>Check your cluster, node, and index health, status, and statistics<br>Administer your cluster, node, and index data and metadata<br>Perform CRUD (Create, Read, Update, and Delete) and search operations against your indexes<br>Execute advanced search operations such as paging, sorting, filtering, scripting, aggregations, and many others</p><h5 id="使用-cat-API探索集群的健康情况"><a href="#使用-cat-API探索集群的健康情况" class="headerlink" title="使用_cat API探索集群的健康情况"></a>使用_cat API探索集群的健康情况</h5><pre><code>GET /_cat/health?v# 结果epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1534319381 15:49:41  elasticsearch green           3         3    118  59    0    0        0             0                  -                100.0%</code></pre><p>集群的健康状态(status)有三种:</p><ul><li>green：一切正常（集群功能齐全）</li><li>yellow：所有数据都可用，但存在一些副本未分配（群集功能齐全）</li><li>red：一些数据由于某种原因不可用（群集部分功能失效）</li></ul><h5 id="查看节点信息"><a href="#查看节点信息" class="headerlink" title="查看节点信息"></a>查看节点信息</h5><pre><code>GET /_cat/nodes?v# 结果（我的ES集群安装了三个节点）ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name10.100.97.207           30          96  13    0.15    0.08     0.08 mdi       *      master10.100.97.246           68          96   3    0.00    0.00     0.00 mdi       -      hadoop210.100.98.22            15          97   2    0.00    0.02     0.04 mdi       -      hadoop3</code></pre><h5 id="查看索引信息"><a href="#查看索引信息" class="headerlink" title="查看索引信息"></a>查看索引信息</h5><pre><code>GET /_cat/indices?v# 结果health status index                           uuid                   pri rep docs.count docs.deleted store.size pri.store.sizegreen  open   logstash-2015.05.20             4BjPjpq6RhOSCNUPMsY0MQ   5   1       4750            0     46.8mb         24.5mbgreen  open   logstash-2015.05.18             mDkUKHSWR0a8UeZlKzts8Q   5   1       4631            0     45.6mb         23.8mbgreen  open   hockey                          g1omiazvRSOE117w_uy_wA   5   1         11            0     45.3kb         22.6kbgreen  open   .kibana                         AGdo8im_TxC04ARexUxqxw   1   1        143           10    665.6kb        332.8kbgreen  open   shakespeare                     5009bDa7T16f5qTeyOdTlw   5   1     111396            0     43.9mb           22mbgreen  open   logstash-2015.05.19             az4Jen4nT7-J9yRYpZ0A9A   5   1       4624            0     44.7mb         23.1mb...</code></pre><h3 id="操作数据"><a href="#操作数据" class="headerlink" title="操作数据"></a>操作数据</h3><h5 id="插入文档并查询"><a href="#插入文档并查询" class="headerlink" title="插入文档并查询"></a>插入文档并查询</h5><pre><code># 插入一个文档PUT /customer/_doc/1?pretty{  &quot;name&quot;: &quot;John Doe&quot;}# 结果{  &quot;_index&quot;: &quot;customer&quot;,  &quot;_type&quot;: &quot;_doc&quot;,  &quot;_id&quot;: &quot;1&quot;,  &quot;_version&quot;: 1,  &quot;result&quot;: &quot;updated&quot;,  &quot;_shards&quot;: {    &quot;total&quot;: 2,    &quot;successful&quot;: 2,    &quot;failed&quot;: 0  },  &quot;_seq_no&quot;: 1,  &quot;_primary_term&quot;: 1}# 查询该文档GET /customer/_doc/1#结果{  &quot;_index&quot;: &quot;customer&quot;,  &quot;_type&quot;: &quot;_doc&quot;,  &quot;_id&quot;: &quot;1&quot;,  &quot;_version&quot;: 1,  &quot;found&quot;: true,  &quot;_source&quot;: {    &quot;name&quot;: &quot;John Doe&quot;  }}</code></pre><blockquote><p>note:</p><ol><li><code>customer</code> 为索引名，<code>_doc</code> 为type，1为文档_id，需要注意的是：在es6.x建议索引的type值固定为<code>_doc</code>，在之后的版本将删除type了；文档id若不指定，es会自动分配一个_id给文档</li><li>插入文档后，查看索引信息<code>GET /_cat/indices?v</code>可以看到多了 customer 的索引信息</li><li>文档结果，_source字段是原始的json内容，其他的为文档元数据</li></ol></blockquote><h5 id="文档元数据"><a href="#文档元数据" class="headerlink" title="文档元数据"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-fields.html" target="_blank" rel="noopener">文档元数据</a></h5><p>用于标注文档的元信息</p><ul><li>_index: 文档所在的索引名</li><li>_type: 文档所在的类型名</li><li>_id: 文档的唯一id</li><li>_uid: 组合id，由_type和_id组成（6.0开始_type不再起作用，同_id一样）</li><li>_source: 文档的原始json数据，可以从这里获取每个字段的内容</li><li>_all: 整合所有字段内容到该字段，默认禁用</li><li>_routing 默认值为 _id，决定文档存储在哪个shard上：<code>shard_num = hash(_routing) % num_primary_shards</code> </li></ul><h5 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h5><pre><code>DELETE customer#结果{  &quot;acknowledged&quot;: true}GET /_cat/indices?v# 再次查看索引信息，可以发现 customer 不存在，已被删除</code></pre><h5 id="更新文档"><a href="#更新文档" class="headerlink" title="更新文档"></a>更新文档</h5><pre><code>PUT /customer/_doc/1?pretty{  &quot;name&quot;: &quot;John Doe&quot;}POST /customer/_doc/1/_update{  &quot;doc&quot;: { &quot;name&quot;: &quot;Jane Doe&quot; }}POST /customer/_doc/1/_update{  &quot;doc&quot;: { &quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 20 }}# 可以看到 \_version的值一直在增加</code></pre><h5 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h5><pre><code>DELETE /customer/_doc/2</code></pre><h5 id="批量操作"><a href="#批量操作" class="headerlink" title="批量操作"></a>批量操作</h5><p>es提供了<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/docs-bulk.html" target="_blank" rel="noopener">_bulk API</a>供批量操作，可以提高索引、更新、删除等操作的效率</p><p>_bulk操作的类型有四种：</p><ul><li>index 索引：若已存在，则覆盖，文档不存在则创建</li><li>create 创建：文档不存在则异常</li><li>delete 删除</li><li>update 更新</li></ul><pre><code># _bulk 任务：# 1. index创建 customer索引下id为3的文档# 2. delete删除 customer索引下id为3的文档# 3. create创建 customer索引下id为3的文档# 4. update更新 customer索引下id为3的文档POST _bulk{&quot;index&quot;:{&quot;_index&quot;:&quot;customer&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;3&quot;}}{&quot;name&quot;:&quot;whirly&quot;}{&quot;delete&quot;:{&quot;_index&quot;:&quot;customer&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;3&quot;}}{&quot;create&quot;:{&quot;_index&quot;:&quot;customer&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;3&quot;}}{&quot;name&quot;:&quot;whirly2&quot;}{&quot;update&quot;:{&quot;_index&quot;:&quot;customer&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;3&quot;}}{&quot;doc&quot;:{&quot;name&quot;:&quot;whirly3&quot;}}</code></pre><p><img src="http://image.laijianfeng.org/20180815_164226.png" alt="image"></p><blockquote><p>note:</p><ol><li>批量查询用的是 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/docs-multi-get.html" target="_blank" rel="noopener">Multi Get API</a></li></ol></blockquote><h3 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/_exploring_your_data.html" target="_blank" rel="noopener">探索数据</a></h3><p>一个<a href="https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/src/test/resources/accounts.json" target="_blank" rel="noopener">简单的数据集</a>，数据结构如下：</p><pre><code>{    &quot;account_number&quot;: 0,    &quot;balance&quot;: 16623,    &quot;firstname&quot;: &quot;Bradshaw&quot;,    &quot;lastname&quot;: &quot;Mckenzie&quot;,    &quot;age&quot;: 29,    &quot;gender&quot;: &quot;F&quot;,    &quot;address&quot;: &quot;244 Columbus Place&quot;,    &quot;employer&quot;: &quot;Euron&quot;,    &quot;email&quot;: &quot;bradshawmckenzie@euron.com&quot;,    &quot;city&quot;: &quot;Hobucken&quot;,    &quot;state&quot;: &quot;CO&quot;}</code></pre><p>导入这个简单的数据集到es中</p><pre><code># 下载wget https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/src/test/resources/accounts.json# 导入curl -H &quot;Content-Type: application/json&quot; -XPOST &quot;localhost:9200/bank/_doc/_bulk?pretty&amp;refresh&quot; --data-binary &quot;@accounts.json&quot;</code></pre><p>上述命令是通过 _bulk API 将 account.json 的内容插入 bank 索引中，type 为 _doc</p><pre><code># account.json的内容:{&quot;index&quot;:{&quot;_id&quot;:&quot;1&quot;}}{&quot;account_number&quot;:1,&quot;balance&quot;:39225,&quot;firstname&quot;:&quot;Amber&quot;,&quot;lastname&quot;:&quot;Duke&quot;,&quot;age&quot;:32,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;880 Holmes Lane&quot;,&quot;employer&quot;:&quot;Pyrami&quot;,&quot;email&quot;:&quot;amberduke@pyrami.com&quot;,&quot;city&quot;:&quot;Brogan&quot;,&quot;state&quot;:&quot;IL&quot;}...# 导入完成后可以看到 bank 索引已存在 1000 条数据GET bank/_search</code></pre><h5 id="查询数据-API"><a href="#查询数据-API" class="headerlink" title="查询数据 API"></a>查询数据 API</h5><p>任务：查询所有数据，根据 account_number 字段升序排序</p><ol><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-uri-request.html#search-uri-request" target="_blank" rel="noopener">URI Search 方式</a></p><pre><code>GET /bank/_search?q=*&amp;sort=account_number:asc&amp;pretty</code></pre></li><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-request-body.html" target="_blank" rel="noopener">Request Body Search</a> 方式</p><pre><code>GET /bank/_search{&quot;query&quot;: { &quot;match_all&quot;: {} },&quot;sort&quot;: [ { &quot;account_number&quot;: &quot;asc&quot; }]}</code></pre></li></ol><p>结果</p><pre><code>{  &quot;took&quot;: 41,  &quot;timed_out&quot;: false,  &quot;_shards&quot;: {    &quot;total&quot;: 5,    &quot;successful&quot;: 5,    &quot;skipped&quot;: 0,    &quot;failed&quot;: 0  },  &quot;hits&quot;: {    &quot;total&quot;: 1000,    &quot;max_score&quot;: null,    &quot;hits&quot;: [      {        &quot;_index&quot;: &quot;bank&quot;,        &quot;_type&quot;: &quot;account&quot;,        &quot;_id&quot;: &quot;0&quot;,        &quot;_score&quot;: null,        &quot;_source&quot;: {          &quot;account_number&quot;: 0,          &quot;balance&quot;: 16623,          &quot;firstname&quot;: &quot;Bradshaw&quot;,          &quot;lastname&quot;: &quot;Mckenzie&quot;,          &quot;age&quot;: 29,          &quot;gender&quot;: &quot;F&quot;,          &quot;address&quot;: &quot;244 Columbus Place&quot;,          &quot;employer&quot;: &quot;Euron&quot;,          &quot;email&quot;: &quot;bradshawmckenzie@euron.com&quot;,          &quot;city&quot;: &quot;Hobucken&quot;,          &quot;state&quot;: &quot;CO&quot;        },        &quot;sort&quot;: [          0        ]      }...    ]  }}</code></pre><p>各个参数意思：</p><ul><li>took：本次查询耗费的时间（单位：毫秒）</li><li>timed_out：是否超时</li><li>_shards：本次查询搜索的 shard 的数量，包括成功的和失败的</li><li>hits：查询结果</li><li>hits.total：匹配的文档数量</li><li>hits.hits：匹配的文档，默认返回10个文档</li><li>hits.sort：排序的值</li><li>_score：文档的得分</li><li>hits.max_score：所有文档最高的得分</li></ul><h3 id="简要介绍-Query-DSL"><a href="#简要介绍-Query-DSL" class="headerlink" title="简要介绍 Query DSL"></a>简要介绍 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.3/query-dsl.html" target="_blank" rel="noopener">Query DSL</a></h3><p>这个Elasticsearch提供的基于 json 的查询语言，我们通过一个小任务来了解一下</p><p>任务要求：</p><ol><li>查询 firstname 中为 “R” 开头，年龄在 20 到 30 岁之间的人物信息</li><li>限制返回的字段为 firstname,city,address,email,balance</li><li>根据年龄倒序排序，返回前十条数据</li><li>对 firstname 字段进行高亮显示</li><li>同时求所有匹配人物的 平均balance</li></ol><pre><code>GET bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: [        {          &quot;match_phrase_prefix&quot;: {            &quot;firstname&quot;: &quot;R&quot;          }        }      ],      &quot;filter&quot;: {        &quot;range&quot;: {          &quot;age&quot;: {            &quot;gte&quot;: 20,            &quot;lte&quot;: 30          }        }      }    }  },  &quot;from&quot;: 0,  &quot;size&quot;: 10,  &quot;sort&quot;: [    {      &quot;age&quot;: {        &quot;order&quot;: &quot;desc&quot;      }    }  ],  &quot;_source&quot;: [    &quot;firstname&quot;,    &quot;city&quot;,    &quot;address&quot;,    &quot;email&quot;,    &quot;balance&quot;  ],  &quot;highlight&quot;: {    &quot;fields&quot;: {      &quot;firstname&quot;: {}    }  },  &quot;aggs&quot;: {    &quot;avg_age&quot;: {      &quot;avg&quot;: {        &quot;field&quot;: &quot;balance&quot;      }    }  }}</code></pre><p>其中：</p><ul><li>query 部分可以写各种查询条件</li><li>from, size 设置要返回的文档的起始序号</li><li>sort 设置排序规则</li><li>_source 设置要返回的文档的字段</li><li>highlight 设置高亮的字段</li><li>aggs 为设置聚合统计规则</li></ul><h5 id="更多查询示例"><a href="#更多查询示例" class="headerlink" title="更多查询示例"></a>更多查询示例</h5><ul><li>match_all 查询 bank 索引所有文档</li></ul><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;match_all&quot;: {}  },  &quot;size&quot;: 2}</code></pre><ul><li>match 全文搜索，查询 address 字段值为 mill lane 的所有文档</li></ul><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;match&quot;: {      &quot;address&quot;: &quot;mill lane&quot;    }  }}</code></pre><ul><li>match_phrase 短语匹配</li></ul><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;match_phrase&quot;: {      &quot;address&quot;: &quot;mill lane&quot;    }  }}</code></pre><blockquote><p>note:<br>match 和 match_phrase 的区别：</p><ul><li>match 中会分词，将 mill lane 拆分为 mill 和 lane， 实际查询 address 中有 mill <strong>或者</strong> lane 的文档</li><li>match_phrase：将 mill lane 作为一个整体查询，实际查询 address 中有 mill lane 的文档</li></ul></blockquote><ul><li>布尔查询（多条件查询）</li></ul><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: [        { &quot;match&quot;: { &quot;age&quot;: &quot;40&quot; } }      ],      &quot;must_not&quot;: [        { &quot;match&quot;: { &quot;state&quot;: &quot;ID&quot; } }      ]    }  }}</code></pre><ul><li>布尔查询-过滤<br>查询 bank 索引中 balance 值在 20000 到 30000 之间的文档</li></ul><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: { &quot;match_all&quot;: {} },      &quot;filter&quot;: {        &quot;range&quot;: {          &quot;balance&quot;: {            &quot;gte&quot;: 20000,            &quot;lte&quot;: 30000          }        }      }    }  }}</code></pre><ul><li>聚合查询<br>对所有文档进行聚合，state 值相同的分到同一个桶里，分桶结果命名为 group_by_state ，再对每个桶里的文档的 balance 字段求平均值，结果命名为 average_balance，通过设置 size 的值为0，不返回任何文档内容</li></ul><pre><code>GET /bank/_search{  &quot;size&quot;: 0,  &quot;aggs&quot;: {    &quot;group_by_state&quot;: {      &quot;terms&quot;: {        &quot;field&quot;: &quot;state.keyword&quot;      },      &quot;aggs&quot;: {        &quot;average_balance&quot;: {          &quot;avg&quot;: {            &quot;field&quot;: &quot;balance&quot;          }        }      }    }  }}</code></pre><p>分别计算 age 值在 20~30 ，30~40，40~50 三个年龄段的男和女的平均存款balance</p><pre><code>GET /bank/_search{  &quot;size&quot;: 0,  &quot;aggs&quot;: {    &quot;group_by_age&quot;: {      &quot;range&quot;: {        &quot;field&quot;: &quot;age&quot;,        &quot;ranges&quot;: [          {            &quot;from&quot;: 20,            &quot;to&quot;: 30          },          {            &quot;from&quot;: 30,            &quot;to&quot;: 40          },          {            &quot;from&quot;: 40,            &quot;to&quot;: 50          }        ]      },      &quot;aggs&quot;: {        &quot;group_by_gender&quot;: {          &quot;terms&quot;: {            &quot;field&quot;: &quot;gender.keyword&quot;          },          &quot;aggs&quot;: {            &quot;average_balance&quot;: {              &quot;avg&quot;: {                &quot;field&quot;: &quot;balance&quot;              }            }          }        }      }    }  }}</code></pre><blockquote><p>参考文档：</p><ol><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html" target="_blank" rel="noopener">elasticsearch 官方文档 Getting Started</a></li><li>慕课网 <a href="https://coding.imooc.com/class/181.html" target="_blank" rel="noopener">Elastic Stack从入门到实践</a></li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;需要明白的问题&quot;&gt;&lt;a href=&quot;#需要明白的问题&quot; class=&quot;headerlink&quot; title=&quot;需要明白的问题&quot;&gt;&lt;/a&gt;需要明白的问题&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;什么是倒排索引？它的组成是什么？&lt;/li&gt;
&lt;li&gt;常见的相关性算分方法有哪些？&lt;/li
      
    
    </summary>
    
      <category term="大数据" scheme="http://laijianfeng.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="elasticsearch" scheme="http://laijianfeng.org/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>使用hexo+github pages搭建博客</title>
    <link href="http://laijianfeng.org/2018/05/%E4%BD%BF%E7%94%A8hexo-github-pages%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://laijianfeng.org/2018/05/使用hexo-github-pages搭建博客/</id>
    <published>2018-05-23T03:35:46.000Z</published>
    <updated>2018-06-10T12:17:39.520Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么写博客"><a href="#为什么写博客" class="headerlink" title="为什么写博客"></a>为什么写博客</h2><p>就如我在博客主页上所说，主要有三点：</p><ol><li>记录与分享</li><li>锤炼技术，提高写作能力和表达能力</li><li>树立个人品牌，提高影响力</li></ol><p>而在此博客之前，我在CSDN上写过一些博客，截止于2018年5月23日，个人资料如下：</p><img title="小旋锋的csdn个人资料" alt="小旋锋的csdn个人资料" src="http://image.laijianfeng.org/static/images/201805/20180523_232554.png"><p>当我在CSDN上写博客的时候，几乎每天都会去看看阅读量增加了多少，排名增加了多少，又增加了几个粉丝或者新评论，每每都会带给我兴奋感，让我感到写博客其实是一件很有意义的事情，并且反过来推动我学习和记录，写更多的博客。</p><p>而为什么现在要重新整一个博客呢？主要是因为之前CSDN的博客更多的是转载和低质量的，而博主即将毕业，正走在程序员的职业道路上，需要树立个人品牌，写博客是目前对我比较合适且能做到的方式。</p><p>而独立博客自由度更高，第三方博客平台推广则更快，所以最终决定采用独立博客首发，第三方平台分发引流的模式。</p><p>我的第三方平台账户：</p><ul><li><a href="https://www.jianshu.com/u/ae269fd3620a" target="_blank" rel="noopener">小旋锋的简书</a></li><li><a href="https://blog.csdn.net/wwwdc1012" target="_blank" rel="noopener">小旋锋的csdn博客</a></li><li><a href="https://www.zhihu.com/people/whirlys/activities" target="_blank" rel="noopener">小旋锋的知乎</a></li><li><a href="http://image.laijianfeng.org/static/images/201805/20180523_230522.jpg" target="_blank" rel="noopener">小旋锋的微信公众号</a></li></ul><h2 id="Hexo主题选择"><a href="#Hexo主题选择" class="headerlink" title="Hexo主题选择"></a>Hexo主题选择</h2><p><a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">Hexo</a> 是一个快速、简洁且高效的博客框架，可托管于github pages，可免去维护服务器的麻烦，博主们可更专注于内容的创作，并且Hexo主题众多，总有一款适合你。</p><p>我对主题的要求主要有：</p><ol><li>不要太大众</li><li>大气美观</li><li>功能齐全</li></ol><p>经过了几天的搜索之后，筛选了几个比较满意的Hexo主题如下：</p><ol><li><a href="http://blog.zhangruipeng.me/hexo-theme-hueman/about/index.html" target="_blank" rel="noopener">Hueman</a></li><li><a href="http://jacman.wuchong.me/2014/11/20/how-to-use-jacman/" target="_blank" rel="noopener">jacman</a></li><li><a href="https://www.haomwei.com/" target="_blank" rel="noopener">大道至简</a></li><li><a href="http://threehao.com/" target="_blank" rel="noopener">Loo’s Blog</a></li><li><a href="http://yelog.org/2017/03/23/3-hexo-instruction/" target="_blank" rel="noopener">3-hexo</a></li></ol><p>最终选择了 <a href="http://yelog.org/2017/03/23/3-hexo-instruction/" target="_blank" rel="noopener">3-hexo</a> 这款主题，当然还有很多不错的主题。</p><h2 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h2><h3 id="1-根据-Hexo官网-步骤安装-git，node-js"><a href="#1-根据-Hexo官网-步骤安装-git，node-js" class="headerlink" title="1. 根据 Hexo官网 步骤安装 git，node.js"></a>1. 根据 <a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">Hexo官网</a> 步骤安装 git，node.js</h3><h3 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2. 安装Hexo"></a>2. 安装Hexo</h3><pre><code class="javascript">npm install -g hexo-cli</code></pre><p>安装 Hexo 完成后，新建一个博客的主目录，然后执行以下命令：</p><pre><code class="shell">hexo init &lt;folder&gt;cd &lt;folder&gt;npm install</code></pre><p>新建完成之后该目录的目录结构如下:</p><p>.</p><p>├── _config.yml        # 网站的 配置 信息</p><p>├── package.json        # 应用程序的信息</p><p>├── scaffolds            # 模板文件夹</p><p>├── source            # 博文源文件目录</p><p>|   ├── _drafts        # 草稿文件夹</p><p>|   └── _posts            # 博文文件夹</p><p>└── themes            # 主题文件夹</p><p>再执行以下命令，访问 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a> 即可快速体验Hexo</p><pre><code class="shell">hexo ghexo s</code></pre><img src="http://image.laijianfeng.org/static/images/201805/20180524_003246.png"><h3 id="3-根据-Hexo文档-对网站做一些简单的配置，然后修改主题为-3-hexo"><a href="#3-根据-Hexo文档-对网站做一些简单的配置，然后修改主题为-3-hexo" class="headerlink" title="3. 根据 Hexo文档 对网站做一些简单的配置，然后修改主题为 3-hexo"></a>3. 根据 <a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">Hexo文档</a> 对网站做一些简单的配置，然后修改主题为 <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank" rel="noopener">3-hexo</a></h3><p>安装</p><pre><code class="shell">git clone https://github.com/yelog/hexo-theme-3-hexo.git themes/3-hexo</code></pre><p>修改hexo根目录的_config.yml中的theme参数</p><pre><code class="javascript">theme: 3-hexo</code></pre><p>然后执行 hexo clean &amp; hexo g &amp; hexo s 即可看到效果</p><p>更多的主题配置可见 <a href="http://yelog.org/2017/03/23/3-hexo-instruction/" target="_blank" rel="noopener">3-hexo使用说明</a></p><h3 id="4-配置-github-pages"><a href="#4-配置-github-pages" class="headerlink" title="4. 配置 github pages"></a>4. 配置 github pages</h3><p>到github上创建一个新的空仓库，名字格式为 账户名.github.io，譬如我的github账户名是 <a href="https://github.com/whirlys" target="_blank" rel="noopener">whirlys</a>，所以我的github pages 仓库的名字应为 whirlys.github.io</p><p>安装插件</p><pre><code class="shell">npm install hexo-deployer-git --save</code></pre><p>然后配置 Hexo根目录的 _config.yml，xxx为你的用户名，注意还需要加入你的 github 用户名和密码，不然后面推送失败（但是上传代码时注意防止密码泄露）</p><pre><code class="shell">deploy:  type: git  repo: https://[github用户名]:[github密码]@github.com/xxx/xxx.github.io.git  branch: master</code></pre><p>如果你是第一次配置 github 远程仓库，你还须将你电脑的ssh key 配置到 github 上，具体可参考 <a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374385852170d9c7adf13c30429b9660d0eb689dd43a000" target="_blank" rel="noopener">git远程仓库</a></p><p>推送Hexo到github</p><pre><code class="shell">hexo deploy</code></pre><p>访问 xxx.github.io 即可看到你的 github pages 博客了</p><h3 id="5-绑定私有域名"><a href="#5-绑定私有域名" class="headerlink" title="5. 绑定私有域名"></a>5. 绑定私有域名</h3><p>我的域名为 laijianfeng.org，是一年前买 腾讯云1元学生主机 时送的，当然可以选择其他域名提供商</p><p>在 hexo source 目录下新建一个 CNAME 文件（没有后缀名），在文件里填入你的域名，然后 hexo d 推送到github</p><p>登录域名提供商网站，进入域名解析页面，分别添加两条记录</p><table><thead><tr><th>主机记录</th><th>记录类型</th><th>线路类型</th><th>记录值</th></tr></thead><tbody><tr><td>@</td><td>CNAME</td><td>默认</td><td>xxx.github.io</td></tr><tr><td>www</td><td>CNAME</td><td>默认</td><td><a href="http://www.xxx.github.io" target="_blank" rel="noopener">www.xxx.github.io</a></td></tr></tbody></table><p>等待十分钟之后，访问你的域名即可跳转到你的博客</p><h3 id="6-其他的配置"><a href="#6-其他的配置" class="headerlink" title="6. 其他的配置"></a>6. 其他的配置</h3><ul><li>接入评论，3-hexo主题中已经集成了多种评论，我选择了gitment，具体的配置参考 <a href="http://yelog.org/2017/06/26/gitment/" target="_blank" rel="noopener">完美替代多说-gitment</a>，如果gitment遇到问题，譬如报Error：validation failed异常，可参考 <a href="http://xichen.pub/2018/01/31/2018-01-31-gitment/" target="_blank" rel="noopener">添加Gitment评论系统踩过的坑</a> 以及 <a href="https://github.com/imsun/gitment/issues" target="_blank" rel="noopener">gitment issue</a>上的解决方法</li><li>使用七牛云图床，参考 <a href="http://skyhacks.org/2017/08/02/UseQiniudnToStorePic/" target="_blank" rel="noopener">使用七牛为Hexo存储图片</a> 和 <a href="https://github.com/gyk001/hexo-qiniu-sync" target="_blank" rel="noopener">Hexo七牛同步插件</a></li><li>代码高亮，字数统计，参考 <a href="http://yelog.org/2017/03/07/3-hexo/" target="_blank" rel="noopener">Hexo主题3-hexo</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;为什么写博客&quot;&gt;&lt;a href=&quot;#为什么写博客&quot; class=&quot;headerlink&quot; title=&quot;为什么写博客&quot;&gt;&lt;/a&gt;为什么写博客&lt;/h2&gt;&lt;p&gt;就如我在博客主页上所说，主要有三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;记录与分享&lt;/li&gt;
&lt;li&gt;锤炼技术，提高
      
    
    </summary>
    
      <category term="后端" scheme="http://laijianfeng.org/categories/%E5%90%8E%E7%AB%AF/"/>
    
    
      <category term="博客" scheme="http://laijianfeng.org/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
